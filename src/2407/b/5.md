<!-- # B5 流水线处理器 -->
# B5 Pipelined Processor

<!-- 我们已经通过icache提升了NPC的指令供给能力, 虽然面积预算很有限,
但添加icache还是明显提升了NPC的总体性能表现.
剩下的优化方向包括提升数据供给能力和提升计算效率. -->

We have improved the NPC's instruction supply capacity through icache,
although the area budget is very limited, adding icache has significantly improved the overall performance of the NPC.
The remaining areas for optimization include improving data supply capabilities and computational efficiency.

<!-- > #### todo::评估dcache的理想收益
> 提升数据供给能力的有效方法是添加dcache.
> 我们在上一节已经要求你通过性能计数器来估算dcache的理想收益了,
> 在对icache进行优化后, dcache在理想情况下带来的加速比可能会发生变化.
> 尝试重新评估dcache的理想收益. -->

> #### todo::Evaluating the ideal benefit of dcache
> An effective method for improving data supply capacity is to add dcache.
> In the previous section, we asked you to estimate the ideal benefit of dcache using performance counters.
> After optimizing icache, the acceleration ratio provided by dcache under ideal conditions may change.
> Try reevaluating the ideal benefit of dcache.

<!-- -->
<!-- > #### todo::通过cachesim评估dcache的预期性能表现
> 尝试改进你的cachesim, 让其可以读入mtrace,
> 然后评估一定大小的dcache的预期性能表现. -->

> #### todo::Evaluate the expected performance of the dcache using cachesim.
> Try to improve your cachesim so that it can read mtrace,
> then evaluate the expected performance of a dcache of a certain size.

<!-- 你应该会发现, 在剩余的面积预算下, 我们很难通过dcache来有效提升NPC的数据供给能力.
因此, 把剩余的面积用在提升计算效率, 是更科学的决策.
目前的NPC是多周期的, 这意味着NPC每经过若干个周期才能执行一条指令.
如果能提升NPC执行指令的吞吐, 就可以提升其计算效率.
流水线作为一种指令级并行技术, 可以有效提升NPC执行指令的吞吐. -->

You will likely find that, within the remaining area budget, it is challenging to effectively enhance the NPC's data supply capacity through dcache.
Therefore, allocating the remaining area to improving computational efficiency is a more scientifically sound decision.
The current NPC is multi-cycle, meaning it can only execute one instruction after several cycles.
If we can increase the NPC's instruction throughput, we can improve its computational efficiency.
Pipeline technology, as an instruction-level parallelism technique, can effectively enhance the NPC's instruction throughput.

<!-- > #### todo::评估提升计算效率的理想收益
> 我们还没有介绍流水线的具体实现,
> 但你已经可以根据性能计数器来估算流水线技术的理想收益了.
> 假设除访存指令之外, NPC每个周期都可以执行一条指令,
> 尝试估算在当前icache缺失情况的条件下, 此时NPC的理想加速比能到多少? -->

> #### todo::Evaluating the ideal gain in computational efficiency
> We have not yet introduced the specific implementation of the pipeline,
> but you can already estimate the ideal gain of pipeline technology based on performance counters.
> Assuming that NPC can execute one instruction per cycle except for memory access instructions,
> try to estimate the ideal acceleration ratio of NPC under the current conditions of icache deficiency.

<!-- -->
<!-- > #### caution::从全系统角度评估性能收益
> 这个估算结果可能会出乎你的意料.
>
> 还记得Amdahl's law那个加速5000倍的例子吗?
> 只看一项技术自身带来的性能收益, 和放到全系统场景下带来的性能收益, 可能是完全不同的.
> 如果你实现了乱序多发射, 但指令供给和数据供给跟不上,
> 在真实芯片中它们只是一堆占面积占功耗但对程序的运行性能提升不大的门电路. -->

> #### caution::Evaluate performance gains from a system-wide perspective
> This estimate may surprise you.
>
> Remember the example of a 5,000x speedup from Amdahl's law?
> The performance gains from a single technology, when viewed in isolation, may be vastly different from the gains achieved when that technology is integrated into a full system context.
> If you implement out-of-order execution and multiple issue, but the instruction and data supply cannot keep pace,
> in a real chip, these features may simply be a collection of gates that occupy space and consume power but offer little improvement in program performance.

<!-- ## 流水线的基本原理 -->
## Pipeline Basics

<!-- ### 工厂流水线 -->
### Factory Pipeline

<!-- 在我们的生活中也存在流水线的思想, 最常见的就是工厂中的流水线.
例如, 某工厂生产产品需要经过组装, 贴纸, 装入包装袋, 装入包装盒, 外检这5道工序.
如果分别用1~5来标识这些工序, 用A, B, C......标识不同的产品,
那么, 不采用流水线方式的时空图如下: -->

The concept of pipelines also exists in our lives, the most common example being factory pipeline.
For example, a factory produces products that need to go through five processes: assembly, labeling, bagging, boxing, and external inspection.
If we use 1 to 5 to label these processes and A, B, C, etc. to label different products,
then the space-time diagram without using a pipeline would be as follows:
```
          ----> Time
 | Product
 | +---+---+---+---+---+
 V |A.1|A.2|A.3|A.4|A.5|
   +---+---+---+---+---+
                       +---+---+---+---+---+
                       |B.1|B.2|B.3|B.4|B.5|
                       +---+---+---+---+---+
                                           +---+---+---+---+---+
                                           |C.1|C.2|C.3|C.4|C.5|
                                           +---+---+---+---+---+
================================================================
          ----> Time
 | Employee
 | +---+               +---+                +---+
 V |A.1|               |B.1|                |C.1|
   +---+               +---+                +---+
       +---+               +---+                +---+
       |A.2|               |B.2|                |C.2|
       +---+               +---+                +---+
           +---+               +---+                +---+
           |A.3|               |B.3|                |C.3|
           +---+               +---+                +---+
               +---+               +---+                +---+
               |A.4|               |B.4|                |C.4|
               +---+               +---+                +---+
                   +---+                +---+                +---+
                   |A.5|                |B.5|                |C.5|
                   +---+                +---+                +---+
```

<!-- 而采用流水线方式的时空图如下: -->
The space-time diagram with a pipeline would be as follows:

```
          ----> Time
 | Product
 | +---+---+---+---+---+
 V |A.1|A.2|A.3|A.4|A.5|
   +---+---+---+---+---+
       +---+---+---+---+---+
       |B.1|B.2|B.3|B.4|B.5|
       +---+---+---+---+---+
           +---+---+---+---+---+
           |C.1|C.2|C.3|C.4|C.5|
           +---+---+---+---+---+
               +---+---+---+---+---+
               |D.1|D.2|D.3|D.4|D.5|
               +---+---+---+---+---+
                   +---+---+---+---+---+
                   |E.1|E.2|E.3|E.4|E.5|
                   +---+---+---+---+---+
================================================================
          ----> Time
 | Employee
 | +---+---+---+---+---+
 V |A.1|B.1|C.1|D.1|E.1|
   +---+---+---+---+---+
       +---+---+---+---+---+
       |A.2|B.2|C.2|D.2|E.2|
       +---+---+---+---+---+
           +---+---+---+---+---+
           |A.3|B.3|C.3|D.3|E.3|
           +---+---+---+---+---+
               +---+---+---+---+---+
               |A.4|B.4|C.4|D.4|E.4|
               +---+---+---+---+---+
                   +---+---+---+---+---+
                   |A.5|B.5|C.5|D.5|E.5|
                   +---+---+---+---+---+
```

<!-- 可以看到, 在流水线方式中, 虽然每个产品的生产时间没有减少,
但由于每位员工都一直保持工作状态, 它们能连续处理不同产品的同一道工序,
使得从总体上来看, 每一时刻都有一件产品完成生产, 从而提升产线的吞吐. -->

As can be seen, in the pipeline method, although the production time of each product has not been reduced,
since each employee remains in a working state, they can continuously process the same process for different products,
so that, overall, one product is completed at any given moment, thereby increasing the throughput of the production line.

<!-- ### 指令流水线 -->
### Instruction Pipeline

<!-- 类比工厂流水线, 处理器也可以用流水线方式来执行指令.
我们把指令执行的过程分为若干个阶段, 让每个部件处理其中一个阶段,
并让这些部件保持工作状态, 可以连续处理不同指令的同一个阶段,
使得从总体上来看, 每个周期都有一条指令完成执行, 从而提升处理器的吞吐. -->

Similar to a factory pipeline, processors can also use pipelines to execute instructions.
We divide the instruction execution process into several stages, allowing each component to handle one stage,
and keeping these components in working state so that they can continuously handle the same stage of different instructions.
This means that, overall, one instruction is completed in each cycle, thereby improving the processor's throughput.

<!-- 在学习总线的时候, 我们已经要求大家把NPC升级成一个分布式控制的多周期处理器.
多周期处理器已经有阶段的概念了, 其工作过程和上述工厂场景中的非流水线方式非常类似.
因此, 指令流水线的工作方式也不难理解了. -->

When learning bus, we asked everyone to upgrade NPC to a distributed control multi-cycle processor.
Multi-cycle processors already have the concept of stages, and their working process is very similar to the non-pipeline method in the factory scenario mentioned above.
Therefore, it is not difficult to understand how instruction pipelines work.

<!-- 我们可以对几种处理器的性能进行简单的分析评估.
假设将处理器的工作分为取指, 译码, 执行, 访存, 写回这5个阶段,
它们的逻辑延迟都是1ns, 且先不考虑取指和访存的延迟. -->

We can perform a simple analysis and evaluation of the performance of several processors.
Assuming that the processor's work is divided into five stages: instruction fetch (IF), decoding (ID, instruction decode), execution (EX), load-store (LS), and write back (WB),
their logical delays are all 1 ns, and we will not consider the delays of instruction fetch and load-store for now.

<!-- 1. 单周期处理器: 阶段间无寄存器, 因此关键路径为5ns, 频率为200MHz.
   其中, 一条指令需要执行1周期, 即5ns;
   每1周期执行1条指令, 即`IPC = 1`.
1. 多周期处理器: 阶段间有寄存器, 因此关键路径为1ns, 频率为1000MHz.
   其中, 一条指令需要执行5周期, 即5ns;
   每5周期执行1条指令, 即`IPC = 0.2`.
1. 流水线处理器: 阶段间有寄存器, 因此关键路径为1ns, 频率为1000MHz.
   其中, 一条指令需要执行5周期, 即5ns;
   每1周期执行1条指令, 即`IPC = 1`. -->

1. Single-cycle processor: No registers between stages, so the critical path is 5 ns and the frequency is 200 MHz.
   Each instruction takes 1 cycle to execute, i.e., 5 ns;
   Each cycle executes 1 instruction, i.e., `IPC = 1`.
1. Multi-cycle processor: There are registers between stages, so the critical path is 1 ns and the frequency is 1000 MHz.
   Each instruction takes 5 cycles to execute, i.e., 5 ns;
   One instruction is executed every 5 cycles, i.e., `IPC = 0.2`.
1. Pipelined Processor: There are registers between stages, so the critical path is 1 ns and the frequency is 1000 MHz.
   Each instruction takes 5 cycles to execute, i.e., 5 ns;
   Each cycle executes 1 instruction, i.e., `IPC = 1`..

<!-- | 处理器 | 频率    | 指令执行延迟 | IPC |
| :-:    | :-:     | :-:          | :-: |
| 单周期 | 200MHz  | 5ns          | 1   |
| 多周期 | 1000MHz | 5ns          | 0.2 |
| 流水线 | 1000MHz | 5ns          | 1   | -->

|    Processor    | Frequency    | Instruction Execution Delay | IPC |
|       :-:       |    :-:       |         :-:                 | :-: |
|   Single cycle  |    200MHz    |         5ns                 | 1   |
| Multiple cycles |    1000MHz   |         5ns                 | 0.2 |
|    Pipelined    |    1000MHz   |         5ns                 | 1   |

<!-- 可以看到, 虽然指令执行的延迟仍然是5ns, 但流水线具有频率高和IPC高的优势,
这些优势本质上是由指令级并行技术带来的:
流水线处理器的每个周期都在处理5条不同的指令. -->

As can be seen, although the instruction execution delay is still 5ns, the pipeline has the advantages of high frequency and high IPC.
These advantages are essentially brought about by instruction-level parallelism technology:
each cycle of the pipelined processor processes five different instructions.

<!-- 当然, 上面只是从理想情况下分析得到的数据.
如果考虑SoC中的访存, IPC就远远没有这么高了;
此外流水线处理器也并不是总能在每个周期都执行5条指令, 我们会在下文进一步分析. -->

Of course, the above data is only obtained from an ideal situation.
If we consider the load-store operations in SoC, the IPC will not be as high.
In addition, pipelined processors cannot always execute five instructions in each cycle, which will be further analyzed below.

<!-- > #### question::更长的流水线
> 上面的例子只将流水线划分成5个阶段. 事实上, 我们可以将流水线划分成更多的阶段,
> 使得每一个阶段的逻辑更简单, 从而提升处理器整体的频率. 这种流水线称为"超流水"(superpipeline).
> 例如, 如果能将流水线划分成30级, 按照上面的估算, 理论上频率可以达到6GHz.
>
> 但目前的主流高性能处理器一般只会将流水线划分成15级左右,
> 你认为有可能是哪些因素导致不宜将流水线划分成过多的阶段? -->

> #### question::Longer Pipelines
> The above example only divides the pipeline into five stages. In fact, we can divide the pipeline into more stages,
> making the logic of each stage simpler and thereby increasing the overall frequency of the processor. This type of pipeline is called “superpipeline.”
> For example, if the pipeline can be divided into 30 stages, according to the above estimate, the frequency can theoretically reach 6GHz.
>
> However, current mainstream high-performance processors generally only divide the pipeline into about 15 stages.
> What factors do you think may make it inappropriate to divide the pipeline into too many stages?

<!-- ## 流水线的简单实现 -->
## Simple implementation of Pipeline

<!-- 基于握手机制的多周期处理器来实现流水线并不困难,
对于每个阶段的输入`in`和输出`out`, 我们只需要正确处理以下信号
(`bits`指代阶段之间需要传输的负载):
* `out.bits`, 由当前阶段生成
* `out.valid`, 由当前阶段生成, 通常还与`in.valid`有关
* `in.ready`, 由当前阶段生成, 忙碌时置为无效, 处理完当前指令时置为有效
* `out.ready`, 与下一阶段的`in.ready`相同
* `in.bits`, 当前阶段的`in.ready`和上一阶段的`out.valid`同时有效时,
  更新成上一阶段的`out.bits`
* `in.valid`, 作为作业留给大家 -->

It is not difficult to implement a pipelined processor based on a multi-cycle processor with a handshake mechanism.
For each stage's input `in` and output `out`, we only need to correctly process the following signals
(`bits` refers to the load that needs to be transferred between stages):
* `out.bits`, generated by the current stage
* `out.valid`, generated by the current stage, usually related to `in.valid`
* `in.ready`, generated by the current stage, set to invalid when busy, and set to valid after the current instruction is processed
* `out.ready`, identical to the next stage's `in.ready`
* `in.bits`, when both the current stage's `in.ready` and the previous stage's `out.valid` are valid,
  it is updated to the previous stage's `out.bits`
* `in.valid`, left as an exercise for you

<!-- 根据上述内容, 我们可以将后三个信号的处理包装成一个函数,
然后用它来连接各个阶段即可: -->

Based on the above content, we can package the processing of the last three signals into a function,
and then use it to connect the various stages:

```scala
def pipelineConnect[T <: Data, T2 <: Data](prevOut: DecoupledIO[T],
  thisIn: DecoupledIO[T], thisOut: DecoupledIO[T2]) = {
    prevOut.ready := thisIn.ready
    thisIn.bits := RegEnable(prevOut.bits, prevOut.valid && thisIn.ready)
    thisIn.valid := ???
  }

pipelineConnect(ifu.io.out, idu.io.in, idu.io.out)
pipelineConnect(idu.io.out, exu.io.in, exu.io.out)
pipelineConnect(exu.io.out, lsu.io.in, lsu.io.out)
// ...
```

<!-- 特别地, IFU无需等待当前指令执行结束, 即可马上取出下一条指令.
上述的`RegEnable`起到了传统教科书中"流水段寄存器"的作用,
但我们也可以从总线的视角将它理解为下游模块接收消息的缓冲区:
上下游握手成功后, 上游模块认为下游模块已经成功接收到消息,
因此它不再保存该消息, 故下游模块需要将收到的消息记录到缓冲区中, 防止消息丢失.
而对于前三个信号, 由于其具体逻辑与当前阶段的行为有关,
因此需要在当前阶段对应的模块中实现. -->

Specifically, the IFU does not need to wait for the current instruction to finish executing before fetching the next instruction.
The `RegEnable` mentioned above serves the role of the “pipeline stage register” described in traditional textbooks,
but we can also understand it from the bus perspective as a buffer for downstream modules to receive messages:
After successful handshaking between upstream and downstream modules, the upstream module assumes the downstream module has successfully received the message,
so it no longer retains the message. Therefore, the downstream module must record the received message in the buffer to prevent message loss.
For the first three signals, since their specific logic is related to the behavior of the current stage,
they must be implemented in the corresponding module of the current stage.

<!-- 特别地, 流水线处理器中会存在一些无法继续执行当前指令的情况, 称为"冒险"(Hazard).
冒险主要分为3类: 结构冒险, 数据冒险和控制冒险.
如果无视冒险强行执行, 将会导致CPU状态机的转移结果与ISA状态机不一致,
表现为指令的执行结果不符合其语义.
因此在流水线设计中, 我们需要检测出冒险,
要么通过硬件设计消除它们, 要么从时间上等待冒险不再发生.
对于后者, 可以通过对`in.ready`和`out.valid`添加等待条件来实现. -->

In particular, there are some situations in pipelined processors where the current instruction cannot be executed, which is called a “hazard.”
Hazards are mainly divided into three categories: structural hazards, data hazards, and control hazards.
If hazards are ignored and execution is forced, it will cause the CPU state machine transfer results to be inconsistent with the ISA state machine,
resulting in the instruction execution results not matching their semantics.
Therefore, in pipeline design, we need to detect hazards
and either eliminate them through hardware design or wait for the hazards to no longer occur.
For the latter, this can be achieved by adding wait conditions to `in.ready` and `out.valid`.

<!-- ### 结构冒险 -->
### Structural Hazard

<!-- 结构冒险是指流水线中的不同阶段需要同时访问同一个部件,
但该部件无法支持被多个阶段同时访问.
例如, 在下图所示的指令序列中,
在T4时刻, I1正在LSU中读数据, I4正在IFU中取指令, 两者都需要读内存;
在T5时刻, I1正在WBU中写寄存器, I4正在IDU中读寄存器, 两者都需要访问寄存器堆. -->

Structural hazards occur when different stages in the pipeline need to access the same component simultaneously,
but the component cannot support simultaneous access by multiple stages.
For example, in the instruction sequence shown in the figure below,
at time T4, I1 is reading data in the LSU, and I4 is fetching instructions in the IFU, both of which require memory access;
at time T5, I1 is writing registers in the WBU, and I4 is reading registers in the IDU, both of which require access to the register file.

```
           T1   T2   T3   T4   T5   T6   T7   T8
         +----+----+----+----+----+
I1: lw   | IF | ID | EX | LS | WB |
         +----+----+----+----+----+
              +----+----+----+----+----+
I2: add       | IF | ID | EX | LS | WB |
              +----+----+----+----+----+
                   +----+----+----+----+----+
I3: sub            | IF | ID | EX | LS | WB |
                   +----+----+----+----+----+
                        +----+----+----+----+----+
I4: xor                 | IF | ID | EX | LS | WB |
                        +----+----+----+----+----+
```

<!-- 部分结构冒险可以从硬件设计上完全避免, 使其不会在CPU执行过程中发生:
我们只需要在硬件设计上让这些部件支持同时被多个阶段访问即可.
具体地:
* 对于寄存器堆, 我们只需要独立实现其读口和写口,
  让IDU通过读口访问寄存器堆, 让WBU通过写口访问寄存器堆
* 对于内存, 则有多种解决方案
  * 像寄存器堆那样将读口和写口分开, 实现真双口的内存
  * 将内存分为指令存储器和数据存储器, 两者独立工作
  * 引入cache, 如果在cache中命中, 则无需访问内存 -->

Some structural hazards can be completely avoided in hardware design so that they do not occur during CPU execution:
We only need to enable these components to support simultaneous access by multiple stages in the hardware design.
Specifically:
* For the register file, we only need to implement its read port and write port independently,
  allowing the IDU to access the register file through the read port and the WBU to access the register file through the write port
* For memory, there are multiple solutions:
  * Separate read and write ports like the register file to implement true dual-port memory
  * Divide memory into instruction memory and data memory, which operate independently
  * Introduce a cache; if the cache hit occurs, there is no need to access memory

<!-- > #### caution::教科书和真实系统的差异
> 教科书上的解决方案大多基于一些简化过的假设,
> 这些假设在真实的处理器中很可能不再成立,
> 因此它们不一定适合在"一生一芯"的场景中使用.
>
> 例如, SDRAM的存储颗粒无法将读口跟写口分开,
> 无论是READ命令还是WRITE命令, 都是通过SDRAM的存储器总线传递给SDRAM颗粒.
> 而将内存分为指令存储器和数据存储器, 将会使得指令和数据位于不同的地址空间,
> 这违反了ISA规范对指令和数据采用统一地址空间的内存模型.
> 一方面, 现代编译器无法编译出适应上述方案的程序;
> 另一方面, 即使采用汇编语言来开发程序,
> 类似bootloader的加载程序功能也无法正确运行.
>
> 事实上, "一生一芯"对大家提出了更高的要求: 学会从全系统的角度评价一个方案.
> 教科书上的简化假设能帮助大家聚焦到当前知识点的学习中,
> 但你将来要面对的是真实的项目, 只有学会将系统中的各个因素关联起来,
> 你才能在将来的工作中做出合理有效的决策. -->

> #### caution::Differences between textbooks and real systems
> Most solutions in textbooks are based on simplified assumptions,
> which may not hold true in real processors,
> so they may not be suitable for use in OSOC scenarios.
>
> For example, SDRAM memory chips cannot separate read ports from write ports.
> Both READ and WRITE commands are transmitted to SDRAM chips via the SDRAM memory bus.
> Dividing memory into instruction memory and data memory will cause instructions and data to be located in different address spaces,
> which violates the ISA specification's memory model of using a unified address space for instructions and data.
> On the one hand, modern compilers cannot compile programs that adapt to the above scheme;
> on the other hand, even if assembly language is used to develop programs,
> programs such as bootloader's loader cannot run correctly.
>
> In fact, OSOC places higher demands on everyone: learning to evaluate a scheme from a system-wide perspective.
> The simplified assumptions in textbooks can help everyone focus on learning the current knowledge points,
> but in the future, you will face real projects, and only by learning to connect the various factors in the system,
> you will be able to make reasonable and effective decisions in your future work.

<!-- 还有一些结构冒险还是无法完全避免, 例如:
* cache缺失时, IFU和LSU还是要同时访问内存
* SDRAM控制器的队列满了, 无法继续接收请求
* 除法器的计算需要花数十个周期, 在一次计算结束之前无法开始另一次 -->

Some structural hazards cannot be completely avoided, such as:
* When the cache is missing, IFU and LSU still need to access memory at the same time
* The SDRAM controller queue is full and cannot continue to receive requests
* The divider calculation takes dozens of cycles, and another calculation cannot be started before the previous one is completed

<!-- 为了应对上述情况, 一种简单的处理方式是等待:
如果IFU和LSU同时访存, 就让一个等待另一个;
等SDRAM控制器的队列有空闲位置;
等除法器完成当前计算.
一个好消息是, 总线天生就具备等待的功能,
因此只要从设备, 下游模块或仲裁器把ready置为无效,
就能把结构冒险的检测和处理归约到总线状态机,
从而无需实现专门的结构冒险检测和处理逻辑. -->

To deal with the above situation, a simple approach is to wait:
If IFU and LSU perform load-store operations at the same time, let one wait for the other;
Wait until the SDRAM controller queue has free space;
Wait until the divider completes the current calculation.
The good news is that the bus inherently has a wait function,
so as long as the slave device, downstream module, or arbitrator sets ready to invalid,
the detection and handling of structural hazards can be reduced to the bus state machine,
eliminating the need to implement dedicated structural hazard detection and handling logic.

<!-- > #### question::谁等谁?
> 在等待的时候, 是让IFU等待LSU, 还是让LSU等待IFU?
> 还是两种方案都可以? 为什么? -->

> #### question::Who waits for whom?
> While waiting, should IFU wait for LSU, or should LSU wait for IFU?
> Or are both options possible? Why?

<!-- ### 数据冒险 -->
### Data Hazard

<!-- 数据冒险是指不同阶段的指令依赖同一个寄存器数据, 且至少有一条指令写入该寄存器.
例如, 在下图所示的指令序列中,
I1要写a0寄存器, 但要在T5时刻结束时才完成写入, 在这之前,
I2在T3时刻读到a0的旧值, I3在T4时刻读到a0的旧值,
I4在T5时刻读到a0的旧值, I5在T6时刻才能读到a0的新值. -->

Data hazards occur when instructions at different stages depend on the same register data, and at least one instruction writes to that register.
For example, in the instruction sequence shown in the figure below,
I1 writes to register a0, but the write operation is not completed until the end of time T5. Before that,
I2 reads the old value of a0 at time T3, I3 reads the old value of a0 at time T4,
I4 reads the old value of a0 at time T5, and I5 can only read the new value of a0 at time T6.

```
                    T1   T2   T3   T4   T5   T6   T7   T8   T9
                  +----+----+----+----+----+
I1: add a0,t0,s0  | IF | ID | EX | LS | WB |
                  +----+----+----+----+----+
                       +----+----+----+----+----+
I2: sub a1,a0,t0       | IF | ID | EX | LS | WB |
                       +----+----+----+----+----+
                            +----+----+----+----+----+
I3: and a2,a0,s0            | IF | ID | EX | LS | WB |
                            +----+----+----+----+----+
                                 +----+----+----+----+----+
I4: xor a3,a0,t1                 | IF | ID | EX | LS | WB |
                                 +----+----+----+----+----+
                                      +----+----+----+----+----+
I5: sll a4,a0,1                       | IF | ID | EX | LS | WB |
                                      +----+----+----+----+----+
```

<!-- 上述数据冒险称为写后读(Read After Write, RAW)冒险, RAW冒险的特征是,
一条指令需要写入某寄存器, 而另一条更年轻的指令需要读出该寄存器.
显然, 如果不处理这种数据冒险, 指令I2, I3和I4将会因为读到a0的旧值而计算出错误的结果,
违反指令执行的语义. -->

The above data hazard is called a read after write (RAW) hazard. The characteristic of a RAW hazard is that
one instruction needs to write to a certain register, while another younger instruction needs to read that register.
Obviously, if this data hazard is not handled, instructions I2, I3, and I4 will calculate the wrong result because they read the old value of a0,
violating the semantics of instruction execution.

<!-- > #### comment::指令之间的顺序关系
> 中文的"前"和"后"存在歧义, 可以指代"以前"/"前方"/"以后"/"后方",
> 在动态指令序列中一般采用"年老"和"年轻"来描述不同指令之间的先后关系.
> 例如, "I1比I2年老"(I1 is older than I2), 表示I1在时间上比I2先执行;
> "I2比I1年轻"(I2 is yonger than I1), 表示I2在时间上比I1后执行. -->

> #### comment::The sequential relationship between instructions
<!-- > The Chinese terms “before” and ‘after’ are ambiguous and can refer to “prior to,” “ahead of,” “after,” or “behind.” -->
> In dynamic instruction sequences, “older” and “younger” are generally used to describe the sequential relationship between different instructions.
> For example, “I1 is older than I2” indicates that I1 is executed before I2 in terms of time;
> “I2 is younger than I1” indicates that I2 is executed after I1 in terms of time.

<!-- 解决RAW冒险有多种方式.
从软件上来看, 指令是由编译器生成的,
因此一种方式是让编译器来检测RAW冒险, 并插入空指令,
来等待写入寄存器的指令完成写入, 如下图所示: -->

There are several ways to solve the RAW hazard.
From a software perspective, instructions are generated by the compiler,
so one way is to have the compiler detect the RAW hazard and insert a nop instruction
to wait for the instruction writing to the register to complete, as shown in the figure below:

```
                    T1   T2   T3   T4   T5   T6   T7   T8   T9   T10  T11  T12
                  +----+----+----+----+----+
I1: add a0,t0,s0  | IF | ID | EX | LS | WB |
                  +----+----+----+----+----+
                       +----+----+----+----+----+
    nop                | IF | ID | EX | LS | WB |
                       +----+----+----+----+----+
                            +----+----+----+----+----+
    nop                     | IF | ID | EX | LS | WB |
                            +----+----+----+----+----+
                                 +----+----+----+----+----+
    nop                          | IF | ID | EX | LS | WB |
                                 +----+----+----+----+----+
                                      +----+----+----+----+----+
I2: sub a1,a0,t0                      | IF | ID | EX | LS | WB |
                                      +----+----+----+----+----+
                                           +----+----+----+----+----+
I3: and a2,a0,s0                           | IF | ID | EX | LS | WB |
                                           +----+----+----+----+----+
                                                +----+----+----+----+----+
I4: xor a3,a0,t1                                | IF | ID | EX | LS | WB |
                                                +----+----+----+----+----+
                                                     +----+----+----+----+----+
I5: sll a4,a0,1                                      | IF | ID | EX | LS | WB |
                                                     +----+----+----+----+----+
```

<!-- 插入空指令的本质还是等待, 但其实编译器还可以做得更好:
与其等待, 还不如执行一些有意义的指令.
这可以通过让编译器进行指令调度的工作来实现,
编译器可以尝试寻找一些没有数据依赖关系的指令, 在不影响程序行为的情况下调整其顺序.
一个例子如下, 其中I6, I7和I8均与I1无数据依赖关系,
因此可以调度到I1之后执行: -->

The essence of inserting empty instructions is still waiting, but compilers can actually do better:
Instead of waiting, it is better to execute some meaningful instructions.
This can be achieved by letting the compiler perform instruction scheduling.
The compiler can try to find instructions that have no data dependencies and adjust their order without affecting the behavior of the program.
An example is shown below, where I6, I7, and I8 have no data dependencies with I1,
so they can be scheduled to execute after I1:

```
I1: add a0,t0,s0          I1: add a0,t0,s0
I2: sub a1,a0,t0          I6: add t5,t4,t3
I3: and a2,a0,s0          I7: add s5,s4,s3
I4: xor a3,a0,t1   --->   I8: sub s6,t4,t2
I5: sll a4,a0,1           I2: sub a1,a0,t0
I6: add t5,t4,t3          I3: and a2,a0,s0
I7: add s5,s4,s3          I4: xor a3,a0,t1
I8: sub s6,t4,t2          I5: sll a4,a0,1
```

<!-- > #### comment::编译器和乱序执行处理器
> 如果把编译器的指令调度工作下放到硬件来完成, 我们就得到了一个乱序执行处理器.
> 当然, 为了在硬件上完成指令调度的工作, 我们还需要额外添加不少硬件模块.
> 但从本质上来说, 这两项技术都是为了提升处理器执行指令的效率. -->

> #### comment::Compilers and Out-of-Order Execution Processors
> If we delegate the compiler's instruction scheduling task to the hardware, we get an out-of-order execution processor.
> Of course, in order to perform instruction scheduling in hardware, we need to add a number of additional hardware modules.
> However, fundamentally, both of these technologies are designed to improve the efficiency of the processor's instruction execution.

<!-- 不过对于指令调度的工作, 编译器只能尽力而为, 并非总能找到合适的指令.
例如, 除法指令需要执行数十个周期, 编译器通常很难找到这么多合适的指令.
在这些情况下, 如果要让编译器来处理RAW冒险, 还是只能插入空指令. -->

However, when it comes to instruction scheduling, compilers can only do their best and cannot always find suitable instructions.
For example, division instructions require dozens of cycles to execute, and it is usually difficult for compilers to find so many suitable instructions.
In such cases, if the compiler is to handle RAW hazards, it can only insert empty instructions.

<!-- 一个更糟糕的消息是, 有的RAW光靠编译器是无法解决的.
考虑被依赖的指令是一条load指令, 这种RAW冒险称为load-use冒险: -->

Worse still, some RAWs cannot be resolved by the compiler alone.
Consider a load instruction as the dependent instruction. This type of RAW hazard is called a load-use hazard:

```
                    T1   T2   T3  ....  T?   T?   T?   T?   T?   T?   T?
                  +----+----+----+--------------+----+
I1: lw  a0,t0,s0  | IF | ID | EX |      LS      | WB |
                  +----+----+----+--------------+----+
                       +----+----+----+----+----+
    nop X ?            | IF | ID | EX | LS | WB |
                       +----+----+----+----+----+
                                                +----+----+----+----+----+
I2: sub a1,a0,t0                                | IF | ID | EX | LS | WB |
                                                +----+----+----+----+----+
```

<!-- 事实上, 在真实的SoC中, 软件几乎无法预测一条访存指令在将来执行时的延迟:
* 如果cache命中, 数据可能3个周期后返回
* 如果cache缺失, 要访问SDRAM, 数据可能30个周期后返回
* 如果正好碰上SDRAM充电刷新, 数据可能30+?个周期后返回
* 如果CPU频率从500MHz提升到600MHz, 数据返回所需的周期数更多 -->

In fact, in real SoCs, it is almost impossible for software to predict the latency of a load-store instruction when it is executed in the future:
* If the cache hit, the data may return after 3 cycles
* If the cache miss, accessing SDRAM may result in the data returning after 30 cycles
* If it happens to coincide with SDRAM charge refresh, the data may return after 30+ cycles
* If the CPU frequency is increased from 500MHz to 600MHz, the number of cycles required for data return increases

<!-- 也因为这样, 现代处理器几乎都采用硬件检测并处理RAW冒险的方案.
由于寄存器的写入操作发生在WBU中, 因此需要写入的寄存器编号会也会随着流水线传播到WBU,
也即, 我们可以从每个阶段中找到相应指令将要写入哪一个寄存器.
如果位于IDU的指令要读出的寄存器与后续某阶段中将要写入的寄存器相同, 则发生RAW冒险: -->

For this reason, modern processors almost all use hardware to detect and handle RAW hazards.
Since register write operations occur in the WBU, the register numbers to be written are also propagated to the WBU along with the pipeline.
In other words, we can find which register the corresponding instruction will write to in each stage.
If the register to be read by the instruction located in the IDU is the same as the register to be written in a subsequent stage, a RAW hazard occurs:

```scala
def conflict(rs: UInt, rd: UInt) = (rs === rd)
def conflictWithStage[T <: Stage](rs1: UInt, rs2: UInt, stage: T) = {
  conflict(rs1, stage.rd) || conflict(rs2, stage.rd)
}
val isRAW = conflictWithStage(IDU.rs1, IDU.rs2, EXU) ||
            conflictWithStage(IDU.rs1, IDU.rs2, LSU) ||
            conflictWithStage(IDU.rs1, IDU.rs2, WBU)
```

<!-- 上面的伪代码只是一个大致的思路, 实际上你还需要考虑更多的问题:
并非所有指令都需要写入寄存器, 并非所有阶段都正在执行指令,
并非所有指令都需要读出rs2(如U型指令), 零寄存器的值恒为`0`等.
如何写出正确的RAW检测代码, 就交给你来思考了. -->

The pseudocode above is just a rough idea. In reality, you need to consider more issues:
Not all instructions need to be written to registers, not all stages are executing instructions,
not all instructions need to read rs2 (such as U-type instructions), and the value of the zero register is always `0`, etc.
How to write the correct RAW detection code is up to you to figure out.

<!---
检查计分板(scoreboard), 它为每个通用寄存器记录是否将要被写入.
具体地, 这种方法需要分配一个数组`Busy`,
在ID阶段, 若指令需要写入`R(x)`, 则将`Busy(x)`置`1`;
在WB阶段, 若指令需要写入`R(x)`, 则将`Busy(x)`置`0`;
在ID阶段, 若指令需要读出`R(x)`, 且`Busy(x)`为`1`, 则发生RAW冒险.
不过这只是一个大致的思路, 我们将更多的细节作为作业留给大家思考.
-->

<!-- 检测到RAW冒险后, 最简单的处理方式还是等待: 只要把`in.ready`和`out.valid`置为无效即可.
可见, 这种硬件检测和处理RAW冒险的方案, 无需提前得知指令执行何时结束,
这是因为指令执行过程中的各种等待都会通过总线的握手信号传递到流水线当中,
因此适用性比上述的软件方案更强. -->

After detecting a RAW hazard, the simplest way to handle it is to wait: just set `in.ready` and `out.valid` to invalid.
It can be seen that this hardware detection and handling of RAW hazards does not require advance knowledge of when the instruction execution will end.
This is because various waits during instruction execution are passed to the pipeline through the bus handshake signal.
Therefore, it is more applicable than the above software solution.

```
                    T1   T2   T3   T4   T5   T6   T7   T8   T9   T10  T11  T12
                  +----+----+----+----+----+
I1: add a0,t0,s0  | IF | ID | EX | LS | WB |
                  +----+----+----+----+----+
                       +----+-------------------+----+----+----+
I2: sub a1,a0,t0       | IF |         ID        | EX | LS | WB |
                       +----+-------------------+----+----+----+
                            +-------------------+----+----+----+----+
I3: and a2,a0,s0            |         IF        | ID | EX | LS | WB |
                            +-------------------+----+----+----+----+
                                                +----+----+----+----+----+
I4  xor a3,a0,t1                                | IF | ID | EX | LS | WB |
                                                +----+----+----+----+----+
                                                     +----+----+----+----+----+
I5: sll a4,a0,1                                      | IF | ID | EX | LS | WB |
                                                     +----+----+----+----+----+
```

<!-- ### 控制冒险 -->
### Control Hazards

<!-- 控制冒险是指跳转指令会改变指令执行顺序, 导致IFU可能会取到不该执行的指令.
例如, 在下图所示的指令序列中, T4的IFU具体应该取出哪条指令,
需要等到I3在T5时刻计算出跳转结果后才能得知. -->

Control hazard refers to a situation where a jump instruction changes the order of instruction execution, causing the IFU to retrieve instructions that should not be executed.
For example, in the instruction sequence shown in the figure below, which instruction the IFU of T4 should retrieve
can only be known after I3 calculates the jump result at time T5.

```
                 T1   T2   T3   T4   T5   T6   T7   T8
               +----+----+----+----+----+
I1: 100   add  | IF | ID | EX | LS | WB |
               +----+----+----+----+----+
                    +----+----+----+----+----+
I2: 104   lw        | IF | ID | EX | LS | WB |
                    +----+----+----+----+----+
                         +----+----+----+----+----+
I3: 108   beq 200        | IF | ID | EX | LS | WB |
                         +----+----+----+----+----+
                              +----+----+----+----+----+
I4: ???   ???                 | IF | ID | EX | LS | WB |
                              +----+----+----+----+----+
```

<!-- 除了上述分支指令, `jal`和`jalr`也会造成类似问题.
假设上图的I3为跳转指令, 我们期望在T4时刻就取出跳转目标处的指令,
而在T4时刻IDU正好在对I3进行译码, 按道理是可以赶上的,
但现代处理器一般会认为还是赶不上, 从而将其作为控制冒险来处理. -->

In addition to the above branch instructions, `jal` and `jalr` also cause similar problems.
Assuming that I3 in the above figure is a jump instruction, we expect to retrieve the instruction at the jump target at time T4.
At time T4, the IDU is decoding I3, so logically it should be able to catch up.
However, modern processors generally consider that it cannot catch up, and therefore treat it as a control hazard.

<!-- > #### question::为什么现代处理器要这样处理?
> 事实上, 有一些教科书确实会通过上述方式来处理控制冒险.
> 你认为有哪些因素使得上述的教科书方案无法在真实的处理器设计中采用? -->

> #### question::Why do modern processors handle this in this way?
> In fact, some textbooks do handle control hazards in the manner described above.
> What factors do you think prevent the textbook approach from being used in actual processor designs?

<!-- 甚至CPU抛出异常也会导致控制冒险.
抛出异常时, 需要马上从`mtvec`所指的内存位置重新取指, 但通常来说,
处理器无法在取指时刻就得知这条指令的执行是否会发生异常. -->

Even CPU exceptions can cause control hazards.
When an exception is thrown, the instruction must be fetched immediately from the memory location pointed to by mtvec, but generally speaking,
the processor cannot know at the time of fetching whether the execution of this instruction will cause an exception.

<!-- 上述问题都是因为在取指阶段无法确定接下来真正需要取出哪条指令.
如果选择等待, 就要等待上一条指令几乎执行完成, 才能得知下一条指令的真正地址.
例如, 访存指令要等到访存结束后, 通过总线的`resp`信号才能确定访存过程没有抛出异常.
显然, 这个方案会使得指令流水线流不起来, 大幅降低处理器执行指令的吞吐.
而如果选择不等待, 就有可能取出了一部分不该执行的指令,
如果不采取进一步的处理措施, 处理器的状态转移就会与ISA状态机不一致, 从而导致执行结果不正确. -->

The above problems are all caused by the inability to determine which instruction needs to be fetched next during the instruction fetch phase.
If you choose to wait, you have to wait until the previous instruction is almost complete before you can know the actual address of the next instruction.
For example, load-store instructions have to wait until the load-store operation is complete before the `resp` signal on the bus can confirm that no exceptions were thrown during the load-store process.
Obviously, this solution will cause the instruction pipeline to stall, greatly reducing the processor's instruction execution throughput.
If you choose not to wait, you may fetch some instructions that should not be executed.
If no further processing measures are taken, the processor's state transition will be inconsistent with the ISA state machine, resulting in incorrect execution results.

<!-- 为了应对控制冒险, 现代处理器通常采用"推测执行"(speculative execution)的技术.
推测执行本质上是一种预测技术, 其基本思想是, 在等待的同时尝试推测一个选择,
如果猜对了, 就相当于提前做出了正确的选择, 从而节省了等待的开销.
推测执行具体由三部分组成:
* 选择策略 - 得到正确结果之前, 通过一定的策略推测一个选择
* 检查机制 - 得到正确结果时, 检查之前推测的选择是否与正确结果一致
* 错误恢复 - 如果检查后发现不一致, 则需要回滚到选择策略时的状态,
  并根据得到的正确结果做出正确的选择 -->

In order to deal with control hazards, modern processors often use speculative execution technology.
Speculative execution is essentially a prediction technology. The basic idea is to try to guess a choice while waiting.
If the guess is correct, it is equivalent to making the correct choice in advance, thereby saving the overhead of waiting.
Speculative execution consists of three parts:
* Selection strategy - Before obtaining the correct result, a choice is speculated based on a certain strategy
* Checking mechanism - When the correct result is obtained, the previously speculated choice is checked to see if it matches the correct result
* Error recovery - If the check reveals a mismatch, the system must roll back to the state at the time of the selection strategy,
and make the correct choice based on the obtained correct result

<!-- 针对控制冒险, 一种最简单的推测执行策略是"总是推测接下来执行下一条静态指令".
从上述三部分考虑这种策略的实现, 具体如下:
* 选择策略 - 非常简单, 只需要让IFU一直取出`PC + 4`处的指令即可.
* 检查机制 - 根据指令的语义, 只有在执行分支和跳转指令, 以及抛出异常时,
  CPU才有可能改变执行流, 其他情况下都是顺序执行.
  因而在其他情况下, 上述推测的选择总是正确的, 无需额外检查.
  故只需要在执行分支和跳转指令, 以及抛出异常时, 才需要检查跳转结果与推测的选择是否一致,
  也即, 检查跳转结果是否为`PC + 4`.
* 错误恢复 - 如果发现上述跳转结果不为`PC + 4`, 则说明之前的推测是错误的,
  基于这一推测所取出的指令都不应该被执行, 应该将其从流水线上消除, 这一动作称为"冲刷";
  同时还需要让IFU从正确的跳转结果处取指. -->

For control hazards, the simplest speculative execution strategy is to “always speculate that the next static instruction will be executed.”
Considering the implementation of this strategy from the above three aspects, the details are as follows:
* Selection strategy - Very simple, just let the IFU continuously fetch instructions from `PC + 4`.
* Checking mechanism - According to the semantics of the instruction, only when executing branch and jump instructions, as well as when throwing exceptions,
  is it possible for the CPU to change the execution flow. In other cases, execution is sequential.
  Therefore, in other cases, the above speculative selection is always correct, and no additional checks are required.
  Therefore, it is only necessary to check whether the jump result is consistent with the speculative selection when executing branch and jump instructions, as well as when throwing exceptions.
  In other words, check whether the jump result is `PC + 4`.
* Error recovery - If the above jump result is not `PC + 4`, it means that the previous speculation was wrong,
  and the instructions taken based on this speculation should not be executed, but should be eliminated from the pipeline. This action is called “flushing”;
  at the same time, the IFU needs to take instructions from the correct jump result.

<!-- 推测执行所带来的性能提升与推测的准确率有关, 如果推测的准确率高,
则IFU能以高概率提前取到正确的指令, 从而节省等待的开销;
如果推测的准确率低, 则IFU经常取到不该执行的指令, 这些指令后续又被冲刷,
在这段时间内, 流水线的行为等价于未执行任何有效指令, 从而降低了执行指令的吞吐.
具体地:
* 由于异常属于处理器执行过程中的小概率事件, 绝大部分指令的执行都不会抛出异常,
  因此针对异常, 上述策略的准确率接近100%
* 分支指令的执行结果只有"跳转"(taken)和"不跳转"(not taken),
  上述策略相当于总是预测"不跳转",
  因此从概率上来说, 针对分支指令, 上述策略的准确率接近50%
* 跳转指令的行为是无条件跳转到目标地址, 但目标地址的可能有很多,
  正好跳转到`PC + 4`的概率非常低,
  因此针对跳转指令, 上述策略的准确率接近0% -->

The performance improvement brought about by speculative execution is related to the accuracy of the speculation. If the accuracy of the speculation is high,
the IFU can retrieve the correct instruction in advance with a high probability, thereby saving the overhead of waiting.
If the accuracy of the speculation is low, the IFU often fetches instructions that should not be executed, and these instructions are subsequently flushed.
During this period, the behavior of the pipeline is equivalent to not executing any valid instructions, thereby reducing the throughput of executing instructions.
Specifically:
* Since exceptions are low-probability events in the processor execution process, the vast majority of instructions will not throw exceptions when executed.
  Therefore, for exceptions, the accuracy rate of the above strategy is close to 100%.
* The execution results of branch instructions are only taken and not taken.
  The above strategy is equivalent to always predicting “not taken.”
  Therefore, in terms of probability, for branch instructions, the accuracy rate of the above strategy is close to 50%.
* The behavior of a jump instruction is to jump unconditionally to the target address, but there are many possible target addresses.
  The probability of jumping exactly to `PC + 4` is very low.
  Therefore, for jump instructions, the accuracy of the above strategy is close to 0%.

<!-- 根据上述分析, 推测执行一方面可以正确处理控制冒险,
另一方面, 相对于消极等待的方式, 推测执行还可以带来一定的性能提升.
但针对分支指令和跳转指令, 上述的推测执行方案还有较大的提升空间, 我们会在下文继续讨论. -->

Based on the above analysis, speculative execution can correctly handle control hazards on the one hand,
and on the other hand, it can also bring certain performance improvements compared to passive waiting.
However, for branch instructions and jump instructions, there is still room for improvement in the above speculative execution scheme, which we will discuss further below.

<!-- 关于推测执行的实现, 还有一些需要注意的细节:
* 从需求的角度来看, 冲刷是为了将处理器的状态恢复成发生控制冒险之前的时刻,
  因此, 我们可以从状态机的视角推导出应该如何处理相关的实现细节.
  状态机视角告诉我们, 处理器的状态由时序逻辑电路决定,
  而处理器的状态更新又受到控制信号的控制, 因此, 要实现冲刷的效果,
  我们只需要考虑将相关的控制信号置为无效即可.
  例如, 通过将`valid`置为无效, 可以将大部分部件正在执行的指令直接冲刷掉.
* 但如果部件中还存在一些影响控制信号的状态, 你还需要进行额外的考量,
  例如icache中的状态机, 尤其是发出的AXI请求无法撤回, 因此需要等待请求完成.
* 推测执行意味着当前执行的操作不一定是将来真正需要的,
  如果推测错误, 就应该取消相关操作.
  但有一些操作很难取消, 包括更新寄存器堆, 更新CSR, 写内存, 访问外设等,
  这些模块的状态一旦发生改变, 就很难恢复到旧状态.
  因此, 需要在确认推测正确后, 才能更新这些模块的状态. -->

There are some details to note regarding the implementation of speculative execution:
* From a demand perspective, flushing is intended to restore the processor's state to the moment before the control hazard occurred.
  Therefore, we can deduce how to handle the relevant implementation details from the perspective of the state machine.
  The state machine perspective tells us that the state of the processor is determined by the sequential logic circuit,
  and the state update of the processor is controlled by the control signal. Therefore, to achieve the effect of flushing,
  we only need to consider setting the relevant control signals to invalid.
  For example, by setting `valid` to invalid, most of the instructions being executed by the components can be flushed directly.
* However, if there are still some states within the components that affect control signals, additional considerations are needed,
  such as the state machine in the I-cache, especially since issued AXI requests cannot be retracted, so we must wait for the request to complete.
* Speculative execution means that the currently executed operation may not necessarily be needed in the future.
  If the speculation is wrong, the relevant operation should be canceled.
  However, some operations are difficult to cancel, including updating the register file, updating the CSR, writing to memory, accessing peripherals, etc.
  Once the state of these modules changes, it is difficult to restore them to their previous state.
  Therefore, the state of these modules can only be updated after confirming that the speculation is correct.

<!-- > #### todo::实现简单的流水线处理器
> 用最简单的方式处理各种冒险, 从而实现最基本的流水线结构.
> 实现后, 尝试运行microbench, 从而检查你的实现是否正确.
>
> Hint:
> * 为了让DiffTest正确工作, 你可能需要修改向仿真环境传递的信号
> * 目前你可以先忽略异常处理相关的实现, 我们接下来再实现它 -->

> #### todo::Implement a simple pipelined processor
> Handle various hazards in the simplest way possible to implement the most basic pipeline structure.
> After implementation, try running microbench to check if your implementation is correct.
>
> Hint:
> * In order for DiffTest to work correctly, you may need to modify the signals passed to the simulation environment.
> * For now, you can ignore the implementation related to exception handling. We will implement it later.

<!-- -->
<!-- > #### question::嵌套的推测执行
> 思考一下, 如果遇到了连续的分支指令或者跳转指令, 你的设计还能正确工作吗? -->

> #### question::Nested speculative execution
> Think about it: if you encounter consecutive branch instructions or jump instructions, will your design still work correctly?

<!-- 对于流水线处理器中异常的实现, 我们还需要考虑如下细节:
* 抛出异常时, 需要将`mepc`设置为发生异常的指令的PC值, 这种特性称为"精确异常".
  如果不满足这一特性, 从异常处理通过`mret`返回时,
  就无法精确返回到之前发生异常的指令处, 从而使得发生异常前后的状态不一致.
  这可能会导致系统软件无法利用异常机制实现现代操作系统的某些关键机制,
  例如进程切换, 请页调度等.
  但在流水线处理器中, IFU的PC会不断变化, 等到某条指令在执行过程中抛出异常时,
  IFU的PC已经与这条指令不匹配了.
  为了获取与该指令匹配的PC, 我们需要在IFU取指时将相应的PC一同传递到下游.
* 指令可能会在推测执行时抛出异常, 但如果推测错误,
  那实际上这条指令不应该被执行, 所抛出的异常也不应该被处理.
  因此, 在异常处理过程中需要更新处理器状态的操作,
  都需要等到确认推测正确后, 才能真正进行,
  包括写入`mepc`, `mcause`, 跳转到`mtvec`所指示的位置等.
* `mcause`的更新取决于异常的类型, 在RISC-V处理器中, 不同的异常号由不同的部件产生.
  产生的异常号也需要传递到流水线的下游,
  等到确认推测正确后, 才能真正写入`mcause`. -->

For the implementation of exceptions in pipelined processors, we also need to consider the following details:
* When throwing an exception, `mepc` needs to be set to the PC value of the instruction where the exception occurred. This feature is called “precise exception.”
  If this property is not satisfied, when returning from exception handling through `mret`,
  it is impossible to return precisely to the instruction where the exception occurred, resulting in inconsistency between the states before and after the exception.
  This may cause the system software to be unable to use the exception mechanism to implement certain key mechanisms of modern operating systems,
  such as process switching and page scheduling.
  However, in a pipelined processor, the PC of the IFU changes constantly, and by the time an exception is thrown during the execution of an instruction,
  the PC of the IFU no longer matches that instruction.
  In order to obtain the PC that matches the instruction, we need to pass the corresponding PC downstream when the IFU fetches the instruction.
* An instruction may throw an exception during speculative execution, but if the speculation is incorrect,
  then the instruction should not actually be executed, and the thrown exception should not be handled.
  Therefore, operations that require updating the processor state during exception handling
  must wait until the speculation is confirmed to be correct before they can actually be performed,
  including writing to `mepc`, `mcause`, jumping to the location indicated by `mtvec`, etc.
* The update of `mcause` depends on the type of exception. In RISC-V processors, different exception numbers are generated by different components.
  The generated exception numbers also need to be passed to the downstream of the pipeline,
  and only after the speculation is confirmed to be correct can `mcause` be actually written.

<!-- | 异常号 | 异常描述                       | 最早产生的部件 |
| ---    | ---                            | ---      |
|  0     | Instruction address misaligned | IFU      |
|  1     | Instruction access fault       | IFU      |
|  2     | Illegal Instruction            | IDU      |
|  3     | Breakpoint                     | IDU      |
|  4     | Load address misaligned        | LSU      |
|  5     | Load access fault              | LSU      |
|  6     | Store/AMO address misaligned   | LSU      |
|  7     | Store/AMO access fault         | LSU      |
|  8     | Environment call from U-mode   | IDU      |
|  9     | Environment call from S-mode   | IDU      |
| 11     | Environment call from M-mode   | IDU      |
| 12     | Instruction page fault         | IFU      |
| 13     | Load page fault                | LSU      |
| 15     | Store/AMO page fault           | LSU      | -->

| Exception Number |      Exception Description     | Component Where Exception First Occurred |
|       ---        |               ---              |                    ---                   |
|        0         | Instruction address misaligned |                    IFU                   |
|        1         | Instruction access fault       |                    IFU                   |
|        2         | Illegal Instruction            |                    IDU                   |
|        3         | Breakpoint                     |                    IDU                   |
|        4         | Load address misaligned        |                    LSU                   |
|        5         | Load access fault              |                    LSU                   |
|        6         | Store/AMO address misaligned   |                    LSU                   |
|        7         | Store/AMO access fault         |                    LSU                   |
|        8         | Environment call from U-mode   |                    IDU                   |
|        9         | Environment call from S-mode   |                    IDU                   |
|       11         | Environment call from M-mode   |                    IDU                   |
|       12         | Instruction page fault         |                    IFU                   |
|       13         | Load page fault                |                    LSU                   |
|       15         | Store/AMO page fault           |                    LSU                   |

<!-- > #### todo::实现支持异常处理的流水线
> 按照上述内容, 实现支持异常处理的流水线.
> 实现后, 运行一些异常处理相关的测试, 来检查你的实现是否正确. -->

> #### todo::Implement a pipeline that supports exception handling
> Based on the above content, implement a pipeline that supports exception handling.
> After implementation, run some exception handling-related tests to check if your implementation is correct.

<!-- > #### question::发生多个异常
> 在流水线处理器中, 不同阶段会执行不同的指令,
> 这意味着不同阶段可能会同时产生不同的异常.
> 例如, 在IFU发现指令地址不对齐的同时, IDU发现非法指令, LSU发现读访存错误.
> 这时应该如何处理?
>
> 尽管目前我们不要求你实现所有异常, 但你仍然可以思考, 你的设计可以正确处理这一情况吗? -->

> #### question::Multiple exceptions occur
> In a pipelined processor, different instructions are executed at different stages,
> which means that different exceptions may occur simultaneously at different stages.
> For example, while the IFU finds that the instruction address is not aligned, the IDU finds an illegal instruction, and the LSU finds a load-store error.
> How should this be handled?
>
> Although we do not require you to implement all exceptions at this time, you can still think about whether your design can handle these situations correctly.

<!-- > #### comment::Intel的超流水架构
> 在2005年之前的一段时间, Intel曾经使用超流水技术追求极致的主频.
> 在2004年2月, Intel发布了一款架构代号为[Prescott][prescott wiki]的处理器,
> 其流水线深度达到前所未有的31级, 即使在当时的90nm工艺节点上, 主频也达到了3.8GHz.
> 但在实测时发现, 和上一代20级流水线的Northwood相比,
> 这款处理器的性能[并没有得到太大提升][prescott stackexchange],
> 反而成为了x86史上运行最烫和功耗最高的单核处理器.
> 事实上, 如果不是散热和功耗问题, 这款处理器的主频还能比3.8GHz更高:
> [在发布的一年前, Intel宣称这款处理器的主频最高可以达到5GHz][prescott anandtech].
>
> 从微结构层面来看, 31级流水线的执行效率也并不尽如人意.
> 一方面, 流水线中的指令充斥着数据冒险, 很多指令因为RAW冒险, 只能在流水线中等待.
> 另一方面, 分支指令因推测执行错误而冲刷流水线的代价却很高.
> 假设处理器在第26级计算出分支指令是否跳转, 在推测执行错误的情况下,
> 前面的25个周期所取出的指令需要全部冲刷.
> 特别地, Prescott是一个多发射处理器, [每周期可进行4次简单ALU操作][prescott anandtech2],
> 估算其为4发射, 因此在推测执行错误的情况下, 需要冲刷`25 * 4 = 100`条指令!
> 尽管Prescott采用了一些先进的技术提升推测的准确率,
> 但根据实测结果, [仍然有不少程序的性能因为过长的流水线而下降][prescott wiki2].
>
> 后来, Intel放弃了这种激进超流水的技术路线,
> [后续架构的流水线深度普遍在15级左右, 最多不超过20级][intel cpu wiki]. -->

> #### comment::Intel's Superpipeline Architecture
> Prior to 2005, Intel used superpipeline technology to pursue the ultimate clock speed.
> In February 2004, Intel released a processor with the code name [Prescott][prescott wiki],
> which had an unprecedented 31-stage pipeline depth and a clock speed of 3.8GHz, even with the 90nm process node at the time.
> However, actual measurements showed that compared to the previous generation 20-stage pipeline Northwood,
> the performance of this processor [did not improve much][prescott stackexchange],
> but instead became the hottest and most power-consuming single-core processor in x86 history.
> In fact, if it weren't for cooling and power consumption issues, the clock speed of this processor could have been even higher than 3.8GHz:
> [A year before its release, Intel claimed that the clock speed of this processor could reach up to 5GHz][prescott anandtech].
>
> From a microarchitecture perspective, the execution efficiency of the 31-stage pipeline was not ideal either.
> On the one hand, the instructions in the pipeline were full of data hazards, and many instructions could only wait in the pipeline due to RAW hazards.
> On the other hand, the cost of flushing the pipeline due to speculative execution errors in branch instructions is very high.
> Assuming that the processor calculates whether to take a branch instruction at the 26th level, in the case of a speculative execution error,
> all instructions taken in the previous 25 cycles need to be flushed.
> In particular, Prescott is a multi-issue processor [that can perform four simple ALU operations per cycle][prescott anandtech2],
> estimating it to be a 4-issue processor, in the case of speculative execution errors, it needs to flush 25 * 4 = 100 instructions!
> Although Prescott uses some advanced technologies to improve the accuracy of speculation,
> according to actual measurement results, [the performance of many programs still declines due to the excessive length of the pipeline][prescott wiki2].
>
> Later, Intel abandoned this aggressive superpipeline technology route,
> [the pipeline depth of subsequent architectures is generally around 15 levels, with a maximum of 20 levels][intel cpu wiki].

[prescott wiki]: https://en.wikipedia.org/wiki/NetBurst#Revisions
[prescott stackexchange]: https://softwareengineering.stackexchange.com/questions/210818/how-long-is-a-typical-modern-microprocessor-pipeline
[prescott anandtech]: https://www.anandtech.com/show/1073
[prescott anandtech2]: https://www.anandtech.com/show/1230/7
[prescott wiki2]: https://en.wikipedia.org/wiki/Pentium_4#Prescott
[intel cpu wiki]: https://en.wikipedia.org/wiki/List_of_Intel_CPU_microarchitectures

<!-- ## 流水线处理器的测试验证 -->
## Testing and Verification of Pipelined Processors

<!-- 流水线的处理对象是指令, 这意味着, 不同的指令序列将对流水线的行为产生不同的影响.
因此, 流水线的验证过程应将指令序列作为测试输入, 并尽可能覆盖各种不同的情况.
根据指令在流水线中的行为, 可粗略分为以下10个种类:
* 加/减/逻辑/移位等ALU可在一周期内完成计算的指令
* 控制流转移指令, 分3种: 条件分支, `jal`, `jalr`
* 访存指令, 分2种: load, store
* CSR指令
* `ecall`, `mret`
* `fence.i` -->

The processing object of the pipeline is instructions, which means that different instruction sequences will have different effects on the behavior of the pipeline.
Therefore, the verification process of the pipeline should use instruction sequences as test inputs and cover as many different situations as possible.
According to the behavior of instructions in the pipeline, they can be roughly divided into the following 10 types:
* Instructions that can be completed by the ALU in one cycle, such as addition, subtraction, logic, and shift
* Control flow transfer instructions, divided into three types: conditional branch, `jal`, and `jalr`
* Load-store instructions, divided into two types: load and store
* CSR instructions
* `ecall`, `mret`
* `fence.i`

<!-- 仅仅考虑上述10类指令在传统5级流水线中的组合情况, 就已经有$10^5=100000$种可能.
事实上, 还需要考虑上文提到的各种冒险: 访存可能需要等待,
指令之间存在数据依赖, 控制流转移指令会导致流水线冲刷......
总之, 不同情况组合而成的指令序列实在太多,
即使是开发一个指令生成器, 也很难保证覆盖上述所有情况,
如果是人工设计那么多测试用例, 就更困难了. -->

Just considering the combinations of the above 10 types of instructions in a traditional 5-stage pipeline, there are already $10^5=100000$ possibilities.
In fact, we also need to consider the various hazards mentioned above: load-store may need to wait,
there are data dependencies between instructions, control flow transfer instructions may cause pipeline flushes...
In short, there are too many instruction sequences formed by different combinations of situations.
Even if an instruction generator is developed, it is difficult to ensure that all of the above situations are covered.
It is even more difficult to design so many test cases manually.

<!-- > #### comment::通过传统的指令测试集无法证明处理器的正确性
> 如果你了解类似`riscv-tests`的指令测试集, 你需要明白,
> 通过`riscv-tests`并不代表流水线NPC是正确的.
> 事实上, `riscv-tests`中提供的测试用例数量远远没到上文粗略计算的100000,
> 这就已经说明了, 必定存在某些指令序列, 是`riscv-tests`无法覆盖的.
> 更本质地, `riscv-tests`测试的是单条指令的行为,
> 而充分测试流水线处理器, 需要的是遍历各种指令序列.
> 因此, 如果你的流水线NPC仅仅通过了`riscv-tests`这些传统的指令测试集,
> 你不应该对你的NPC实现感到100%放心. -->

> #### comment::The correctness of the processor cannot be proven by traditional instruction test sets.
> If you are familiar with instruction test sets such as `riscv-tests`, you need to understand that
> passing `riscv-tests` does not mean that the pipeline NPC is correct.
> In fact, the number of test cases provided in `riscv-tests` is far from the rough estimate of 100,000 mentioned above.
> This already indicates that there must be certain instruction sequences that cannot be covered by `riscv-tests`.
> More fundamentally, `riscv-tests` tests the behavior of a single instruction,
> while to fully test a pipelined processor, it is necessary to traverse various instruction sequences.
> Therefore, if your pipelined NPC only passes the traditional instruction test set `riscv-tests`,
> you should not be 100% confident in your NPC implementation.

<!-- 既然人工参与测试用例的设计很困难, 那就想办法交给工具吧!
回顾我们在验证缓存的时候用到的形式化验证工具,
它可以自动帮助我们找到违反`assert`的测试用例,
如果找不到, 就证明了设计的正确性.
如果能够将形式化验证应用到流水线NPC,
就可以让工具帮助我们自动寻找错误的指令序列了! -->

Since it is difficult for humans to design test cases, let's find a way to delegate this task to tools!
Let's review the formal verification tool we used to verify the cache.
It can automatically help us find test cases that violate `assert`.
If no such cases are found, it proves that the design is correct.
If we can apply formal verification to the pipeline NPC,
we can let the tool help us automatically find incorrect instruction sequences!

<!-- 要使用形式化验证, 我们还需要一个REF, 以及编写合适的验证条件(即`assert`).
对于REF, 我们需要可以正确执行指令序列的另一个实现.
由于我们使用的形式化验证工具只能在RTL层次进行验证,
因此暂时无法接入类似NEMU和Spike的指令集模拟器.
不过我们可以考虑之前开发的单周期NPC,
它同样作为RISC-V ISA的微结构实现, 必然能正确执行指令序列. -->

To use formal verification, we also need a REF and to write appropriate verification conditions (i.e., `assert`).
For REF, we need another implementation that can correctly execute the instruction sequence.
Since the formal verification tool we use can only perform verification at the RTL level,
it is currently not possible to access instruction set emulators such as NEMU and Spike.
However, we can consider the single-cycle NPC developed earlier,
which is also a microarchitecture implementation of the RISC-V ISA and can necessarily execute the instruction sequence correctly.

<!-- 至于验证条件, 一个想法是像DiffTest那样, 在DUT和REF执行完相同的指令后,
检查两者GPR的状态是否一致.
在C代码中, 检查所有GPR是否一致只是一次`memcmp()`的调用, 开销并不大;
但在形式化验证中, 检查所有GPR是否一致却是作为"解方程"的约束条件,
会对BMC的求解过程带来不小的开销.
为此, 我们有必要寻找一种较为简单的对比方法. -->

As for the verification conditions, one idea is to check whether the states of the GPRs of the DUT and REF are consistent after they have executed the same instructions, as in DiffTest.
In C code, checking whether all GPRs are consistent is just a single `memcmp()` call, which is not very costly;
but in formal verification, checking whether all GPRs are consistent is treated as a constraint condition for “solving equations,”
which can significantly increase the computational cost of the BMC solving process.
Therefore, it is necessary to find a simpler comparison method.

<!-- 回顾状态机模型, 新状态取决于当前状态以及状态的转移.
因此, 除了直接对比新状态, 我们还可以从另一个角度来进行对比:
在当前状态一致的情况下, 如果状态的转移也一致, 那么新状态也应当一致.
通常来首, 描述一次状态转移比描述状态本身(即状态空间)要小很多,
因此对比状态的转移是否一致, 通常是一种更简单的对比方式.
以上述GPR的状态为例, 虽然GPR的状态空间有$16\times 32=512$位,
但一条RISC-V指令最多写入一个GPR,
我们只需要记录这条指令将哪个GPR更新成哪个值,
并对比DUT和REF的这条记录是否一致, 就不必直接对比512位的GPR状态空间是否一致,
从而大幅简化对比的开销. -->

Reflecting on the state machine model, the new state depends on the current state and the state transition.
Therefore, in addition to directly comparing the new states, we can also compare them from another perspective:
If the current states are consistent and the state transitions are also consistent, then the new states should also be consistent.
Generally speaking, describing a state transition is much simpler than describing the state itself (i.e., the state space),
so comparing whether the state transitions are consistent is typically a simpler comparison method.
Taking the above GPR state as an example, although the GPR state space has $16\times 32=512$ bits,
a RISC-V instruction can write to at most one GPR.
We only need to record which GPR this instruction updates to which value,
and compare whether the DUT and REF records are consistent, without having to directly compare whether the 512-bit GPR state space is consistent,
thus greatly simplifying the comparison overhead.

<!-- 根据上文的分析, 我们很容易编写出验证顶层模块的伪代码.
这里我们用Chisel作为伪代码, 如果你使用Verilog开发, 你仍然可以借鉴相关思路来编写验证顶层模块. -->

Based on the analysis above, it is easy to write pseudocode for verifying the top-level module.
Here, we use Chisel as pseudocode. If you are developing in Verilog, you can still draw on the relevant ideas to write pseudocode for verifying the top-level module.
```scala
class PipelineTest extends Module {
  val io = IO(new Bundle {
    val inst = Input(UInt(32.W))
    val rdata = Input(UInt(XLEN.W))
  })

  val dut = Module(new PipelineNPC)
  val ref = Module(new SingleCycleNPC)

  dut.io.imem.inst  := io.inst
  dut.io.imem.valid := ...
  dut.io.dmem.rdata := io.rdata
  dut.io.dmem.valid := ...
  // ...

  ref.io.imem.inst := dut.io.wb.inst
  // ...

  when (dut.io.wb.valid) {
    assert(dut.io.wb.rd  === ref.io.wb.rd)
    assert(dut.io.wb.res === ref.io.wb.res)
  }
}
```

<!-- 上述伪代码只给出了大致的框架, 还有不少细节需要你注意并补充:
* 为了减小BMC的求解空间, DUT只包含流水线本身, 而不包含缓存和各种外设模块,
  但可以通过握手信号实现缓存缺失带来的访存延迟
* 指令是验证顶层模块的输入之一, 这意味着BMC将会遍历各种指令,
  在界的作用下, BMC将会遍历各种指令在不同周期下的组合,
  也即实现了在给定长度下所有指令序列的遍历.
* 生成的指令序列将会依次输入DUT的IFU, 但由于对一条指令来说,
  在流水线NPC中需要等待若干周期才能完成执行, 而在单周期NPC中只需要1周期即可完成执行,
  因此需要在DUT和REF之间进行同步, 使得DUT执行完一条指令后,
  才将该指令输入到REF中, 故需要在DUT的WBU中获得当前完成执行的指令.
* 除了对比写入的GPR编号`rd`及其值`res`外, 还需要对比PC,
  以检查控制流的转移是否正确.
  不过我们可以选择对比PC的新值, 这样能提早一个周期捕捉PC不一致的错误.
* 有的指令不写入GPR, 对于这些指令, 我们无需对比`rd`和`res`.
* 对于load指令, 其访存结果也作为流水线的输入之一, 故需要在验证顶层模块的端口处体现.
  为了保证DUT和REF在执行同一条load指令后状态一致, 还需要保证它们读出相同的数据.
* 对于store指令的执行结果, 我们不进行专门的对比, 主要有两点考虑:
  1. 写数据和写地址都来源于GPR, 如果它们有错,
     那么一定存在一条比store指令年老的指令向GPR写入了错误的值,
     这种情况可以通过上述机制检查出来.
  2. 如果写数据和写地址没错, 但AXI写通道的信号错了,
     这个问题属于AXI总线实现的范畴, 不属于流水线的验证范畴.
     原则上也可以在流水线的验证框架中加入对AXI信号的检查,
     但这将会增加BMC的约束条件, 从而增加求解的开销.
* 需要正确处理IFU和LSU的握手信号.
* 对于CSR的写入, 我们也不进行专门的对比, 因为若对某CSR的写入实现错误,
  则可以通过额外一条指令将该CSR的值读入GPR中, 将错误的值暴露出来,
  从而将CSR写入的对比归约为GPR写入的对比.
* 还需要令GPR和CSR在DUT和REF中都有相同的初始状态,
  因此需要对GPR和CSR初始化.
  注意, 这只是形式化验证的要求, 在仿真和流片时, 并非所有GPR和CSR都需要初始化.
* 由于指令序列是BMC由生成的, 如果不加任何限制,
  生成的指令序列中可能会包含非法指令.
  如果NPC不支持非法指令异常的处理, 则NPC执行非法指令的行为是未定义的,
  这将不利于对比指令序列的执行结果.
  为了解决这个问题, 一种思路是只允许BMC生成合法的指令,
  这可以通过Chisel和SystemVerilog皆提供的`assume`语句来实现, 它用于表示验证的前提条件.
  例如, 若`isIllegal`表示"指令译码结果为非法指令",
  则`assume(!isIllegal)`表示将"指令译码结果不为非法指令"作为验证的前提条件,
  此时, BMC将在此前提条件下进行求解, 从而排除指令序列中的非法指令.
* CSR指令中的CSR寄存器地址也需要考虑, 同样可以通过`assume`语句,
  将CSR指令中的CSR寄存器地址限制在已实现的CSR范围内.
* 另一个需要考虑的是不对齐访存, 如果不加任何限制,
  生成的访存指令中计算得到的地址可能是不对齐的.
  这个问题同样可以通过`assume`语句来解决. -->

The above pseudocode only provides a rough framework. There are still many details that you need to pay attention to and supplement:
* In order to reduce the solution space of BMC, the DUT only includes the pipeline itself, without the cache and various peripheral modules.
  However, the load-store delay caused by cache misses can be achieved through handshake signals.
* Instructions are one of the inputs for verifying the top-level module, which means that the BMC will traverse various instructions.
  Under the influence of the bound, the BMC will traverse various combinations of instructions in different cycles,
  which means that all instruction sequences of a given length will be traversed.
* The generated instruction sequence will be input into the IFU of the DUT in order, but since it takes several cycles to complete the execution of an instruction in a pipeline NPC, while it only takes one cycle in a single-cycle NPC,
  it is necessary to synchronize between the DUT and REF so that after the DUT completes the execution of an instruction,
  the instruction is only input into the REF. Therefore, it is necessary to obtain the currently completed instruction in the WBU of the DUT.
* In addition to comparing the written GPR number `rd` and its value `res`, it is also necessary to compare the PC
  to check whether the control flow transfer is correct.
  However, we can choose to compare the new value of the PC, which allows us to detect PC inconsistency errors one cycle earlier.
* Some instructions will not update GPR. For these instructions, we do not need to compare `rd` and `res`.
* For load instructions, the load-store results are also used as one of the inputs to the pipeline, so they need to be reflected in the ports of the top-level module.
  To ensure that the DUT and REF are in the same state after executing the same load instruction, it is also necessary to ensure that they read the same data.
* We do not perform a specific comparison of the execution results of store instructions. There are two main considerations for this:
  1. Both the write data and write address come from GPR. If they are incorrect,
     then there must be an instruction older than the store instruction that wrote the incorrect value to GPR.
     This situation can be checked using the above mechanism.
  2. If the write data and write address are correct, but the AXI write channel signal is incorrect,
     this problem belongs to the scope of AXI bus implementation, not the scope of pipeline verification.
     In principle, checks for AXI signals can also be added to the pipeline verification framework,
     but this will increase the constraints on BMC, thereby increasing the cost of solving.
* The handshake signals between IFU and LSU must be handled correctly.
* We do not perform a specific comparison for CSR writing either, because if there is an error in the implementation of a certain CSR writing,
  the value of that CSR can be read into GPR with an additional instruction, exposing the incorrect value,
  thus reducing the comparison of CSR writing to the comparison of GPR writing.
* It is also necessary to ensure that GPR and CSR have the same initial state in both DUT and REF.
  Therefore, GPR and CSR need to be initialized.
  Note that this is only a requirement for formal verification. During simulation and tape-out, not all GPR and CSR need to be initialized.
* Since the instruction sequence is generated by BMC, if no restrictions are imposed,
  the generated instruction sequence may contain illegal instructions.
  If the NPC does not support the handling of illegal instruction exceptions, the behavior of the NPC executing illegal instructions is undefined,
  which is not beneficial to comparing the execution results of instruction sequences.
  To solve this problem, one idea is to only allow BMC to generate legal instructions.
  This can be achieved by using the `assume` statement provided by both Chisel and SystemVerilog, which is used to express the preconditions for verification.
  For example, if `isIllegal` indicates that “the instruction decoding result is an illegal instruction,”
  then `assume(!isIllegal)` indicates that “the instruction decoding result is not an illegal instruction” as a prerequisite for verification.
  At this point, BMC will solve under this prerequisite, thereby eliminating illegal instructions in the instruction sequence.
* The CSR register addresses in CSR instructions also need to be considered. Similarly, you can use the `assume` statement to restrict the CSR register addresses in CSR instructions to the range of implemented CSRs.
* Another consideration is unaligned memory access. If no restrictions are imposed,
  the addresses calculated in the generated memory access instructions may be unaligned.
  This issue can also be resolved using the `assume` statement.

<!-- > #### hint::更换效率更高的模型检测工具
> 我们在2024/08/20 02:30:00修改了调用形式化验证工具的方式,
> 将对Z3求解器的调用更换成对BtorMC模型检测器的调用, 以此提升形式化验证的效率.
> 如果你使用Chisel进行开发, 请重新参考缓存部分"形式化验证的简单示例"小节的内容. -->

> #### hint::Switching to a more efficient model checking tool
> On 2024/08/20 02:30:00, we modified the way we call the formal verification tool,
> replacing the call to the Z3 solver with a call to the BtorMC model checker to improve the efficiency of formal verification.
> If you are developing with Chisel, please refer to the “Simple Example of Formal Verification” section in the caching section.

<!-- -->
<!-- > #### option::通过形式化验证测试流水线的实现
> 尽管这不是必须的, 我们还是强烈建议你通过形式化验证来测试你的流水线.
> 不过你需要仔细思考`assert`和`assume`这些验证条件如何编写,
> 如果它们的编写不恰当, 可能会导致误报或漏报:
> 误报可以在调试时发现并修复相应的验证条件, 但漏报就难以发现了.
> 因此, 这个任务本质上也是考察大家对流水线细节的理解是否有足够深入.
>
> 此外, 关于BMC的界, 你可以挑选一个合适的参数,
> 使形式化验证工具能遍历足够多的指令序列, 并覆盖各种冒险组合的情况.
> 通常来说, 界可能需要达到10以上, 不过这要求求解过程花费数小时甚至数十小时的时间,
> 但从工具的便利性来说, 这还是非常值得的,
> 因为如果通过人工方式编写测试用例, 即使花费数天的时间,
> 也不一定能够编写出可以覆盖一些极端情况的测试用例.
> 不过为了快速找到一些反例, 你可以从一个小的界开始测试, 然后逐渐测试更大的界. -->

> #### option::Implementing Pipeline Testing with Formal Verification
> Although it is not mandatory, we strongly recommend that you test your pipeline with formal verification.
> However, you need to carefully consider how to write the verification conditions for `assert` and `assume`.
> If they are written incorrectly, it may lead to false positives or false negatives:
> False positives can be found and fixed during debugging, but false negatives are difficult to find.
> Therefore, this task is essentially to test whether everyone has a deep enough understanding of the details of the pipeline.
>
> In addition, regarding the bounds of BMC, you can choose an appropriate parameter
> so that the formal verification tool can traverse a sufficient number of instruction sequences and cover various hazard combinations.
> Generally speaking, the bound may need to be greater than 10, but this requires the solution process to take several hours or even tens of hours.
> However, in terms of the convenience of the tool, it is still very worthwhile,
> because if you write test cases manually, even if it takes several days,
> you may not be able to write test cases that cover some extreme cases.
> However, in order to quickly find some counterexamples, you can start testing with a small bound and then gradually test larger bounds.

<!-- -->
<!-- > #### comment::用形式化验证测试更复杂的处理器
> 中科院软件所的研究团队开发了一套基于形式化验证的RISC-V处理器测试框架,
> 通过它甚至找到了可启动Linux的[NutShell处理器][nutshell]中一些非常隐蔽的bug,
> 具体可参考他们研发的[nutshell-fv项目][nutshell fv].
> 这个案例也反应出形式化验证技术的优势.
> 不过由于目前NPC的功能经过了诸多简化, 为了接入到这个测试框架中,
> 你可能需要对你的代码和测试框架的代码进行一些调整.
> 如果你感兴趣, 可以阅读项目中的README来了解其使用方法,
> 并通过阅读相关代码了解测试框架的相关细节. -->

> #### comment::Using formal verification to test more complex processors
> A research team at the Institute of Software, Chinese Academy of Sciences, has developed a formal verification-based RISC-V processor testing framework.
> Through this framework, they even found some very hidden bugs in the [NutShell Processor][nutshell]that can boot Linux.
> For details, please refer to their [nutshell-fv][nutshell fv] project.
> This case also highlights the advantages of formal verification technology.
> However, since the current NPC functionality has been significantly simplified, to integrate it into this testing framework,
> you may need to make some adjustments to your code and the testing framework's code.
> If you are interested, you can read the README in the project to learn how to use it,
> and review the relevant code to understand the details of the testing framework.

[nutshell fv]: https://github.com/iscas-tis/nutshell-fv/tree/chisel6.4.0-example-with-bug
[nutshell]: https://github.com/OSCPU/NutShell

<!-- ## 让流水线流起来 -->
## Make the pipeline flowing

<!-- 实现简单的流水线处理器后, 我们来讨论如何提升流水线的效率. -->
After implementing a simple pipelined processor, we will discuss how to improve the efficiency of the pipeline.

<!-- > #### todo::评估简单流水线的性能
> 在优化之前, 你需要先评估一下上述简单流水线的性能.
> 和实现流水线之前的多周期相比, 你发现实现流水线后性能有多少提升?
> 如果你发现性能反而倒退了, 我们建议你通过性能计数器深入理解其原因. -->

> #### todo::Evaluate the performance of a simple pipeline
> Before optimization, you need to evaluate the performance of the simple pipeline described above.
> Compared to the multiple cycles before implementing the pipeline, how much improvement in performance did you find after implementing the pipeline?
> If you find that performance has actually declined, we recommend that you use performance counters to gain a deeper understanding of the reasons why.

<!-- 流水线的理想情况是每周期都可以完成一条指令的执行,
但一般情况下流水线的吞吐无法达到理想情况, 主要原因如下:
* 指令供给能力不足, 无法向流水线提供足够的指令
* 数据供给能力不足, 访存指令的执行被阻塞
* 计算效率不足, 由于上述三种冒险的存在, 流水线需要阻塞 -->

The ideal situation for a pipeline is that one instruction can be executed per cycle.
However, in general, the throughput of a pipeline cannot reach the ideal situation. The main reasons are as follows:
* Insufficient instruction supply capacity, unable to provide sufficient instructions to the pipeline
* Insufficient data supply capacity, load-store instruction execution is blocked
* Insufficient computing efficiency, due to the existence of the above three hazards, the pipeline needs to be blocked

<!-- > #### todo::定位性能瓶颈
> 要提升流水线的效率, 我们首先要定位性能瓶颈.
> 尝试在流水线处理器中添加更多性能计数器, 从各种阻塞原因中分析出当前的性能瓶颈. -->

> #### todo::Identify performance bottlenecks
> To improve pipeline efficiency, we must first identify performance bottlenecks.
> Try adding more performance counters to the pipelined processor and analyze the current performance bottlenecks from various causes of blockages.

<!-- -->
<!-- > #### caution::根据你的设计自行决定是否采用以下优化方案
> 下面介绍一些可能有用的优化方案, 具体是否在RTL设计中采用, 取决于很多因素,
> 包括你之前的设计, 对当前性能计数器的分析, 以及剩余的可用面积等.
> 但我们还是要求你在进行RTL实现之前对相应技术的预期性能收益进行评估,
> <font color=red>这对体系结构设计能力来说是非常重要的训练:
> 你可以选择不实现某个优化技术, 但你需要用量化分析方法得出的数据佐证你的决策.</font>
>
> 不过"减少数据冒险的阻塞"对应优化方案是一个重要的知识点, 我们将其作为必做题.
>
> 总之, 如果之前的面积优化工作做得比较好, 现在你就有更多的机会来进行优化了. -->

> #### caution::It is up to you to decide whether or not to use the following optimizations based on your design.
> The following are some optimizations that may be useful, and whether or not to use them in your RTL design will depend on a number of factors,
> including your previous design, an analysis of your current performance counters, and the amount of area left available.
> But we do ask that you evaluate the expected performance benefit of the technique before proceeding with the RTL implementation.
> <font color=red>This is a very important training exercise for architectural design skills:
> You can choose not to implement an optimization technique, but you need to back up your decision with data from quantitative analysis methods. </font>
>
> However, "reducing the blockage of data hazards" is an important point for optimization, and we make it mandatory.
>
> In short, if you've done a good job of area optimization, you now have more opportunities to optimize.

<!-- ### 提升指令供给能力 -->
### Enhancing instruction supply capacity

<!-- 在之前的多周期处理器中, 假设每条指令执行5个周期,
这时, 只要指令供给能力达到0.2条指令/周期, 就能满足多周期处理器的指令消费需求.
但在流水线处理器中, 理想情况下的指令消费需求提升到了1条指令/周期,
如果指令供给能力无法达到相应水平, 就无法发挥流水线的优势.
不过对于icache来说, 缺失时需要访问内存, 此时必定无法达到上述的指令供给能力,
因此我们重点考虑icache在命中时的指令供给能力.
根据icache设计的不同, 是否需要在此处对icache进行优化, 也会有不同的决策. -->

In the previous multi-cycle processor, assuming that each instruction executes for 5 cycles,
as long as the instruction supply capacity reaches 0.2 instructions/cycle, the instruction consumption demand of multi-cycle processor can be satisfied.
However, in pipelined processors, the instruction consumption requirement is ideally increased to 1 instruction/cycle,
if the instruction supply capacity cannot reach the corresponding level, the advantages of pipelining cannot be utilized.
However, for an icache, memory needs to be accessed in the event of cache miss, and the above mentioned instruction supply capacity must not be achieved,
so we focus on the instruction supply capacity of the icache in the event of cache hit.
Depending on the design of the icache, the decision of whether or not to optimize the icache here may vary.

<!-- 具体地, 如果你的icache可以在1周期内判断是否命中, 并在命中的情况下向IFU返回指令,
则icache的指令供给能力已经接近1条指令/周期, 基本满足流水线处理器的指令消费需求.
这种情况下, 你基本上无需进一步提升指令供给能力,
但你可能需要付出频率不高的代价, 毕竟icache需要在一个周期内完成较多的操作,
这也会导致流水线无法在较高频率下工作. -->

Specifically, if your icache can determine whether a cache hit has occurred within 1 cycle and return an instruction to the IFU in the event of cache hit,
then the icache's instruction supply capacity is already close to 1 instruction/cycle, which is basically sufficient for the pipelined processor's instruction consumption demands.
In this case, you basically don't need to further increase the instruction supply capacity,
but you may need to pay the price of low frequency, since the icache needs to complete more operations in a cycle,
which will also lead to the pipeline can't work at a higher frequency.

<!-- 如果你的icache在命中的情况下也需要多个周期才能向IFU返回指令,
则icache的指令供给能力最大为0.5条指令/周期, 甚至更低.
在这样的指令供给能力之下, 流水线的性能会受到明显的约束.
假设icache在命中的情况下需要3个周期才能向IFU返回指令, 则有以下的指令时空图.
由于`cache`的发音和`cash`相同, 因此一些英文文献也用`$`来指代`cache`,
下图为了简化, 也采用`I$`来指代`icache`, 替换之前的`IF`,
因为此时`IF`需要等待的时间与icache的访问时间一致: -->

If your icache also takes multiple cycles to return instructions to the IFU in the event of cache hit,
the maximum instruction supply capacity of the icache is 0.5 instructions/cycle, or even less.
Under this instruction supply capacity, the performance of the pipeline is significantly constrained.
Assuming that it takes 3 cycles for icache to return an instruction to the IFU in the event of cache hit, the following instruction timing diagram is obtained.
Since `cache` is pronounced the same as `cash`, some literatures also use `$` to refer to `cache`,
The following diagram also uses `I$` to refer to `icache` for simplicity, replacing the previous `IF`,
because the waiting time of `IF` is the same as the access time of icache.

```
       T1   T2   T3   T4   T5   T6   T7   T8   T9   T10  T11  T12  T13
     +--------------+----+----+----+----+
 I1  |      I$      | ID | EX | LS | WB |
     +--------------+----+----+----+----+
                    +--------------+----+----+----+----+
 I2                 |      I$      | ID | EX | LS | WB |
                    +--------------+----+----+----+----+
                                   +--------------+----+----+----+----+
 I3                                |      I$      | ID | EX | LS | WB |
                                   +--------------+----+----+----+----+
```

<!-- 要提升icache的指令供给能力, 某种程度上也是要提升icache的吞吐.
这个需求和上文中提到的"提升指令执行的吞吐"非常类似,
很自然地, 我们也可以尝试将icache的访问流水化! -->

To improve the ability of the icache to supply instructions, it is also necessary to improve the throughput of the icache.
This requirement is very similar to the "increase throughput of instruction execution" mentioned above,
Naturally, we can also try to pipeline the icache accesses!

```
       T1   T2   T3   T4   T5   T6   T7   T8   T9
     +----+----+----+----+----+----+----+
 I1  | I$1| I$2| I$3| ID | EX | LS | WB |
     +----+----+----+----+----+----+----+
          +----+----+----+----+----+----+----+
 I2       | I$1| I$2| I$3| ID | EX | LS | WB |
          +----+----+----+----+----+----+----+
               +----+----+----+----+----+----+----+
 I3            | I$1| I$2| I$3| ID | EX | LS | WB |
               +----+----+----+----+----+----+----+
```

<!-- > #### todo::估算icache流水化带来的性能收益
> 尝试根据性能计数器, 粗略估算icache流水化带来的性能收益. -->

> #### todo::Estimating the performance benefit of pipelined icache
> Try to roughly estimate the performance benefit of implementing pipelined icache based on performance counters.

<!-- 上图将icache的访问进一步划分成3个阶段, 并通过流水线技术将这3个阶段在时间上重叠起来,
从而让icache在命中时的吞吐接近1条指令/周期.
为了实现icache的流水化, 你可以根据icache命中时的状态转移过程来设计上述阶段,
这和我们将处理器改成流水线是非常类似的, 甚至比处理器流水线更简单,
因为icache的工作过程不存在控制冒险的概念.
当然, 如果icache发成缺失, 则还是需要阻塞icache流水线,
并通过状态机控制内存的访问和icache的更新.
由于缺失会导致icache的更新, 这可能会导致类似数据冒险的问题,
因此你需要考虑如何正确相应的情况, 具体如何解决, 还是取决于你的实现. -->

The above figure further divides the icache access into 3 phases, and overlaps these 3 phases in time by pipeline technique,
so that the throughput of icache on hit is close to 1 instruction/cycle.
In order to implement pipelined icache, you can design the above phases according to the state transfer process when the icache hits.
This is very similar to how we changed the processor to a pipelined processor, or even simpler than the processor pipeline,
because icache works without happening control hazards.
Of course, if the icache misses, it is still necessary to block the icache pipeline,
and control memory accesses and icache updates through a state machine.
Since a miss causes the icache to be updated, this can lead to problems similar to data hazard,
so you need to think about how to handle the situation correctly, and how to do this depends on your implementation.

<!-- > #### option::实现icache的流水化
> 如果你的icache在命中时需要多个周期才能向IFU返回指令,
> 尝试借鉴指令流水线的思想, 将icache的访问流水化, 从而提升其指令供给能力.
> 实现后, 尝试通过性能计数器和benchmark, 分别评估指令供给能力的提升是否符合预期.
>
> 上文将icache的访问划分成3个阶段, 仅仅是一个示例,
> 你应该根据你的具体设计决定阶段如何划分.
> 如果你之前使用了类似`PipelineConnect()`的方式来实现流水线,
> 你会发现icache的流水化是很容易实现的:
> 你只需要定义好阶段之间需要传递的信息, 就已经实现了接近90%的内容了.
>
> 实现后, 尝试与之前估算的性能提升进行对比, 来检查你的实现是否符合预期. -->

> #### option::Implementing pipelined icache
> If your icache takes multiple cycles to return instructions to the IFU on cache hit,
> try to borrow the idea of an instruction pipeline and pipeline the accesses to the icache to improve its instruction supply capability.
> After implementation, try to evaluate whether the increase in instruction supply capability is as expected by using performance counters and benchmarks, respectively.
>
> The division of icache accesses into three phases above is just an example,
> you should decide how to divide the phases according to your specific design.
> If you've used something like `PipelineConnect()` to implement a pipeline before,
> you'll find that pipelining the icache is very easy to implement:
> You're close to 90% of the way there by just defining the information that needs to be passed between stages.
>
> Once implemented, try comparing it to the previously estimated performance gains to check that your implementation meets expectations.

<!-- ### 减少数据冒险的阻塞 -->
### Reduce data hazard blockings

<!-- 在上述的简单流水线中, 我们是通过阻塞等待的方式来处理RAW数据冒险的,
这一方案需要等待被依赖的寄存器写入新值后, 被阻塞的指令才能继续执行,
显然这样的等待会降低流水线的吞吐. -->

In the simple pipeline described above, we handle RAW data hazards by blocking and waiting.
This approach requires waiting for the dependent registers to be written with new values before the blocked instructions can continue to be executed.
Obviously, such waiting will reduce the throughput of the pipeline.

<!-- 一个观察是, 被依赖寄存器的新值是在WB阶段写入,
但其实这个值最早在EX阶段就已经被计算出来了, 并会随着流水线传递给LS阶段和WB阶段.
因此, 我们可以考虑提前从这些阶段中将计算出的新值拿过来给后续指令使用,
让它们不必等待被依赖寄存器完成更新, 即可拿到正确的源操作数开始执行,
这种技术称为"转发"(forward)或"旁路"(bypass). -->

One observation is that the new value of the dependent register is written in the WB stage,
but in fact, this value has already been calculated in the EX stage and will be passed to the LS stage and WB stage through the pipeline.
Therefore, we can consider taking the calculated new value from these stages in advance for use by subsequent instructions,
so that they do not have to wait for the dependent register to complete the update, and can obtain the correct source operand to start execution.
This technique is called “forwarding” or “bypass.”

```
                    T1   T2   T3   T4   T5   T6   T7   T8
                  +----+----+----+----+----+
I1: add a0,t0,s0  | IF | ID | EX | LS | WB |
                  +----+----+----+----+----+
                                |    |    |
                                V    |    |
                       +----+----+----+----+----+
I2: sub a1,a0,t0       | IF | ID | EX | LS | WB |
                       +----+----+----+----+----+
                                     |    |
                                     V    |
                            +----+----+----+----+----+
I3: and a2,a0,s0            | IF | ID | EX | LS | WB |
                            +----+----+----+----+----+
                                          |
                                          V
                                 +----+----+----+----+----+
I4  xor a3,a0,t1                 | IF | ID | EX | LS | WB |
                                 +----+----+----+----+----+
```

<!-- > #### todo::估算转发技术的理想性能提升
> 可以看到, 转发技术可以很好地消除load-use冒险以外的RAW阻塞.
> 至于load-use冒险, 因为load指令需要等待数据读出后才能转发,
> 在此之前仍然需要阻塞流水线.
>
> 尝试根据性能计数器, 估算转发技术能带来的理想性能提升. -->

> #### todo::Estimating the ideal performance improvement of forwarding technology
> As can be seen, forwarding technology can effectively eliminate RAW blocking other than load-use hazards.
> As for load-use hazards, because load instructions need to wait for data to be read before forwarding,
> the pipeline still needs to be blocked before that.
>
> Try to estimate the ideal performance improvement that forwarding technology can bring based on performance counters.

<!-- 我们先来讨论转发技术对数据通路的修改.
在上述流水线中, 转发源有3个, 分别是EX阶段, LS阶段和WB阶段,
它们都有可能携带可以转发的数据.
但转发并不能无条件进行, 需要转发源满足以下条件:
将要写入寄存器, 待写入的寄存器编号和被依赖寄存器编号一致, 并且数据已经就绪.
前两个条件和上文提到的RAW冒险检测条件一致, 因此可以复用RAW冒险的检测逻辑;
最后一个条件和指令的行为有关, 大部分计算类指令都可以在EX阶段计算出结果,
因此在EX阶段得到的结果即可转发到ID阶段;
但load指令在EX阶段只能计算出访存地址, 因此不应该在EX阶段进行转发,
而且在LS阶段也需要在总线的R通道握手后才能得到返回的数据, 在这之前也不应该进行转发. -->

Let's first discuss how forwarding technology modifies the data path.
In the above pipeline, there are three forwarding sources: the EX stage, the LS stage, and the WB stage.
All of them may carry data that can be forwarded.
However, forwarding cannot be performed unconditionally. The forwarding source must meet the following conditions:
The data is to be written to a register, the register number to be written to is consistent with the dependent register number, and the data is ready.
The first two conditions are consistent with the RAW hazard detection conditions mentioned above, so the RAW hazard detection logic can be reused.
The last condition is related to the behavior of the instruction. Most computational instructions can calculate the result in the EX stage,
so the result obtained in the EX stage can be forwarded to the ID stage.
However, load instructions can only calculate the load-store address in the EX stage, so it should not be forwarded in the EX stage.
Furthermore, in the LS stage, the returned data can only be obtained after the R channel of the bus has been handshaken, so it should not be forwarded before that.

<!-- 除了数据通路的变化, 我们还需要考虑控制通路的修改.
之前在简单流水线处理器中, 一旦检测到RAW冒险, 就会阻塞流水线.
而使用转发技术后, 阻塞流水线的条件将有所变化:
* 如果ID阶段未检测到RAW冒险, 则无需阻塞流水线, 这与上文的简单流水线一致
* 如果ID阶段检测到了RAW冒险, 并且存在某个阶段满足转发条件,
  则无需阻塞流水线, 同时将转发的数据作为ID阶段的输出
* 如果ID阶段检测到了RAW冒险, 但不存在满足转发条件的阶段, 则需要阻塞流水线 -->

In addition to changes in the data path, we also need to consider modifications to the control path.
Previously, in simple pipeline processors, once a RAW hazard was detected, the pipeline would be blocked.
With the use of forwarding technology, the conditions for blocking the pipeline will change:
* If no RAW hazard is detected in the ID stage, there is no need to block the pipeline, which is consistent with the simple pipeline described above.
* If the ID stage detects a RAW hazard and there is a stage that meets the forwarding conditions,
  there is no need to block the pipeline, and the forwarded data is treated as the output of the ID stage.
* If the ID stage detects a RAW hazard but there is no stage that meets the forwarding conditions, the pipeline needs to be blocked.

<!-- 特别地, 如果存在多条指令同时满足转发条件, 则需要仔细考量.
考虑以下指令序列: -->

In particular, if there are multiple instructions that satisfy the forwarding conditions at the same time, careful consideration is required.
Consider the following instruction sequence:
```
                      T1   T2   T3   T4   T5   T6   T7   T8
                    +----+----+----+----+----+
I1: add a0, a0, a1  | IF | ID | EX | LS | WB |
                    +----+----+----+----+----+
                         +----+----+----+----+----+
I2: add a0, a0, a1       | IF | ID | EX | LS | WB |
                         +----+----+----+----+----+
                              +----+----+----+----+----+
I3: add a0, a0, a1            | IF | ID | EX | LS | WB |
                              +----+----+----+----+----+
                                   +----+----+----+----+----+
I4: add a0, a0, a1                 | IF | ID | EX | LS | WB |
                                   +----+----+----+----+----+
```

<!-- 假设I1不依赖于比它更老的指令, 则不必阻塞I1的执行;
对于I2, 它依赖于I1的结果, 但通过转发技术,
可以在T3时刻将I1在EX阶段的结果转发给位于ID阶段的I2, 因此也不必阻塞I2的执行;
对于I3, 在T4时刻发现, 位于EX阶段的I2和位于LS阶段的I1均满足转发条件,
但从ISA状态机的视角考虑, 指令是串行执行的, 因此I3读出的`a0`应该是最近一次写入`a0`的结果,
因此应该选择由位于EX阶段的I2进行转发;
同理, 对于I4, 在T5时刻, 位于EX阶段的I3, 位于LS阶段的I2和位于WB阶段的I1均满足转发条件,
但从ISA状态机的视角考虑, 应该选择由位于EX阶段的I3进行转发.
也即, 存在多条指令同时满足转发条件时, 应该选择最年轻的指令进行转发. -->

Assuming that I1 does not depend on instructions older than itself, there is no need to block the execution of I1.
As for I2, it depends on the result of I1, but through forwarding technology,
the result of I1 in the EX stage can be forwarded to I2 in the ID stage at time T3, so there is no need to block the execution of I2.
For I3, at time T4, it is found that I2 in the EX stage and I1 in the LS stage both satisfy the forwarding conditions,
but from the perspective of the ISA state machine, the instructions are executed serially, so the `a0` read by I3 should be the result of the most recent write to `a0`,
so I2 in the EX stage should be selected for forwarding;
Similarly, for I4, at time T5, I3 in the EX stage, I2 in the LS stage, and I1 in the WB stage all satisfy the forwarding conditions,
but from the perspective of the ISA state machine, I3 in the EX stage should be selected for forwarding.
That is, when multiple instructions satisfy the forwarding conditions at the same time, the youngest instruction should be selected for forwarding.

<!-- > #### todo::实现转发技术
> 根据上述内容, 在流水线中实现转发技术, 从而消除大部分RAW冒险.
> 实现后, 尝试与之前估算的性能提升进行对比, 来检查你的实现是否符合预期. -->

> #### todo::Implementing forwarding technology
> Based on the above, implement forwarding technology in the pipeline to eliminate most RAW hazards.
> After implementation, compare the results with the previously estimated performance improvement to check whether your implementation meets expectations.

<!-- -->
<!-- > #### option::教科书上的转发方案
> 上述转发方案是将其他阶段的计算结果转发到ID阶段后,
> 再选出正确的操作数送入流水段寄存器,
> 而大部分教科书上的转发方案则是转发到EX和LS阶段,
> 然后在计算之前选出正确的操作数.
>
> 尝试对比这两种方案, 如果你想不清楚, 可以分别实现这两种方案,
> 然后从IPC, 频率, 面积等各个方面对比它们. -->

> #### option::Forwarding solutions in textbooks
> The above forwarding solution forwards the calculation results from other stages to the ID stage,
> then selects the correct operand and sends it to the pipeline segment register.
> Most forwarding solutions in textbooks forward to the EX and LS stages,
> then select the correct operand before calculation.
>
> Try comparing these two schemes. If you are unsure, you can implement both schemes separately,
> then compare them in terms of IPC, frequency, area, and other aspects.

<!-- ### 减少控制冒险的阻塞 -->
### Reduce control hazard blockings

<!-- 在上述简单流水线中, 我们用推测执行来应对控制冒险.
具体地, 我们推测"接下来总是执行下一条静态指令",
如果推测正确, 就能消除控制冒险带来的阻塞.
但事实上, 上述推测并非总是正确, 此时需要冲刷流水线, 从而浪费了若干周期.
要提升流水线的执行效率, 一个角度是降低冲刷流水线带来的负面影响. -->

In the simple pipeline described above, we use speculative execution to deal with control hazards.
Specifically, we speculate that “the next static instruction will always be executed.”
If the speculation is correct, the blocking caused by control hazards can be eliminated.
However, in reality, the above speculation is not always correct. In this case, the pipeline needs to be flushed, wasting several cycles.
One way to improve the execution efficiency of the pipeline is to reduce the negative impact of flushing the pipeline.

<!-- > #### todo::估算优化控制冒险相关阻塞带来的性能收益
> 尝试根据性能计数器, 粗略估算完全消除控制冒险相关阻塞时的性能,
> 从而相应优化技术的理想性能收益. -->

> #### todo::Estimate the performance gains associated with eliminating control hazards.
> Attempt to roughly estimate the performance gains associated with completely eliminating control hazards based on performance counters,
> thereby optimizing the ideal performance gains of the technology.

<!-- 要降低冲刷流水线对处理器执行效率带来的负面影响, 可以从以下两个方向考虑:
1. 降低单次冲刷流水线的代价. 这需要尽快计算出分支指令的结果.
   有的教科书中介绍了在ID阶段计算分支结果的方案, 这显然可以提升流水线的IPC,
   但还需要从频率和面积的角度综合评估这一方案.
1. 降低冲刷流水线的次数. 这需要提升推测的准确率.
   根据上文的分析, "推测异常不发生"的准确率已经接近100%,
   因此主要考虑分支指令和跳转指令的推测准确率. -->

To reduce the negative impact of flushing the pipeline on processor execution efficiency, the following two approaches can be considered:
1. Reduce the cost of a single pipeline flush. This requires calculating the results of branch instructions as quickly as possible.
   Some textbooks describe a solution that calculates branch results during the ID phase, which can clearly improve pipeline IPC,
   but this method needs to be evaluated comprehensively from the perspectives of frequency and area.
1. Reduce the number of pipeline flushes. This requires improving the accuracy of speculation.
   According to the above analysis, the speculation accuracy of “no exceptions will happen” is already close to 100%,
   so the main consideration is the accuracy of branch instruction and jump instruction speculation.

<!-- 通常通过"分支预测"(branch prediction)技术来提升分支指令的推测准确率,
进行分支预测的模块称为分支预测器(branch predictor).
分支指令的执行结果只有"跳转"(taken)和"不跳转"(not taken),
因此分支预测技术只需要在两个选择中预测一个即可,
具体如何给出一个预测结果, 称为分支预测算法.
考虑是否参考运行时刻的信息, 分支预测算法分为静态预测和动态预测两种.
这里我们先介绍静态预测算法, 我们将在A阶段介绍动态预测算法. -->

Branch prediction technology is commonly used to improve the accuracy of branch instruction speculation.
The module that performs branch prediction is called a branch predictor.
The execution results of branch instructions are either “taken” or “not taken.”
Therefore, branch prediction technology only needs to predict one of the two choices.
The specific method of giving a prediction result is called a branch prediction algorithm.
Considering whether to refer to runtime information, branch prediction algorithms are divided into static prediction and dynamic prediction.
Here, we will first introduce the static prediction algorithm, and we will introduce the dynamic prediction algorithm in A Stage.

<!-- > #### todo::体会现代处理器中分支预测器的重要性
> 尝试统计动态指令数中分支指令的占比,
> 即, 假设平均每x条指令中包含一条分支指令, 求x.
>
> 假设在某理想的五级流水线处理器中, 指令供给能力为1条/周期,
> 无结构冒险和数据冒险, 访存延迟均为0周期, 跳转指令总能预测正确,
> 但分支指令可能会预测错误, 且分支指令在EX阶段计算出跳转结果.
> 根据上文求得的x, 分别计算处理器在不同分支预测准确率下的IPC:
> 100%, 99.5%, 99%, 95%, 90%, 80%.
>
> 将上述处理器改进为乱序单发射15级流水, 其中分支指令在第13级计算出跳转结果,
> 其他假设不变, 重新计算处理器在不同分支预测准确率下的IPC.
>
> 继续将处理器改进为乱序四发射15级流水, 其他假设不变,
> 重新计算处理器在不同分支预测准确率下的IPC. -->

> #### todo::Understand the importance of branch prediction in modern processors.
> Try to calculate the proportion of branch instructions in the dynamic instruction count.
> That is, assuming that on average every x instructions contain one branch instruction, find x.
>
> Assume that in an ideal five-stage pipeline processor, the instruction supply capacity is 1 instruction per cycle,
> there are no structural hazards or data hazards, the load-store latency is 0 cycles, and jump instructions can always be predicted correctly,
> but branch instructions may be predicted incorrectly, and branch instructions calculate the jump result in the EX stage.
> Based on the x obtained above, calculate the IPC of the processor under different branch prediction accuracies:
> 100%, 99.5%, 99%, 95%, 90%, and 80%.
>
> Modify the above processor to a 15-stage out-of-order single-issue pipeline, in which the branch instruction calculates the jump result at stage 13.
> Keep the other assumptions unchanged and recalculate the IPC of the processor under different branch prediction accuracy rates.
>
> Continue to improve the processor to a 15-stage out-of-order four-issue pipeline. Keep the other assumptions unchanged and
> recalculate the IPC of the processor under different branch prediction accuracy rates.

<!-- 静态预测只根据分支指令本身来进行预测.
由于分支指令本身在程序执行过程中不会发生变化,
因此对于给定的一条分支指令和给定的一种静态预测算法, 其预测结果总是相同的.
上文的"总是推测接下来执行下一条静态指令", 站在分支预测技术的角度,
就是"总是不跳转", 是一种静态预测算法.
另一种静态预测算法是"总是跳转". -->

Static prediction is based solely on the branch instruction itself.
Since the branch instruction itself does not change during program execution,
the prediction result is always the same for a given branch instruction and a given static prediction algorithm.
The above “always speculating that the next static instruction will be executed” is, from the perspective of branch prediction technology,
“always not jumping,” which is a static prediction algorithm.
Another static prediction algorithm is “always jump.”

<!-- 事实上, 根据分支跳转方向的不同, 分支指令的执行结果是否跳转, 是有偏向性的(bias).
这其实和程序中循环的行为有关, 例如,
当分支的跳转目标位于前方(年轻指令)时, 可能是一个循环的出口, 因此偏向不跳转;
而当分支的跳转目标位于后方(年老指令)时, 可能是要重新执行循环体, 因此偏向跳转.
一种利用这一特性的静态预测算法称为BTFN(Backward Taken, Forward Not-taken):
若分支的跳转目标位于后方, 则预测跳转, 反之则预测不跳转.
实现时, 只需要根据B型指令offset的符号位, 即可得到预测结果.
事实上, RISC-V手册也建议编译器按照BTFN的模式生成代码: -->

In fact, depending on the direction of the branch jump, there is a bias in whether or not the branch instruction jumps.
This is actually related to the behavior of loops in the program. For example,
when the branch jump target is located ahead (young instruction), it may be a loop exit, so there is a bias not to jump;
when the branch jump target is located behind (old instruction), it may be a re-execution of the loop body, so there is a bias to jump.
A static prediction algorithm that utilizes this characteristic is called BTFN (Backward Taken, Forward Not-taken):
if the branch jump target is located behind, then predict a jump; otherwise, predict no jump.
When implementing this, you only need to obtain the prediction result based on the sign bit of the B-type instruction offset.
In fact, the RISC-V manual also recommends that compilers generate code according to the BTFN pattern:
```
Software should also assume that backward branches will be predicted taken and
forward branches as not taken, at least the first time they are encountered.
```

<!-- 评价一个分支预测算法的重要指标就是预测准确率.
和之前的icache类似, 对于给定的程序, 需要执行哪些分支指令,
以及每条分支指令的执行结果, 都是固定的. 只要获得程序运行的itrace,
我们就能快速统计一个分支预测算法的准确率, 没有必要进行RTL层次的仿真.
更进一步地, itrace已经包含了完整的指令流,
除分支以外其他指令的trace, 对分支指令的执行结果没有影响,
因此我们真正需要的, 只有分支指令的trace, 我们称为btrace(branch trace). -->

An important indicator for evaluating a branch prediction algorithm is prediction accuracy.
Similar to the previous icache, for a given program, the branch instructions that need to be executed
and the execution results of each branch instruction are fixed. As long as we obtain the itrace of the program running,
we can quickly calculate the accuracy of a branch prediction algorithm without the need for RTL-level simulation.
Furthermore, itrace already contains the complete instruction stream,
and the trace of instructions other than branches has no effect on the execution result of branch instructions.
Therefore, what we really need is only the trace of branch instructions, which we call btrace (branch trace).

<!-- 根据上述分析, 我们只需要实现一个分支预测器的功能模拟器, 我们称它为branchsim.
branchsim接收btrace, 并根据分支预测算法预测出每一条分支指令是否跳转,
然后与btrace中记录的执行结果进行对比, 统计该算法的预测准确率.
至于btrace, 我们可以通过NEMU来快速生成. -->

Based on the above analysis, we only need to implement a branch prediction simulator, which we call branchsim.
branchsim receives btrace and predicts whether each branch instruction will jump based on the branch prediction algorithm.
It then compares the results with the execution results recorded in btrace and calculates the prediction accuracy of the algorithm.
As for btrace, we can quickly generate it using NEMU.

<!-- > #### todo::实现branchsim
> 根据上文的介绍, 实现一个简单的分支预测模拟器branchsim,
> 然后分别评估上述静态预测算法的准确率,
> 并根据预测准确略评估该预测算法带来的性能收益.
>
> 如果你对动态预测算法感兴趣, 可以先学习相关资料,
> 然后在branchsim中实现相应的算法并评估其准确率.
>
> 和cachesim类似, branchsim也可以作为分支预测性能表现的REF.
> 例如, 上文的简单流水线中采用了"总是不跳转"的静态预测算法,
> 相关的性能计数器应该与branchsim给出的统计结果完全一致. -->

> #### todo::Implement branchsim
> Based on the above introduction, implement a simple branch prediction simulator branchsim,
> then evaluate the accuracy of the above static prediction algorithms,
> and evaluate the performance gains brought by the prediction algorithm based on the prediction accuracy.
>
> If you are interested in dynamic prediction algorithms, you can first study the relevant materials,
> then implement the corresponding algorithms in branchsim and evaluate their accuracy.
>
> Similar to cachesim, branchsim can also be used as a REF for branch prediction performance.
> For example, the simple pipeline above uses a static prediction algorithm that “always jumps,”
> and the relevant performance counters should be completely consistent with the statistics provided by branchsim.

<!-- 通过branchsim选出一个表现较好的分支预测算法后,
就可以考虑如何在处理器中实现一个分支预测器了.
通常来说, 分支预测器的预测结果需要提供给IFU使用:
如果预测跳转, 则让IFU从分支指令的跳转目标处取指,
否则, 则让IFU从`PC + 4`处取指.
但实际上, 我们在ID阶段才能得知一条指令是否为分支指令,
如果为分支指令, 也需要在ID阶段才能得知其跳转目标.
而在IF阶段中, 我们只有PC值, 难以获得上述信息来进行分支预测. -->

After selecting a branch prediction algorithm with good performance through branchsim,
we can consider how to implement a branch predictor in the processor.
Generally speaking, the prediction results of the branch predictor need to be provided to the IFU for use:
if a jump is predicted, the IFU fetches the instruction from the jump target of the branch instruction;
otherwise, the IFU fetches the instruction from `PC + 4`.
But in fact, we can only know whether an instruction is a branch instruction at the ID stage,
and if it is a branch instruction, we also need to know its jump target at the ID stage.
In the IF stage, we only have the PC value, and it is difficult to obtain the above information for branch prediction.

<!-- 解决上述问题的方法是通过一张表来维护PC和分支跳转目标的对应关系,
这张表称为BTB(Branch Target Buffer).
BTB可以看作一个特殊的cache, 它通过PC值进行索引,
若命中, 表示该PC对应一条分支指令, 可从中读出跳转目标;
若缺失, 表示该PC对应的指令不是分支指令, 此时让IFU从`PC + 4`处取指即可.
此外, BTB需要在处理器执行过程中进行填充更新,
原则上最早可以在ID阶段中译码出分支指令时更新BTB.
通常BTB的项数有限, 若更新时发现无空闲表项, 根据局部性原理, 此时应该覆盖旧表项.
特别地, 处理器复位时, BTB中的表项均无效, 此时无法读出正确的跳转目标.
但由于分支预测属于推测执行技术, 预测错误也不影响处理器执行程序的正确性,
等到往BTB中写入正确的表项后, 即可进行有效的预测. -->

The solution to the above problem is to maintain a table that maps PC values to branch target addresses.
This table is called the Branch Target Buffer (BTB).
The BTB can be regarded as a special cache indexed by the PC value.
If there is a hit, it means that the PC corresponds to a branch instruction, and the jump target can be read from it.
If there is a miss, it means that the instruction corresponding to the PC is not a branch instruction, and at this point, the IFU can fetch the instruction from PC + 4.
In addition, the BTB needs to be filled and updated during processor execution.
In principle, the BTB can be updated as early as when the branch instruction is decoded in the ID stage.
Usually, the number of items in the BTB is limited. If no free table items are found during the update, according to the principle of locality, the old table items should be overwritten at this time.
In particular, when the processor is reset, all entries in the BTB are invalid, and the correct jump target cannot be read at this time.
However, since branch prediction is a speculative execution technique, prediction errors do not affect the correctness of the processor's program execution.
Once the correct entries are written to the BTB, effective prediction can be performed.

```txt
              tag     target
           +-------+----------+   Branch Target Buffer
+----+     +-------+----------+
| PC |---> +-------+----------+
+-+--+     +-------+----------+
  |        +-------+----------+
  |            |         | branch               predicted
  |            v         | target +-----------+  next PC  +-----+
  |          +----+      +------->|  branch   |---------->| IFU |
  +--------->| == |-------------->| predictor |           +-----+
             +----+    is branch  +-----------+
```

<!-- > #### option::实现分支预测器
> 由于不同分支的信息可能会在项数有限的BTB中互相替换, 因此在进行分支预测时,
> 当前PC对应的分支指令的跳转目标并非总是可获得的.
> 这种情况将会影响分支预测的准确率,
> 因此你需要在branchsim中添加一个BTB, 来校准其预测准确率.
>
> 具体地, 先在RTL中实现一个简单的BTB, 组织方式不限,
> 你可以根据需要选择采用直接映射, 全相联还是组相联.
> 你也可以根据你的需要, 在BTB中添加新的字段.
> 实现后, 评估其面积, 根据剩余的面积选择合适的BTB项数,
> 然后根据这一项数调整branchsim, 重新评估分支预测算法的准确率.
>
> 重新评估后, 再综合所有因素决定如何实现分支预测器.
> 实现后, 将RTL的分支预测准确率与branchsim中统计的分支预测准确率进行对比. -->

> #### option::Implementing a branch predictor
> Since information from different branches may be replaced in a BTB with a limited number of entries, when performing branch prediction,
> the jump target of the branch instruction corresponding to the current PC is not always available.
> This situation will affect the accuracy of branch prediction,
> so you need to add a BTB to branchsim to adjust its prediction accuracy.
>
> Specifically, first implement a simple BTB in RTL, with no restrictions on its organization.
> You can choose to use direct mapping, fully associative, or set-associative based on your needs.
> You can also add new fields to the BTB according to your requirements.
> After implementation, evaluate its area and select an appropriate number of BTB entries based on the remaining area.
> then adjust branchsim according to this number of items, and re-evaluate the accuracy of the branch prediction algorithm.
>
> After re-evaluation, consider all factors to decide how to implement the branch predictor.
> After implementation, compare the branch prediction accuracy of RTL with the branch prediction accuracy statistics in branchsim.

<!-- 上文介绍的是分支指令的预测, 对于跳转指令, 也需要进行推测执行.
跳转指令都是无条件的, 但跳转结果有多种, 因此对跳转指令的预测主要是预期其跳转目标.
跳转指令又分直接跳转`jal`和间接跳转`jalr`.
对于给定的`jal`指令, 其跳转目标是确定的,
因此只要将`jal`指令的跳转目标记录到BTB中, 只要相应表项未被替换,
下次遇到该`jal`指令时就能以100%的准确率取到正确的跳转目标处的指令.
至于`jalr`指令, 其跳转目标由执行该指令时的源寄存器的值决定,
仅靠静态预测算法难以预测成功, 需要考虑相应的动态预测算法.
为简单起见, 目前可对`jalr`指令采用"总是不跳转"的预测算法. -->

The above describes branch instruction prediction. Jump instructions also require speculative execution.
Jump instructions are unconditional, but there are multiple jump results, so jump instruction prediction mainly involves predicting the jump target.
Jump instructions are divided into direct jumps `jal` and indirect jumps `jalr`.
For a given `jal` instruction, the jump target is determined.
Therefore, as long as the jump target of the `jal` instruction is recorded in the BTB, and as long as the corresponding table entry is not replaced,
the next time the `jal` instruction is encountered, the instruction at the correct jump target can be obtained with 100% accuracy.
As for the `jalr` instruction, its jump target is determined by the value of the source register when the instruction is executed.
It is difficult to predict successfully with only a static prediction algorithm, so a corresponding dynamic prediction algorithm needs to be considered.
For simplicity, a prediction algorithm that “always does not jump” can be used for the `jalr` instruction at present.

<!-- > #### todo::估算跳转指令推测执行正确的性能收益
> 尝试根据性能计数器, 分别估算`jal`指令和`jalr`指令推测执行正确时的理想性能收益. -->

> #### todo::Estimating the correct performance gain of speculative execution of jump instructions
> Try to estimate the ideal performance gain when the `jal` and `jalr` instructions are executed correctly based on the performance counter.

<!-- 如果要对`jal`指令的跳转目标进行预测, 你可以让`jal`指令与分支指令共享同一个BTB,
也可以让`jal`指令单独使用另一个BTB.
前者可以节约面积, 但指令之间的表项可能会互相覆盖, 从而影响预测准确率; 后者则相反.
你可以根据branchsim的评估情况决定如何设计. -->

If you want to predict the jump target of the `jal` instruction, you can let the `jal` instruction share the same BTB with the branch instruction,
or you can let the `jal` instruction use another BTB separately.
The former can save space, but the table entries between instructions may overwrite each other, thereby affecting the prediction accuracy; the latter is the opposite.
You can decide how to design based on the evaluation of branchsim.


<!-- > #### option::实现jal指令的跳转目标预测
> 根据上述方案, 实现`jal`指令的跳转目标预测. -->

> #### option::Implementing jump target prediction for jal instructions
> Based on the above plan, implement jump target prediction for `jal` instructions.

<!-- -->
<!-- > #### option::实现ret指令的跳转目标预测
> 一般来说, `jalr`指令的跳转目标比较难预测正确,
> 但作为一类特殊的`jalr`指令, `ret`指令是比较容易预测正确的.
> 如果你感兴趣, 并且有较为充足的剩余面积,
> 可以查阅"返回地址栈"(Return Address Stack)的相关资料,
> 并在处理器中实现`ret`指令的跳转目标预测. -->

> #### option::Predicting the jump target of the ret instruction
> Generally speaking, it is difficult to correctly predict the jump target of the `jalr` instruction.
> However, as a special type of `jalr` instruction, the `ret` instruction is relatively easy to predict correctly.
> If you are interested and have sufficient remaining space,
> you can refer to the relevant information on the “return address stack”
> and implement jump target prediction for the `ret` instruction in the processor.

<!-- > #### todo::优化流水线
> 根据分析出的性能瓶颈, 在满足面积约束的前提下,
> 尝试将有限的面积资源投入到最值得投入的优化技术中, 尽可能提升处理器的性能. -->

> #### todo::Optimize the pipeline
> Based on the analyzed performance bottlenecks, and subject to area constraints,
> attempt to invest limited area resources in the most worthwhile optimization technologies to maximize processor performance.

<!-- -->
<!-- > #### caution::重新审视处理器的性能优化
> 我们能预料到, 在完成这一部分内容的时候, 你并不会感到十分愉快:
> 要么可用面积非常紧张, 难以添加优化技术;
> 要么添加优化技术后处理器频率有所下降, 抵消了该技术对IPC带来的提升;
> 要么投入很大精力之后, 终于使得面积和频率的表现都不错, 但发现IPC的提升很小......
>
> 更重要的是, 你应该会在心中隐隐感觉到一种无奈:
> 流水线作为组成原理教科书上的技术巅峰, 它的性能表现就这?
> 事实上, 这种技术巅峰的幻觉来源于教科书上对指令供给和数据供给的大幅削弱,
> 让你误以为计算效率就是处理器设计的一切,
> 让你觉得乱序多发射就是处理器设计的终极追求.
>
> 事实上, 你的这一体验恰恰反映了计算机系统领域著名的["内存墙"(memory wall)][memory wall]:
> 存储器的性能严重影响了CPU性能的发挥.
> 从理论上来说, 将一个多周期处理器改进为流水线处理器, 应该能得到接近5倍的性能提升,
> 但在一个包含现代存储器的系统中, 最终的性能表现还与存储器的性能表现密切相关.
> 而Amdahl's law其实预言了你的无奈:
> 如果访存跟不上, CPU的计算能力就算再快, 也是徒然.
>
> 因此, 你需要从"学会流水线, 就具备了体系结构设计能力"的幻想中清醒过来,
> 认识到性能计数器, Amdahl's law, 模拟器等等对体系结构设计的意义,
> 学会在面积, 主频和IPC之间作出合理的取舍, 甚至能找到一个各方面表现都不错的方案,
> 这才是一名合格的架构师需要具备的能力.
>
> 而上述的这些能力, 是无法仅仅通过将书上的架构图翻译成RTL代码来获得的.
> 如果你只是参考一些书籍的代码, 就更别想了.
> 相反, 我们之所以按照当前的顺序来组织讲义内容,
> 就是希望大家早日从上面的幻觉中醒悟过来, 在SoC中面对内存墙的现实,
> 然后学会用科学的方法一点一点探索出合理的处理器优化方案.
>
> 从结果上看, 你设计的处理器性能可能还很弱,
> 但如果你完成了我们设置的训练, 你就已经锻炼出真正的体系结构设计能力:
> 你学会分析性能瓶颈, 学会从中思考出新方案, 学会去估算其性能收益,
> 学会在各种约束下将这个方案实现出来, 学会去评测你的实现是否符合预期......
> 这些比你仅仅会用RTL写一个流水线处理器重要得多.
>
> 因此, 你也可以通过以下方法检验自己是否具备体系结构设计能力:
> 如果你脱离了参考书就不知道应该做什么, 或者需要询问他人才知道一个方案的优劣,
> 那就是不具备体系结构设计能力. -->

> #### caution::Re-evaluating processor performance optimization
> We can anticipate that you may not be particularly pleased when completing this section:
> Either the available area is extremely limited, making it difficult to add optimization techniques;
> Or adding optimization techniques results in a decrease in processor frequency, offsetting the IPC improvements provided by the technique;
> Or, after investing a lot of effort, you finally achieve good performance in terms of area and frequency, but find that the improvement in IPC is very small...
>
> More importantly, you should feel a sense of helplessness in your heart:
> As the peak of technology in textbooks on architecture principles, is this the performance of pipelines?
> In fact, this illusion of technological peak comes from the significant weakening of instruction supply and data supply in textbooks,
> which misleads you into thinking that computing efficiency is everything in processor design,
> and makes you think that out-of-order multiple issue is the ultimate pursuit of processor design.
>
> In fact, your experience reflects the well-known [memory wall][memory wall] in the field of computer systems:
> The performance of memory severely limits the performance of the CPU.
> Theoretically, upgrading a multi-cycle processor to a pipeline processor should result in a performance improvement of nearly five times.
> However, in a system that includes modern memory, the final performance is closely related to the performance of the memory.
> Amdahl's law actually predicts your helplessness:
> If the load-store cannot keep up, no matter how fast the CPU's computing power is, it will be futile.
>
> Therefore, you need to wake up from the illusion that “learning the pipeline will give you the ability to design architecture.”
> You need to recognize the significance of performance counters, Amdahl's law, simulators, etc. for architecture design.
> You need to learn to make reasonable trade-offs between area, clock speed, and IPC, and even find a solution that performs well in all aspects.
> This is the capability a qualified architect must possess.
>
> However, these skills cannot be acquired simply by translating the architectural diagrams in books into RTL code.
> If you merely refer to the code in some books, forget about it.
> On the contrary, the reason we have organized the lecture content in the current order
> is to help you awaken from the illusion mentioned above and confront the reality of the memory wall in SoCs,
> and then learn to scientifically explore reasonable processor optimization solutions step by step.
>
> From the results, the performance of the processor you designed may still be weak,
> but if you have completed the training we set, you have already developed real architectural design capabilities:
> you have learned to analyze performance bottlenecks, think of new solutions, estimate performance gains,
> implement these solutions under various constraints, and evaluate whether your implementation meets expectations...
> These are much more important than just being able to write a pipeline processor using RTL.
>
> Therefore, you can also test whether you have architectural design capabilities using the following methods:
> If you don't know what to do without referring to reference books, or if you need to ask others to determine the merits of a plan,
> then you do not have architectural design capabilities.

[memory wall]: https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall

<!-- ## `fence.i`的处理 -->
## Handling `fence.i`

<!-- 最后, 我们还需要对`fence.i`指令在流水线中的执行进行额外的处理.
回顾`fence.i`的语义, 其行为是让在其之后的取指操作都能看到在其之前的store指令修改的数据.
我们已经在实现icache的时候, 通过对icache进行特殊处理,
来保证后续取指操作不会通过icache取到过时的指令. -->

Finally, we need additional processing of the execution of the `fence.i` instruction in the pipeline.
Recall that the semantics of `fence.i` are meant to allow subsequent fetch operations to see the data modified by the previous store instruction.
We have implemented icache with a special treatment
to ensure that subsequent fetch operations do not fetch stale instructions through the icache.

<!-- 在流水线处理器中, 过时的指令可能会存在于流水线中.
考虑如下示例: 假设`fence.i`在EX阶段生效, 但作为比`fence.i`年轻的指令,
I3和I4已经取出并在流水线中, 它们有可能是过时的,
继续执行它们可能会导致CPU状态机的转移结果与ISA状态机不一致, 从而造成错误. -->

In a pipelined processor, stale instructions may exist in the pipeline.
Consider the following example: Suppose `fence.i` is in effect during the EX stage, but as instructions younger than `fence.i`,
I3 and I4 have already been taken out and are in the pipeline, they may be stale, and continuing to
execute them may result in a transfer of the CPU state machine that does not match that of the ISA state machine, thus causing an error.

```
                 T1   T2   T3   T4   T5   T6   T7
               +----+----+----+----+----+
I1: add        | IF | ID | EX | LS | WB |
               +----+----+----+----+----+
                    +----+----+----+----+----+
I2: fence.i         | IF | ID | EX | LS | WB |
                    +----+----+----+----+----+
                         +----+----+
I3: ??? may be stale     | IF | ID |
                         +----+----+
                              +----+
I4: ??? may be stale          | IF |
                              +----+
                                   +----+----+----+----+----+
I5: sub                            | IF | ID | EX | LS | WB |
                                   +----+----+----+----+----+
```

<!-- > #### todo::设计反例
> 根据上述分析, 设计一个`fence.i`相关的测试用例,
> 使得该测试用例在之前的多周期处理器中运行正确, 但在流水线处理器中运行出错. -->

> #### todo::Design a counterexample
> Based on the above analysis, design a `fence.i` related test case,
> so that the test case runs correctly in the previous multi-cycle processor, but runs incorrectly in the pipelined processor.

<!-- 解决上述问题的方案也很简单: 既然上述指令是不应该被执行的, 将其冲刷掉即可.
实现时, 可以复用推测执行错误时的冲刷逻辑. -->

The solution to the above problem is simple: since the above instruction should not be executed, just flush it.
The implementation can reuse the logic of flushing in case of speculative execution errors.

<!-- > #### todo::在流水线中正确实现fence.i
> 在执行`fence.i`时冲刷流水线, 然后重新运行上述测试用例.
> 如果你的实现正确, 测试用例将运行成功. -->

> #### todo::Correctly implementence.i in the pipeline
> Flush the pipeline while executing `fence.i`, then re-run the above test case.
> If your implementation is correct, the test case will run successfully.
