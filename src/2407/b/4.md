<!-- # B4 性能优化和简易缓存 -->
# B4 Performance Optimization and Simple Cache
<!--
TODO: 补充关于zicntr和zihpm的介绍, 并让学生查阅ecall和ebreak是否算入instret CSR
-->


<!-- > #### danger::更新ysyxSoC
> SoC团队已经提供了32位的流片SoC, 我们也在2024/07/26 13:00:00把`ysyxSoC`修改成32位,
> 以帮助大家在接入流片SoC之前在本地环境中进行测试.
> 如果你在上述时间之前获取`ysyxSoC`的代码, 请进行以下步骤:
> 1. 执行以下命令获取新的32位`ysyxSoC`:
>    ```bash
>    cd ysyxSoC
>    git pull origin master # 你可能需要解决一些代码冲突
>    ```
> 1. 将NPC顶层的AXI数据位宽修改为32位, 并去除相关的数据位宽转换代码
> 1. 重新使用32位的`ysyxSoC`环境进行仿真 -->
> #### danger::Update ysyxSoC
> The SoC team has provided a 32-bit tape-out SoC, and we have also modified `ysyxSoC` to 32-bit on 2024/07/26 at 13:00:00,
> to help everyone test locally before connecting to the tape-out SoC.
> If you obtained the `ysyxSoC` code before the above time, please perform the following steps:
> 1. Execute the following command to fetch the new 32-bit `ysyxSoC`:
>    ```bash
>    cd ysyxSoC
>    git pull origin master # You may need to resolve some code conflicts
>    ```
> 1. Change the AXI data width of the top NPC to 32-bit and remove the related data width conversion code.
> 1. Re-run simulations using the 32-bit `ysyxSoC` environment.


<!-- 接入ysyxSoC后, 你设计的NPC已经可以正确与各种设备交互, 从功能上来说已经可以参与流片了.
根据"先完成, 后完美"的系统设计法则, 我们现在可以来讨论如何开展性能优化了. -->
After connecting to ysyxSoC, your designed NPC can now correctly interact with various devices, meaning it is ready to participate in the tape-out from a functional perspective.
Following the principle of "complete first, perfect later" in system design, we can now discuss performance optimization.

<!-- 说是要开展性能优化, 但这只是最终目的.
在一个复杂系统中, 我们将面临很多选择, 例如:
哪些地方是值得优化的? 应该采用什么方案进行优化? 预期收益是多少? 这些方案的开销是什么?
如果我们花费了很大的力气, 发现性能只提升了万分之一, 这肯定不是我们所期望的.
因此, 相比于盲目地编写代码, 我们更需要一套可以指导我们可以回答上述问题的科学方法:
1. 评估当前的性能
1. 寻找性能瓶颈
1. 采用合适的优化方法
1. 评估优化后的性能, 对比获得的性能提升是否符合预期 -->
Although we call it performance optimization, this is just the final goal.
In a complex system, we will face many choices, such as:
Which areas are worth optimizing? What optimization methods should we use? What are the expected benefits? What are the costs of these methods?
If we put in a lot of effort and only achieve a 0.01% performance improvement, this is certainly not what we want.
Therefore, rather than blindly writing code, we need a scientific approach that guides us to answer the above questions:
1. Evaluate current performance
1. Identify performance bottlenecks
1. Apply appropriate optimization methods
1. Evaluate the optimized performance and compare the performance gains with expectations

<!-- ## 性能评估

要谈优化, 首先要知道当前的系统运行得如何.
因此我们需要对性能有一个量化的衡量指标, 而不是凭我们的感觉来判断“运行得好不好”.
用这个指标来对当前的系统进行评估, 就是性能优化的第一步.

在我们的理解中, "性能高"基本上等同于"跑得快".
因此, 一个直接衡量性能的指标就是程序的执行时间.
所以, 评估一个系统的性能, 就是评估程序在这个系统上的执行时间. -->
## Performance Evaluation

Before we can talk about optimization, we first need to know how the current system is performing.
Thus, we need a quantitative performance metric, rather than judging "how well it runs" based on our feelings.
Using this metric to evaluate the current system is the first step in performance optimization.

<!-- ### 基准程序选择

那应该评估哪些程序呢?
程序五花八门, 要对所有程序进行评估是不现实的,
因此我们需要选择一些具有代表性的程序.
所谓的具有代表性, 就是指优化技术在这些程序上带来的性能收益,
与真实应用场景中的性能收益趋势基本一致.

这里提到了“应用场景”, 说明对于不同的应用场景, 上述趋势很可能不尽相同.
这意味着不同的应用场景需要不同的代表性程序, 这就产生了不同的benchmark.
例如, Linpack用于代表超级计算场景, MLPerf用于代表机器学习训练场景,
CloudSuite用于代表云计算场景, Embench用于代表嵌入式场景.
对于通用计算场景, 最著名的benchmark就是SPEC CPU, 用于评估CPU通用计算能力.
SPEC(Standard Performance Evaluation Corporation)是一个组织机构,
目标是建立维护用于评估计算机系统的各种标准, [它定义并发布了多种场景下的benchmark](https://www.spec.org/benchmarks.html).
除SPEC CPU外, 还有面向图形, 工作站, 高性能计算, 存储, 功耗, 虚拟化等各种场景的benchmark.

一个benchmark通常还由若干子项构成, 例如, SPEC CPU 2006的整数测试包括如下子项: -->
### Benchmark Program Selection

So which programs should we evaluate?
There are various programs, and it is unrealistic to evaluate all of them,
so we need to select some representative programs.
"Representative" means that the performance benefits of optimization techniques on these programs should align with those in real-world application scenarios.

Here, we mention "application scenarios", which means the trends for performance benefits may vary across different scenarios.
This implies that different application scenarios require different representative programs, leading to various benchmarks.
For example, Linpack represents the supercomputing scenario, MLPerf represents machine learning training, CloudSuite represents cloud computing, and Embench represents embedded systems.
For general computing scenarios, the most famous benchmark is SPEC CPU, which evaluates CPU general-purpose computational power.
SPEC (Standard Performance Evaluation Corporation) is an organization that defines and maintains benchmarks for evaluating computer systems, [and has published various benchmarks](https://www.spec.org/benchmarks.html) for different scenarios.
In addition to SPEC CPU, there are also benchmarks for graphics, workstations, high-performance computing, storage, power consumption, virtualization, etc.

A benchmark typically consists of several sub-items. For example, SPEC CPU 2006's integer test includes the following sub-items:


<!-- | 子项           | 简介                   |
| ---            | ---                      |
| 400.perlbench  | Perl语言检测垃圾邮件 |
| 401.bzip2      | bzip压缩算法 |
| 403.gcc        | gcc编译器 |
| 429.mcf        | 大型公共交通中单站车辆调度的组合优化问题 |
| 445.gobmk      | 围棋, 人工智能搜索问题 |
| 456.hmmer      | 使用隐马尔可夫模型的基因识别方法进行基因序列搜索 |
| 458.sjeng      | 国际象棋, 人工智能搜索问题 |
| 462.libquantum | 量子计算模拟素数分解 |
| 464.h264ref    | 对YUV格式源文件进行H.264视频编码 |
| 471.omnetpp    | 大型CSMA/CD协议以太网模拟 |
| 473.astar      | A\*算法寻路 |
| 483.xalancbmk  | XML到HTML的格式转换 | -->
| Sub-item         | Description                |
| ---------------- | -------------------------- |
| 400.perlbench    | Perl spam detection         |
| 401.bzip2        | bzip compression algorithm |
| 403.gcc          | gcc compiler               |
| 429.mcf          | Large public transportation optimization problem |
| 445.gobmk        | Go, AI search problem      |
| 456.hmmer        | Gene sequence search using Hidden Markov Models |
| 458.sjeng        | Chess, AI search problem  |
| 462.libquantum   | Quantum computation for prime factorization |
| 464.h264ref      | H.264 video encoding of YUV format source files |
| 471.omnetpp      | Large CSMA/CD protocol Ethernet simulation |
| 473.astar        | A* pathfinding algorithm  |
| 483.xalancbmk    | XML to HTML format conversion |


<!-- 除了整数测试, SPEC CPU 2006还包含浮点测试, 测试程序覆盖流体力学, 量子化学,
生物分子, 有限元分析, 线性规划, 影像光线追踪, 计算电磁学, 天气预报, 语音识别等不同领域.

当然, benchmark也需要与时俱进, 从而代表新时代的程序.
截至2024年, SPEC CPU已经演进了6版, 从一开始的1989年, 1992年, 1995年,
到2000年, 2006年, 最后是最新版本2017年.
SPEC CPU 2017加入了一些新程序来代表新的应用场景, 例如生物医学成像, 3D渲染和动画,
采用蒙特卡罗树搜索的人工智能围棋程序(很大概率受2016年AlphaGo的影响)等. -->
In addition to integer tests, SPEC CPU 2006 also includes floating-point tests, covering areas such as fluid dynamics, quantum chemistry, biomolecules, finite element analysis, linear programming, ray tracing, computational electromagnetics, weather forecasting, and speech recognition.

Of course, benchmarks need to evolve with the times to represent the programs of the new era.
As of 2024, SPEC CPU has evolved through six versions, starting from 1989, 1992, 1995, 2000, 2006, and finally the latest version in 2017.
SPEC CPU 2017 introduced new programs representing new application scenarios, such as biomedical imaging, 3D rendering and animation, and AI Go programs using Monte Carlo Tree Search (likely influenced by AlphaGo in 2016).

<!-- > #### comment::CoreMark和Dhrystone不是好的benchmark
> CoreMark和Dhrystone属于合成程序(synthetic program),
> 意思是用若干个代码片段拼接起来的程序.
> 例如, CoreMark由链表操作, 矩阵乘法和状态机转移操作这三个代码片段组成;
> Dhrystone则由一些字符串操作的代码片段组成.
>
> 合成程序作为benchmark, 最大的问题是其代表性较弱:
> CoreMark和Dhrystone能代表什么应用场景呢? 相比于SPEC CPU 2006中的各种真实应用,
> CoreMark中的代码片段顶多只能算C语言的课后作业;
> Dhrystone就离应用场景更远了, 其代码非常简单(使用短字符串常量),
> 甚至在现代编译器的作用下, 循环体中的代码片段很可能被深度优化(是否还记得NEMU中的`pattern_decode()`),
> 使得评估结果虚高, 无法客观反映系统的性能.
> [这篇文章](https://www.transputer.net/tn/27/tn27.html)详细分析了Dhrystone作为benchmark的缺陷. -->
> #### comment::CoreMark and Dhrystone are not good benchmarks
> CoreMark and Dhrystone are synthetic programs,
> meaning they consist of several code snippets pieced together.
> For example, CoreMark consists of linked list operations, matrix multiplication, and state machine transitions;
> Dhrystone is made up of string operations.
>
> The major problem with synthetic programs as benchmarks is their weak representativeness:
> What application scenarios do CoreMark and Dhrystone represent? Compared to the various real-world applications in SPEC CPU 2006,
> CoreMark’s code snippets are barely C-language homework;
> Dhrystone is even further from real applications, with very simple code (using short string literals),
> and under modern compilers, the code in the loop body is likely to be deeply optimized (remember `pattern_decode()` in NEMU),
> causing the evaluation results to be inflated, failing to objectively reflect system performance.
> [This article](https://www.transputer.net/tn/27/tn27.html) provides a detailed analysis of Dhrystone's flaws as a benchmark.
<!-- >
> 讽刺的是, 今天不少CPU厂商发布产品时,
> 仍然采用CoreMark或Dhrystone的评测结果来标识产品的性能,
> 甚至其中还不乏宣称是面向高性能场景的产品.
> [体系结构一代宗师, 图灵奖得主David Patterson在介绍Embench时](https://www.sigarch.org/embench-recruiting-for-the-long-overdue-and-deserved-demise-of-dhrystone-as-a-benchmark-for-embedded-computing/),
> 评价Dhrystone已经过时很久, 应该停止使用.
> 事实上, Dhrystone的第一版是1984年发布的, 从1988年之后Dhrystone就再也没有得到维护和升级.
> 和上世纪80年代相比, 今天的计算机领域已经发生了翻天覆地的变化,
> 应用程序早已更新换代, 编译技术日趋成熟, 硬件算力也大幅提升,
> 使用40年前的benchmark来评测今天的计算机, 其合理性确实有所欠缺. -->
>
> Ironically, many CPU vendors still use CoreMark or Dhrystone results to indicate their product performance,
> even for products supposedly designed for high-performance scenarios.
> [Turing Award winner David Patterson, a pioneer of computer architecture, commented on Embench](https://www.sigarch.org/embench-recruiting-for-the-long-overdue-and-deserved-demise-of-dhrystone-as-a-benchmark-for-embedded-computing/),
> stating that Dhrystone is long outdated and should be retired.
> In fact, the first version of Dhrystone was released in 1984, and it has not been maintained or updated since 1988.
> Compared to the 1980s, the computer field has drastically changed,
> with updated applications, mature compiler technologies, and much more powerful hardware.
> Using a 40-year-old benchmark to evaluate today's computers is certainly questionable.


<!-- 不过对于面向教学的"一生一芯"来说, SPEC CPU的程序有点过于真实了:
一方面它们的规模很大, 即使在x86真机中也需要花费小时量级的时间来运行;
另一方面它们需要运行在Linux环境中, 这意味着我们首先需要设计一个能正确启动Linux的CPU,
然后才能运行SPEC CPU这个benchmark.

相反, 我们希望有一套适合教学场景的benchmark, 满足以下条件:
* 规模不算太大, 在模拟器甚至在RTL仿真环境中的执行时间不到2小时
* 可在裸机环境中运行, 无需启动Linux
* 程序具有一定代表性, 不像CoreMark和Dhrystone那样采用合成程序 -->
For the "One Student One Chip" teaching scenario, SPEC CPU's programs are a bit too realistic:
On one hand, they are large-scale, requiring hours to run even on an x86 real machine;
On the other hand, they need to run in a Linux environment, meaning we first need to design a CPU that can correctly boot Linux,
and only then can we run the SPEC CPU benchmark.

In contrast, we want a benchmark suitable for teaching scenarios that meets the following conditions:
* Not too large in scale, with execution times under 2 hours in simulators or RTL simulation environments
* Can run in a bare-metal environment, without needing Linux
* The program is representative, unlike CoreMark and Dhrystone which are synthetic programs


<!-- 事实上, `am-kernels`中集成的microbench就是一个不错的选择.
一方面, microbench提供了多种规模的测试集, 模拟器可以采用`ref`规模,
RTL仿真环境可以采用`train`规模;
另一方面, microbench作为一个AM程序, 无需启动Linux即可运行;
此外, microbench包含10个子项, 覆盖排序, 位操作, 语言解释器,
矩阵计算, 素数生成, A\*算法, 最大网络流, 数据压缩, MD5校验和等场景.
因此, 当后续讲义提到性能评估但未明确说明相应的benchmark时,
将默认指代microbench的`train`规模.

如果处理器的应用场景比较明确, 例如运行超级玛丽,
那么还可以直接把超级玛丽作为benchmark,
相当于把“超级玛丽的游戏体验”作为“运行得好”的标准.
和microbench不同, 超级玛丽是一个不会运行结束的程序,
因此可以采用FPS作为量化指标来评估, 而不是采用运行时间. -->
In fact, the microbench integrated in `am-kernels` is a good choice.
On one hand, microbench provides test sets of various sizes, where simulators can use the `ref` size and RTL simulation environments can use the `train` size;
On the other hand, microbench is an AM program, so it can run without Linux;
Additionally, microbench includes 10 sub-items, covering sorting, bit operations, language interpreters,
matrix calculations, prime number generation, A* algorithm, maximum network flow, data compression, MD5 checksum, etc.
Therefore, when future lectures refer to performance evaluation but do not specify the corresponding benchmark,
it will refer to the `train` size of microbench by default.

If the processor's application scenario is clear, such as running Super Mario,
then we can directly use Super Mario as the benchmark,
equating "Super Mario's gameplay experience" with the standard of "good performance".
Unlike microbench, Super Mario is a program that never finishes running,
so we can use FPS as a quantitative metric to evaluate, rather than execution time.

<!-- ## 寻找性能瓶颈

### 性能公式和优化方向

我们可以测量benchmark的运行时间, 来得到系统的性能表现.
但运行时间是一个单一的数据, 我们很难从中找到系统的性能瓶颈来优化运行时间,
因此, 我们需要一些更细致的数据.

事实上, 我们可以把程序的运行时间分解成以下3个因子:

```
        time      inst     cycle     time
perf = ------- = ------ * ------- * -------
        prog      prog      inst     cycle
``` -->
## Finding Performance Bottlenecks

### Performance Formula and Optimization Directions

We can measure the running time of the benchmark to obtain the system's performance.
But the running time is a single data point, and it is difficult to identify performance bottlenecks directly from it,
so we need more detailed data.

In fact, we can break down the program's running time into the following three factors:

```
        time      inst     cycle     time
perf = ------- = ------ * ------- * -------
        prog      prog      inst     cycle
```

<!-- 性能优化的目标是减少程序的运行时间, 也即减小其中的每个因子, 这也揭示了性能优化的三个方向.

第一个优化方向是减少`程序执行的指令数量(即动态指令数)`, 可能的措施包括:
1. 修改程序, 采用更优的算法.
1. 采用更优的编译优化策略. 以gcc为例, 除了使用`-O3`, `-Ofast`等通用的优化等级参数外,
   还可以对目标程序进行针对性的调参. gcc中与生成代码质量相关的编译选项有大约600多个,
   选择合适的编译选项, 能大幅优化程序的动态指令数.
   例如, yzh在开展某课题项目时, 仅使用`-O3`编译CoreMark, 运行10轮的动态指令数约312万条;
   若额外打开一些针对性的编译选项, 则上述动态指令数可下降到约225万条, 程序运行的性能显著提升.
1. 设计并使用行为更复杂的指令集. 我们知道, CISC指令集包含行为较复杂的指令,
   若编译器使用这些复杂指令, 则可以降低动态指令数.
   另外, 也可以在处理器中添加自定义的专用指令, 并让程序使用这些专用指令. -->
The goal of performance optimization is to reduce the program's running time, which means minimizing each of these factors. This reveals three optimization directions.

The first optimization direction is to reduce the `number of instructions executed by the program (i.e., dynamic instruction count)`. Possible measures include:
1. Modifying the program to use more optimal algorithms.
1. Using better compiler optimization strategies. For example, in gcc, in addition to using common optimization flags like `-O3`, `-Ofast`, 
   we can also fine-tune the compilation options for the target program. There are about 600 compiler options in gcc related to code quality, 
   and choosing the right ones can significantly reduce dynamic instruction count.
   For example, when working on a project, yzh compiled CoreMark with just `-O3`, and the dynamic instruction count for 10 runs was about 3.12 million;
   by enabling some targeted compiler options, the dynamic instruction count dropped to about 2.25 million, significantly improving performance.
1. Designing and using more complex instruction sets. We know that CISC instruction sets contain complex instructions,
   and if the compiler uses these, it can reduce dynamic instruction count.
   Additionally, custom dedicated instructions can be added to the processor and used by the program.

<!-- 第二个优化方向是降低`平均每条指令执行所需的周期数`, 也即CPI(Cycles Per Instruction).
一般也称提升CPI的倒数, 即IPC(Instructions Per Cycle), 即提升平均每周期所执行的指令数.
这个指标反映了处理器的微结构设计效果, 强大的处理器通过每周期能执行更多的指令.
因此, 通常通过优化处理器的微结构设计来提升IPC, 从而使处理器更快地完成程序的执行.
微结构设计的优化又有不同的方向, 我们将在下文简单讨论.

第三个优化方向是减少`每个周期的时间`, 也即提升单位时间内的周期数, 后者就是电路的频率.
可能的优化措施包括:
1. 优化数字电路的前端设计, 减小关键路径的逻辑延迟.
1. 优化数字电路的后端设计, 减小关键路径的走线延迟. -->

The second optimization direction is to reduce the `cycles per instruction on average (CPI)`. Another way of saying this is to increase IPC (Instructions Per Cycle).
This metric reflects the processor's microarchitecture design: a powerful processor executes more instructions per cycle.
Thus, microarchitecture optimization typically aims to improve IPC, speeding up program execution.
Microarchitecture optimization has different directions, which we will briefly discuss later.

The third optimization direction is to reduce the `time per cycle`, or increase the cycles per unit time, which is the circuit's frequency.
Possible optimization measures include:
1. Optimizing the front-end design of digital circuits to reduce critical path logic delays.
1. Optimizing the back-end design of digital circuits to reduce critical path wiring delays.

<!-- 如果可以量化地评估上述3个因子, 我们就能更好地评估上述三个优化方向的潜力,
从而指导我们找到性能瓶颈.
幸运的是, 这些指标其实不难获取:
* 动态指令数可在仿真环境中直接统计
* 有了动态指令数, 再统计周期数, 即可计算出IPC
* 电路的频率可查看综合器的报告获取 -->
If we can quantify these three factors, we can better assess the potential of these optimization directions,
which will help guide us in finding the performance bottleneck.
Fortunately, these metrics are not difficult to obtain:
* Dynamic instruction count can be directly counted in the simulation environment.
* With the dynamic instruction count, we can calculate IPC by counting the cycle count.
* Circuit frequency can be found in the synthesizer's report.

<!-- > #### todo::统计IPC
> 尝试在仿真环境中统计IPC.
>
> 实际上, 我们在实现总线的时候要求你评估程序的运行时间, 用的就是上述性能公式.
> 不过当时我们是通过`程序执行所需的周期数 / 频率`来计算,
> 上述性能公式只是把`程序执行所需的周期数`进一步拆成两个因子而已.
> 但这样拆分还是能给我们提供更进一步的信息,
> 毕竟`程序执行所需的周期数`与程序和处理器都相关,
> 而`平均每条指令执行所需的周期数`就只和处理器的处理能力相关了. -->

> #### todo::Count IPC
> Try to count IPC in the simulation environment.
>
> Actually, when we implemented the bus, we asked you to evaluate the program's running time using the performance formula above.
> However, we calculated it using `program cycles / frequency` at that time,
> and the performance formula above simply breaks down `program cycles` into two factors.
> Nevertheless, this breakdown can still provide us with more detailed information,
> since `program cycles` depend on both the program and processor,
> while `cycles per instruction` depends only on the processor's capabilities.

<!-- ### 简单处理器的性能模型

即使IPC不难统计, 但就如统计得到的运行时间无法指导我们如何优化运行时间,
统计得到的IPC也并不能指导我们如何优化IPC.
为了找到性能瓶颈, 我们需要分析IPC受哪些因素的影响, 正如上文分析运行时间的因子一样.
为此, 我们需要重新审视处理器是如何执行指令的. -->

### Simple Processor Performance Model

Even though IPC is easy to count, just as running time can't guide us on how to optimize it,
the measured IPC also can't guide us on how to optimize it.
To find performance bottlenecks, we need to analyze the factors that influence IPC, just as we did for running time.
For this, we need to re-examine how the processor executes instructions.

```
       /--- frontend ---\    /-------- backend --------\
                                  +-----+ <--- 2. computation efficiency
                             +--> | FU  | --+
       +-----+     +-----+   |    +-----+   |    +-----+
       | IFU | --> | IDU | --+              +--> | WBU |
       +-----+     +-----+   |    +-----+   |    +-----+
          ^                  +--> | LSU | --+
          |                       +-----+
1. instruction supply                ^
                    3. data supply --+
```

<!-- 上图是一个简单的处理器结构图, 我们之前都是从功能的角度来理解这个图, 现在我们需要从性能的角度来理解它.
我们可以将处理器划分成前端和后端, 其中, 前端包括取指和译码,
剩余的模块属于后端, 负责真正执行指令并改变处理器状态.
注意, 处理器的前后端划分和上文提到的数字电路的前后端设计并不相同.
事实上, 处理器的前后端设计都属于数字电路的前端设计环节. -->

The above is a simple processor block diagram, and we have previously understood it from a functional perspective. Now, we need to look at it from a performance perspective.
We can divide the processor into the front-end and back-end, where the front-end handles instruction fetch and decode,
and the remaining modules are in the back-end, responsible for executing instructions and updating the processor state.
Note that the front-end and back-end division in processors is different from the digital circuit front-end and back-end design mentioned earlier.
In fact, both the processor's front-end and back-end design belong to the digital circuit's front-end design phase.

<!-- 要提升处理器的执行效率, 就需要做到:
1. 处理器前端需要保证指令供给. 如果前端取不到足够的指令, 就无法完全发挥处理器的计算能力.
   因为每一条指令的执行都需要先取指, 因此前端的指令供给能力将影响所有指令的执行效率.
1. 处理器后端需要保证计算效率和数据供给
   * 对于大部分计算类指令, 其执行效率取决于相应功能单元的计算效率.
     例如, 乘除法指令的执行效率还受乘除法器的计算效率的影响.
     类似的还有浮点执行和浮点处理器单元FPU等.
   * 对于访存类指令, 其执行效率取决于LSU的访存效率.
     特别地, 对于load指令, 处理器需要等待存储器返回数据, 然后才能将数据写回寄存器堆.
     这意味着, load指令的执行效率取决于LSU和存储器的数据供给能力.
     store指令则比较特殊, 因为store指令不需要写入寄存器堆,
     原则上处理器不必等待数据完全写入存储器.
     在高性能处理器中, 通常会设计一个store buffer部件,
     处理器将store指令的信息写入store buffer后, 即认为store执行结束,
     后续由store buffer部件控制将数据真正写入存储器中.
     当然, 这增加了处理器设计的复杂度, 例如load指令还需要检查最新的数据是否在store buffer中. -->
To improve processor execution efficiency, we need to ensure:
1. The front-end guarantees instruction supply. If the front-end can't fetch enough instructions, the processor's computational power can't be fully utilized.
   Because each instruction execution requires an instruction fetch, the front-end's instruction supply capacity affects the execution efficiency of all instructions.
1. The back-end guarantees computation efficiency and data supply.
   * For most computational instructions, their execution efficiency depends on the efficiency of the corresponding functional unit.
     For example, the efficiency of multiplication and division instructions depends on the efficiency of the multiplier/divider.
     Similar effects apply to floating-point operations and floating-point units (FPU).
   * For memory access instructions, their execution efficiency depends on the efficiency of the LSU.
     Specifically, for load instructions, the processor must wait for data from memory before writing it back to the register file.
     This means the execution efficiency of load instructions depends on LSU and memory's data supply capabilities.
     Store instructions are special because they don't require writing to the register file;
     the processor doesn't need to wait for the data to be completely written to memory.
     In high-performance processors, a store buffer is often designed, where the processor considers a store instruction complete after writing to the store buffer,
     and the store buffer handles the actual writing to memory later.
     However, this increases the complexity of the processor design, such as ensuring that load instructions check whether the latest data is in the store buffer.
<!-- 那么, 我们应该如何量化地评估处理器的指令供给, 计算效率和数据供给呢?
换句话说, 我们真正想了解的是, 处理器在运行指定的benchmark时, IFU和LSU等模块有没有全速工作.
为此, 我们又需要收集更多的信息. -->
So how do we quantify instruction supply, computation efficiency, and data supply?
In other words, we want to understand if modules like the IFU and LSU are running at full speed when the processor runs a given benchmark.
For this, we need to gather more information.

<!-- ### 性能事件和性能计数器

为了量化地评估处理器的指令供给, 计算效率和数据供给, 我们需要进一步理解影响它们的细致因素.
以指令供给为例, 指令供给能力怎么算强呢?
能直接反映指令供给能力的, 就是IFU是否取到了指令.
为此, 我们可以把"IFU取到指令"看作一个事件, 来统计这个事件发生的频次,
如果这个事件经常发生, 指令供给能力就强; 否则, 指令供给能力就弱.

这些事件称为性能事件(performance event), 通过它们,
我们可以将性能模型中一些较为抽象的性能指标转化为电路上的具体事件.
类似地, 我们还可以统计"LSU取到数据"这个事件发生的频次, 来衡量数据供给能力的强弱;
统计"EXU完成计算"这个事件发生的频次, 来衡量计算效率的高低.

要统计性能事件发生的频次, 我们只需要在硬件中添加一些计数器,
在检测到性能事件发生时, 计数器的值就加1.
这些计数器称为性能计数器(performance counter).
有了性能计数器, 我们就可以观察"程序在处理器上运行的时间都花在哪里"了,
相当于对处理器内部做profiling.

要在电路上检测性能事件的发生并不难, 我们可以利用总线机制的握手信号进行检测.
例如, IFU取指的R通道握手时, 表示IFU接收到AXI总线返回的数据, 从而完成一次取指操作,
因此当R通道握手时, 我们就可以让相应的性能计数器加1. -->

### Performance Events and Performance Counters

To quantitatively assess the processor's instruction supply, computational efficiency, and data supply, we need to further understand the detailed factors that influence them.
Take instruction supply as an example. How do we determine the strength of instruction supply capability?
The direct indicator of instruction supply capability is whether the IFU has fetched the instruction.
To this end, we can treat “IFU fetching instructions” as an event and count the frequency of this event occurring.  
If this event occurs frequently, the instruction supply capability is strong; otherwise, it is weak.  

These events are called performance events (performance event), and through them,  
we can convert some of the more abstract performance metrics in the performance model into specific events on the circuit.
Similarly, we can also count the frequency of the event “LSU fetching data” to measure the strength of data supply capability;  
and count the frequency of the event “EXU completing computation” to measure the efficiency of computation.  

To count the frequency of performance events, we only need to add some counters to the hardware,  
and increment the counter value by 1 when a performance event occurs.
These counters are called performance counters.  
With performance counters, we can observe “where the program spends its time running on the processor,”  
which is equivalent to profiling the processor's internal operations.  

Detecting the occurrence of performance events on the circuit is not difficult; we can use the handshake signals of the bus mechanism for detection.
For example, when the R channel handshakes during IFU instruction fetch, it indicates that the IFU has received data returned from the AXI bus, thereby completing an instruction fetch operation.  
Therefore, when the R channel handshakes, we can increment the corresponding performance counter by 1.


<!-- > #### todo::添加性能计数器
> 尝试在NPC中添加一些性能计数器, 至少包含如下性能事件的性能计数器:
> * IFU取到指令
> * LSU取到数据
> * EXU完成计算
> * 译码出各种类别的指令, 如计算类指令, 访存指令, CSR指令等
>
> 性能计数器本质上也是由电路实现的. 随着性能计数器的数量增加,
> 它们占用的电路面积将越来越大, 甚至有可能影响电路中的关键路径.
> 因此我们不要求性能计数器参与流片, 你只需要在仿真环境中使用它们即可:
> 你可以用RTL实现性能计数器, 在仿真结束时通过`$display()`等方式输出它们的值,
> 然后在综合时通过配置的方式选择不实例化它们;
> 或者将性能事件的检测信号通过DPI-C接入到仿真环境, 在仿真环境中实现性能计数器.
> 这样, 你就可以随心所欲地添加性能计数器, 不必担心影响电路的面积和频率.
>
> 实现后, 尝试运行microbench的test规模, 收集性能计数器的结果.
> 如果你的实现正确, 语义相近的不同性能计数器的统计结果应存在一致性.
> 例如, 译码得到的不同类别的指令的总数, 应与IFU取到指令的数量一致, 也与动态指令数一致.
> 尝试挖掘更多的一致性关系并检查这些关系是否成立.

相比于事件发生, 有时候我们更关心事件什么时候不发生, 以及为什么不发生.
例如, 其实我们更关心IFU什么时候取不到指令, 以及IFU为什么取不到指令,
梳理其中的缘由有助于我们理解指令供给的瓶颈在哪里,
从而为提升处理器的指令供给提供指导.
我们可以把"事件不发生"定义成一个新的事件, 并为新事件添加性能计数器. -->
> #### todo::Add performance counters  
> Attempt to add some performance counters in the NPC, including at least the following performance counters for performance events:
> * IFU fetches an instruction  
> * LSU fetches data  
> * EXU completes the computation  
> * Decodes various types of instructions, such as computational instructions, memory access instructions, CSR instructions, etc.  
>  
> Performance counters are essentially implemented by circuitry. As the number of performance counters increases,  
> they will occupy an increasingly larger circuit area, potentially affecting critical paths in the circuit.
> Therefore, we do not require performance counters to be included in the tape-out. You can use them solely in the simulation environment:
> You can implement performance counters using RTL and output their values at the end of simulation via methods like `$display()`,
> then configure the synthesis process to avoid instantiating them;
> Alternatively, you can connect the performance event detection signals to the simulation environment via DPI-C and implement performance counters in the simulation environment.  
> This way, you can freely add performance counters without worrying about affecting the circuit area and frequency.  
>  
> After implementation, try running the microbench test scale and collect the performance counter results.
> If your implementation is correct, the statistical results of different performance counters with similar semantics should be consistent.
> For example, the total number of different categories of instructions decoded should be consistent with the number of instructions fetched by the IFU and also with the dynamic instruction count.
> Try to identify more consistent relationships and verify whether these relationships hold true.

Sometimes, we are more concerned with when an event does not occur and why it does not occur, rather than when it does occur.  
For example, we are actually more concerned with when the IFU cannot fetch instructions and why it cannot fetch instructions.  
Understanding the underlying reasons helps us identify bottlenecks in instruction supply,  
which in turn provides guidance for improving the processor's instruction supply.  
We can define “event not occurring” as a new event and add performance counters for the new event.

<!-- > #### todo::添加性能计数器(2)
> 在NPC中添加更多性能计数器, 并尝试分析以下问题:
> * 每种类别的指令占多少比例? 它们各自平均需要执行多少个周期?
> * IFU取不到指令的原因有哪些? 这些原因导致IFU取不到指令的几率分别是多少?
> * LSU的平均访存延迟是多少? -->

> #### todo::Add performance counters (2)
> Add more performance counters in the NPC and attempt to analyze the following issues:
> * What percentage of instructions does each category account for? How many cycles does each category average to execute?
> * What are the reasons why the IFU cannot fetch instructions? What are the probabilities of these reasons causing the IFU to fail to fetch instructions?
> * What is the average memory access latency of the LSU?

<!--  
> #### comment::性能计数器的trace
> 前文介绍的性能计数器的使用方式都是在仿真结束后输出并分析.
> 如果我们每周期都输出性能计数器的值, 我们就能得到性能计数器的trace!
> 根据这种trace, 借助一些绘图工具(如python的matplotlib绘图库),
> 我们可以绘制出性能计数器的值随时间变化的曲线,
> 将仿真过程中性能计数器的变化过程可视化,
> 从而帮助我们更好地判断性能计数器的变化过程是否符合预期. -->

<!-- -->
> #### comment::Performance counter trace
> The usage methods of performance counters described earlier all involve outputting and analyzing the results after simulation.
> If we output the values of performance counters every cycle, we can obtain a performance counter trace!
> Based on this trace, using some plotting tools (such as Python's matplotlib plotting library),
> we can plot the performance counter values over time,
> visualize the changes in performance counters during the simulation,
> and thus better judge whether the changes in performance counters are as expected.

<!-- ### 阿姆达尔定律(Amdahl's law)

性能计数器可以为处理器微结构的优化提供量化指导.
那么, 性能瓶颈究竟在哪里? 哪些优化工作是值得做的呢? 优化工作的预期性能收益是多少?
我们需要在开展具体的优化之前就回答这些问题,
以帮助我们规避那些预期性能收益很低的优化工作,
从而将更多时间投入到收益高的优化工作中.
这听上去像一个预测未来的工作, 但Amdahl's law可以告诉我们答案.

[Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law)由计算机科学家Gene Amdahl在1967年提出,
其内容是:
```
The overall performance improvement gained by optimizing a single part
of a system is limited by the fraction of time that the improved part
is actually used.

优化系统中某部分所获得的总体性能收益, 受限于那部分实际使用的时间占比.
``` -->

### Amdahl's Law

Performance counters can provide quantitative guidance for optimizing processor microarchitecture.  
So, where exactly is the performance bottleneck? Which optimization efforts are worthwhile? What is the expected performance gain from optimization efforts?  
We need to answer these questions before undertaking specific optimizations,  
to help us avoid optimization efforts with low expected performance gains,  
and instead focus more time on optimization efforts with high expected gains.
This may sound like predicting the future, but Amdahl's Law can provide the answer.

[Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law) was proposed by computer scientist Gene Amdahl in 1967,
and states:
```
The overall performance improvement gained by optimizing a single part
of a system is limited by the fraction of time that the improved part
is actually used.

The overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used.
```


<!-- 假设系统某部分实际使用的时间占比是`p`, 该部分在优化后的加速比是`s`,
则整个系统的加速比为`f(s) = 1 / (1 - p + p / s)`, 这就是Amdahl's law的公式表示.

例如, 某程序的运行过程分为独立的两部分A和B, 其中A占80%, B占20%.
* 若将B优化5倍, 则整个程序的加速比是`1 / (0.8 + 0.2 / 5) = 1.1905`;
* 若将B优化5000倍, 则整个程序的加速比是`1 / (0.8 + 0.2 / 5000) = 1.2499`;
* 若将A优化2倍, 则整个程序的加速比是`1 / (0.2 + 0.8 / 2) = 1.6667`. -->
Assuming that the proportion of time that a certain part of the system is actually used is `p`, and the acceleration ratio of that part after optimization is `s`,
then the acceleration ratio of the entire system is `f(s) = 1 / (1 - p + p / s)`, which is the formula for Amdahl's Law.

For example, the execution process of a program is divided into two independent parts, A and B, where A accounts for 80% and B accounts for 20%.
* If B is optimized by a factor of 5, the overall program's speedup ratio is `1 / (0.8 + 0.2 / 5) = 1.1905`;
* If B is optimized by a factor of 5000, the overall program's speedup ratio is `1 / (0.8 + 0.2 / 5000) = 1.2499`;
* If A is optimized by a factor of 2, the overall program acceleration ratio is `1 / (0.2 + 0.8 / 2) = 1.6667`.

<!-- ```
++++++++++++++++++++ooooo   原程序

++++++++++++++++++++o       将B优化5倍

++++++++++++++++++++        将B优化5000倍, 优化后B的运行时间很短

++++++++++ooooo             将A优化2倍
``` -->

```
<------- A --------><-B->
++++++++++++++++++++ooooo   Original program

++++++++++++++++++++o       Optimize B by a factor of 5

++++++++++++++++++++        Optimize B by 5000 times, resulting in a very short runtime for the optimized B

++++++++++ooooo             Optimize A by 2 times
```

<!-- 通常来说, 优化5000倍所需要付出的努力要比优化2倍大得多,
但Amdahl's law告诉我们, 把B优化5000倍, 还不如把A优化2倍.
这个反直觉的结论告诉我们, 不能仅考虑某部分的加速比,
还需要考虑该部分的时间占比, 从整系统的角度评估一项技术的优化效果.
因此, 对处理器的性能优化来说,
提前通过性能计数器测量某个优化对象在运行时间中的占比, 就显得非常重要了. -->

Generally speaking, optimizing by 5000 times requires significantly more effort than optimizing by 2 times,
but Amdahl's law tells us that optimizing B by 5000 times is less effective than optimizing A by 2 times.
This counterintuitive conclusion tells us that we cannot simply consider the acceleration ratio of a particular component,
but must also consider its time proportion, and evaluate the optimization effect of a technology from the perspective of the entire system.
Therefore, for processor performance optimization,
it is very important to measure the proportion of a particular optimization target in the runtime using performance counters in advance.

<!-- > #### todo::根据性能计数器寻找合适的性能瓶颈
> 根据性能计数器的统计结果, 尝试挖掘一些潜在的优化对象,
> 然后利用Amdahl's law估算它们能获取的理论收益, 从而确定系统的性能瓶颈位于何处.


> #### caution::从事计算机体系结构工作的专业素养
> 软件领域有一则广泛流传的忠告:
> ```
> 抛开workload谈优化就是耍流氓.
> ```
> 它的意思是, 优化方案的选择一定要基于负载的运行情况.
>
> 在体系结构领域更是如此, 我们不能依靠直觉来优化处理器设计, 觉得哪里有优化机会就改哪里,
> 否则将很容易采取一个没有效果的方案, 甚至会在实际场景中造成性能倒退.
> 相反, 根据评估数据采取合适的设计方案, 才是科学的做法.
>
> 事实上, Amdahl's law很容易理解, 如果不考虑专业背景,
> 我们甚至可以将它包装成一道数学应用题给小学生解答.
> 但我们也见过很多初学者"耍流氓", 说白了还是缺少相关的专业素养.
>
> 大家来学习“一生一芯”, 并不仅仅是学习RTL编码,
> 更重要的是学习解决问题的科学方法, 锻炼出这个方向的专业素养,
> 使得将来遇到真实问题时, 知道如何通过正确的方式解决. -->

> #### todo::Identify suitable performance bottlenecks using performance counters
> Based on the statistics from performance counters, attempt to identify potential optimization targets,
> then use Amdahl's law to estimate their theoretical performance gains, thereby determining where the system's performance bottlenecks lie.

<!-- -->
> #### caution::Professional ethics in computer architecture
> There is a widely circulated piece of advice in the software field:
> ```
> Discussing optimization without considering the workload is irresponsible.
> ```
> This means that the selection of optimization schemes must be based on the actual runtime conditions of the workload.
>
> This is especially true in the field of computer architecture, where we cannot rely on intuition to optimize processor designs, making changes wherever we perceive optimization opportunities,
> as this can easily lead to ineffective solutions and may even result in performance degradation in real-world scenarios.
> Instead, adopting a suitable design scheme based on evaluation data is the scientific approach.
>
> In fact, Amdahl's Law is easy to understand; without considering professional background,
> we could even package it as a math application problem for elementary school students to solve.
> However, we have also seen many beginners “play fast and loose,” which boils down to a lack of relevant professional expertise.
>
> When we learn “One Student One Chip,” it's not just about learning RTL coding,  
> but more importantly, it's about learning scientific problem-solving methods and developing professional expertise in this field,  
> so that when we encounter real-world problems in the future, we know how to solve them using the correct approach.


<!-- -->
<!-- > #### caution::自顶向下的调试方法
> 你已经调试过很多功能bug了, 但其实还有一种bug叫性能bug,
> 它的表现并不是程序出错或崩溃, 而是程序的运行性能低于预期.
> 当然, 调试性能bug的过程和性能优化的过程有相同之处,
> 都需要寻找系统中的性能瓶颈.
>
> 事实上, 调试功能bug和调试性能bug, 也有相似之处.
> 调试功能bug时, 我们首先看到的是程序出错或崩溃的信息,
> 但仅仅阅读这样的信息, 还很难找到bug;
> 于是要通过各种层次的trace工具来理解程序的行为, 找到程序出错时的具体表现;
> 然后再使用gdb/波形等工具, 在变量/信号层次进行细致分析.
>
> 调试性能bug时, 我们首先看到的是程序的运行时间,
> 但它无法直接告诉我们性能瓶颈在哪里;
> 于是通过性能公式将程序的运行时间分解成3个因子,
> 从编译, 微结构, 频率这3个方向考察优化潜力;
> 对于微结构, 光靠统计的IPC, 还是很难找到性能瓶颈,
> 于是我们需要分析影响IPC的因素, 把处理器划分成三大部分,
> 从指令供给, 数据供给和计算效率来理解处理器执行指令的过程,
> 但我们需要更具体的量化数据;
> 于是我们需要添加性能计数器, 来统计每个模块中性能事件发生的情况,
> 最后通过Amdahl's law找到真正的性能瓶颈.
>
> 调试这两种bug都用到了类似自顶向下的分析方法, 这并不是巧合,
> 而是计算机系统领域中抽象思维的体现: 抽象是理解复杂系统的唯一方式.
> 事实上, 如果你尝试一开始就使用gdb/波形来调试, 你会感到非常困难,
> 这是因为底层的细节数量巨大, 难以提供宏观视角的理解.
> 因此, 我们需要从高层语义开始, 根据合适的路径向下追溯,
> 在底层定位到一个很小的范围, 这才能帮助我们快速找到问题的所在. -->

> #### Caution:: Top-down debugging method
> You've debugged many functional bugs, but there's also a type of bug called a performance bug,
> which doesn't manifest as program errors or crashes, but rather as program performance that falls short of expectations.
> Of course, the process of debugging performance bugs shares similarities with the process of performance optimization,
> both requiring the identification of performance bottlenecks within the system.
>
> In fact, debugging functional bugs and debugging performance bugs also share similarities.
> When debugging functional bugs, we first see error or crash messages from the program,  
> but simply reading such messages makes it difficult to locate the bug;  
> therefore, we use various levels of trace tools to understand the program's behavior and identify the specific manifestations of the error;  
> then we use tools like GDB or waveforms to conduct detailed analysis at the variable/signal level.
>  
> When debugging performance bugs, we first observe the program's runtime,  
> but this does not directly reveal where the performance bottlenecks are;  
> we then decompose the program's runtime into three factors using performance formulas,  
> and examine optimization potential from three directions: compilation, microarchitecture, and frequency;  
> for microarchitecture, relying solely on statistical IPC metrics still makes it difficult to identify performance bottlenecks,
> Therefore, we need to analyze the factors affecting IPC, dividing the processor into three major components,
> understanding the processor's instruction execution process through instruction supply, data supply, and computational efficiency,
> but we require more specific quantitative data;
> thus, we need to add performance counters to track the occurrence of performance events in each module,
> and finally use Amdahl's Law to identify the true performance bottleneck.
>
> Debugging both types of bugs involves a similar top-down analysis approach, which is no coincidence,  
> but rather a reflection of abstract thinking in the field of computer systems: abstraction is the only way to understand complex systems.  
> In fact, if you try to debug using GDB/waveforms from the start, you will find it very challenging,  
> as the sheer volume of low-level details makes it difficult to gain a macro-level understanding.
> Therefore, we need to start from high-level semantics and trace downwards along appropriate paths,
> narrowing the scope to a very small area at the lower level, which enables us to quickly pinpoint the issue.


<!-- ### 校准访存延迟

我们将NPC接入ysyxSoC后, SDRAM控制器等模块提供了更真实的访存过程.
可以想象, 如果我们在接入ysyxSoC之前统计性能计数器,
由于访存延迟的差异, 其结果将与接入ysyxSoC后大不相同.
而不同的统计结果将会指导我们往不同的方向进行优化,
但如果我们以流片为目标, 这些不同方向的优化很可能无法取得预期的效果.
因此, 如果仿真环境的行为和真实芯片越接近, 评估结果的误差就越小,
在性能计数器的指导下开展的优化所取得的性能提升就越真实.

事实上, 之前的ysyxSoC环境是假设处理器和各种外设运行在同一频率下:
verilator仿真的一个周期, 既是处理器中的一个周期, 也是外设中的一个周期.
但实际上并非如此: 受电气特性的影响, 外设通常只能运行在低频率,
例如SDRAM颗粒通常只能运行在100MHz左右, 过高的频率会导致时序违例,
使得SDRAM颗粒无法正确工作; 但另一方面, 使用先进工艺的处理器通常能够运行在更高的频率,
例如, yzh某个版本的多周期NPC在`yosys-sta`项目默认提供的nangate45工艺下频率达到约1.2GHz.
在上述配置下, SDRAM控制器中经过1个周期, NPC应该经过12个周期,
但verilator感知不到两者的频率差异, 仍然按照两者频率相同的假设进行仿真,
使得仿真结果比真实芯片乐观很多, 从而也可能会使得一些优化措施无法在真实芯片中取得预期的效果. -->
### Calibrating Memory Access Latency

After connecting the NPC to the ysyxSOC, modules such as the SDRAM controller provide a more realistic memory access process.
Imagine if we had counted performance counters before connecting to the ysyxSOC;  
due to differences in memory access latency, the results would be significantly different from those obtained after connecting to the ysyxSOC.  
Different statistical results would guide us toward different optimization directions,  
but if our goal is to achieve the desired performance, these different optimization directions may not yield the expected results.
Therefore, the closer the behavior of the simulation environment aligns with that of the actual chip, the smaller the error in the evaluation results,  
and the performance improvements achieved through optimizations guided by performance counters will be more accurate.

In fact, the previous ysyxSOC environment assumed that the processor and various peripherals operate at the same frequency:
One cycle in Verilator simulation corresponds to one cycle in both the processor and peripherals.  
However, this is not the case in reality: Due to electrical characteristics, peripherals typically operate at lower frequencies,  
such as SDRAM chips, which usually run at around 100MHz. Higher frequencies can cause timing violations,  
preventing SDRAM chips from functioning correctly; On the other hand, processors using advanced processes can typically operate at higher frequencies,  
for example, in a certain version of yzh's multi-cycle NPC, the frequency reaches approximately 1.2GHz under the nangate45 process provided by default in the `yosys-sta` project.
Under the above configuration, the SDRAM controller completes one cycle, while the NPC should complete 12 cycles,  
but Verilator does not detect the frequency difference between the two and simulates them as if they have the same frequency,  
resulting in simulation results that are much more optimistic than the actual chip, which may also cause some optimization measures to fail to achieve the expected effects on the actual chip.


<!-- > #### danger::更新yosys-sta
> 我们在2024/04/09 08:30:00更新了`yosys-sta`项目, 添加了iEDA团队研发的网表优化工具,
> 大幅优化了yosys生成的综合网表, 使其时序评估结果更接近商业工具.
> 如果你在上述时间之前获得`yosys-sta`的代码, 请删除已有的`yosys-sta`项目并重新克隆. -->
> #### danger::Update yosys-sta
> We updated the `yosys-sta` project on 2024/04/09 08:30:00, adding a netlist optimization tool developed by the iEDA team,
> which significantly optimizes the comprehensive netlist generated by yosys, making its timing evaluation results closer to those of commercial tools.
> If you obtained the `yosys-sta` code prior to the above time, please delete the existing `yosys-sta` project and re-clone it.

<!-- 为了获得更准确的仿真结果, 以指导我们进行更有效的优化, 我们需要对访存延迟进行校准(calibration).
校准的方式有两种, 一种是使用支持多个时钟域的仿真器, 例如VCS或[ICARUS verilog](https://github.com/steveicarus/iverilog).
和采用周期精确模型方式实现的verilator不同, 这种仿真器采用事件队列模型的方式实现,
把Verilog中的每次计算都看作事件, 并且可以维护事件的延迟,
从而可以正确地维护多时钟域中不同模块在不同频率下工作时每次计算的顺序.
不过为了维护事件队列模型, 这种仿真器的运行速度通常要低于verilator. -->

To obtain more accurate simulation results to guide us in more effective optimization, we need to calibrate memory access latency.  
There are two calibration methods: one is to use a simulator that supports multiple clock domains, such as VCS or [ICARUS Verilog](https://github.com/steveicarus/iverilog).
Unlike Verilator, which implements a cycle-accurate model, these simulators use an event queue model,  
treating each computation in Verilog as an event and maintaining event delays,  
thereby correctly maintaining the order of computations across different modules in multiple clock domains operating at different frequencies.  
However, to maintain the event queue model, these simulators typically run slower than Verilator.

<!-- 校准的第二种方式是修改RTL代码, 在ysyxSoC中插入一个延迟模块,
负责将请求延迟若干周期, 来模拟设备在低频率运行的效果,
使得NPC等待的周期数与其将来在高频率运行时所等待的周期数接近.
这种方式的实现不算复杂, 而且可以使用较快的verilator来仿真, 我们选择这种方式.
此外, 这种方式也适用于FPGA. -->

The second calibration method involves modifying the RTL code by inserting a delay module into ysyxSOC,  
which delays requests by a certain number of cycles to simulate the device's operation at low frequencies,  
ensuring that the number of cycles NPC waits is close to the number it would wait at high frequencies.  
This method is not overly complex and can be simulated using the faster Verilator, so we opted for this approach.
Additionally, this method is also applicable to FPGAs.

<!-- 很自然, 为了实现延迟模块, 我们只要在延迟模块收到设备的回复后,
不马上回复给上游模块, 而是等待一定的周期数再回复, 但如何计算需要等待的周期数却是一个挑战.
考虑上文yzh的例子, 如果一个请求在SDRAM控制器中需要花费6个周期, 那NPC应该总共等待`6 * 12 = 72`个周期;
如果碰上SDRAM控制器在刷新SDRAM颗粒, 请求在SDRAM控制器中花费了10个周期, NPC应该总共等待`10 * 12 = 120`个周期;
如果请求发往flash, 在SPI master中花费了150个周期, NPC应该总共等待`150 * 12 = 1800`个周期.
可以看到, 延迟模块需要等待的周期数与设备服务请求所花费的时间有关,
并不是一个固定的常数, 因此需要在延迟模块中动态计算.
假设请求在设备中花费了`k`个周期, 处理器和设备的频率比是`r`(应有`r >= 1`),
那么延迟模块中需要计算出处理器所需等待的周期数`c = k * r`.
为了进行这一动态计算, 我们又需要考虑两个问题:
1. 如何低开销地实现乘法?
1. 如果`r`是小数, 如何实现小数的乘法?
   例如`yosys-sta`项目报告的频率是550MH, 那么`r = 550 / 100 = 5.5`,
   但如果把5.5按5来计算, 一个在设备端花费6周期请求将会在处理器端引入3个周期的误差,
   对高速运行的CPU来说误差太大, 误差的积累会明显地影响性能计数器的值, 从而进一步影响优化的决策. -->

Naturally, to implement the delay module, we only need to delay the response to the upstream module after the delay module receives the device's response,  
rather than responding immediately. However, calculating the number of cycles to wait is a challenge.
Considering the example mentioned earlier by yzh, if a request takes 6 cycles in the SDRAM controller, the NPC should wait a total of `6 * 12 = 72` cycles;  
if the SDRAM controller is refreshing the SDRAM chips and the request takes 10 cycles in the SDRAM controller, the NPC should wait a total of `10 * 12 = 120` cycles;
If the request is sent to flash and takes 150 cycles in the SPI master, the NPC should wait a total of `150 * 12 = 1800` cycles.
As can be seen, the number of cycles the delay module needs to wait is related to the time the device takes to process the request,
and is not a fixed constant, so it needs to be dynamically calculated within the delay module.
Assuming the request takes `k` cycles in the device, and the frequency ratio between the processor and the device is `r` (where `r >= 1`),  
the delay module must calculate the number of cycles the processor needs to wait, `c = k * r`.
To perform this dynamic calculation, we need to consider two issues:
1. How to implement multiplication with low overhead?
1. If `r` is a decimal, how to implement decimal multiplication?  
For example, the frequency reported by the `yosys-sta` project is 550 MHz, so `r = 550 / 100 = 5.5`,
   but if we calculate 5.5 as 5, a request that takes 6 cycles on the device side will introduce a 3-cycle error on the processor side.  
   For a high-speed CPU, this error is too large, and the accumulation of errors will significantly affect the value of the performance counter, thereby further affecting optimization decisions.

<!-- 考虑到ysyxSoC的代码不参与综合和流片, 其实我们可以用一些简单的方法解决问题,
例如用``*``来计算乘法的结果, 用定点数来表示小数.
不过作为一个练习, 我们还是要求大家尝试可综合的实现方式,
将来如果你需要在可综合电路中解决类似的问题, 你就知道怎么做了.

首先我们先考虑`r`是整数时, 如何实现乘法.
既然延迟模块本身也需要等待设备的回复, 等待的时间正好是请求在设备中花费的周期数`k`,
那干脆让延迟模块在等待的每个周期中对一个计数器进行累加, 每周期加`r`即可.
对于给定的处理器频率和设备频率, `r`是个定值, 因此可以直接硬编码到RTL代码中.
在延迟模块收到设备的回复后, 就进入等待状态, 每周期让计数器减1, 减到0时再将请求回复给上游. -->

Considering that the ysyxSOC code does not participate in synthesis and tape-out, we can actually use some simple methods to solve the problem,  
such as using ``*`` to calculate the result of multiplication and using fixed-point numbers to represent decimals.  
However, as an exercise, we still require everyone to try a synthesizable implementation method,  
so that in the future, if you need to solve similar problems in synthesizable circuits, you will know how to do it.

First, let's consider how to implement multiplication when `r` is an integer.
Since the delay module itself also needs to wait for the device's response, the waiting time is exactly the number of cycles `k` the request spends in the device.
Therefore, we can simply have the delay module increment a counter by `r` each cycle while waiting.
Given the processor frequency and device frequency, `r` is a constant value, so it can be directly hard-coded into the RTL code.  
After the delay module receives the device's response, it enters a waiting state, decrementing the counter by 1 each cycle, and once it reaches 0, it sends the request response back to the upstream.  

<!-- 然后我们来考虑`r`是小数的情况.
既然小数部分不方便处理, 直接截断又会引入较大误差,
我们可以想办法将小数部分并入整数部分进行累加.
事实上, 我们可以引入一个放大系数`s`, 累加时每周期往计数器加`r * s`,
即累加结束时, 计数器的值为`y = r * s * k`,
然后在进入等待状态前, 将计数器更新为`y / s`即可.
因为`s`是一个常数, 因此`r * s`的结果也可以直接硬编码到RTL中,
当然这里的`r * s`很有可能还不是整数, 这里我们将其截断为整数,
虽然这理论上这仍然会引入一定的误差, 但我们可以证明误差比之前小很多.
不过`y`的值是动态计算的, 不能硬编码到RTL中,
因此对于一般的`s`, `y / s`需要计算除法.
你应该很快想到, 我们可以取一些特殊的`s`, 来简化这一计算的过程!
通过这种方式, 我们可以把误差减少到原来的`1/s`,
即原来在累加阶段累积的误差达到`s`时, 在这种新方法下的误差才增加`1`.

回顾当前的ysyxSoC, 其中SDRAM采用APB接口, 因此我们需要实现一个APB的延迟模块.
ysyxSoC中已经包含一个APB延迟模块的框架, 并集成到APB Xbar的上游,
可捕捉所有APB访问请求, 包括SDRAM的访问请求.
但该框架并未提供延迟模块的具体实现, 因此默认无延迟效果.
为了校准ysyxSoC中SDRAM的访问延迟, 你还需要实现APB延迟模块的功能. -->

Next, let's consider the case where `r` is a decimal number.  
Since the decimal part is inconvenient to handle, simply truncating it would introduce significant error,
we can try to incorporate the fractional part into the integer part for accumulation.  
In fact, we can introduce a scaling factor `s`, adding `r * s` to the counter each cycle during accumulation,  
so that at the end of accumulation, the counter value is `y = r * s * k`,  
and before entering the waiting state, the counter is updated to `y / s`.
Since `s` is a constant, the result of `r * s` can also be directly hard-coded into the RTL.  
Of course, `r * s` may not be an integer here, so we truncate it to an integer.  
Although this theoretically still introduces some error, we can prove that the error is much smaller than before.
However, the value of `y` is dynamically calculated and cannot be hard-coded into the RTL.  
Therefore, for general `s`, `y / s` requires division.  
You should quickly realize that we can choose some special `s` values to simplify this calculation process!
By doing so, we can reduce the error to `1/s` of the original,  
meaning that the error accumulated during the accumulation phase reaches `s` before the error increases by `1` under this new method.

Looking at the current ysyxSOC, where SDRAM uses an APB interface, we need to implement an APB delay module.
The ysyxSOC already includes a framework for an APB delay module, integrated upstream of the APB Xbar,  
which captures all APB access requests, including those for SDRAM.  
However, this framework does not provide the specific implementation of the delay module, so it has no delay effect by default.  
To calibrate the access delay of SDRAM in the ysyxSOC, you also need to implement the functionality of the APB delay module.

<!-- > #### todo::校准访存延迟
> 按照上文的介绍, 在ysyxSoC中实现APB延迟模块, 以校准仿真环境的访存延迟.
> 具体地, 如果你选择Verilog, 你需要在`ysyxSoC/perip/amba/apb_delayer.v`中实现相应代码;
> 如果你选择Chisel, 你需要在`ysyxSoC/src/amba/APBDelayer.scala`的`APBDelayerChisel`模块中实现相应代码,
> 并将`ysyxSoC/src/amba/APBDelayer.scala`中的`Module(new apb_delayer)`修改为实例化`APBDelayerChisel`模块.
>
> 为了实现APB延迟模块, 你需要根据APB协议的定义, 梳理出一个APB事务何时开始, 何时结束.
> 假设一个APB事务从`t0`时刻开始, 设备端在`t1`时刻返回APB的回复,
> APB延迟模块在`t1'`时刻向上游返回APB的回复, 则应有等式`(t1 - t0) * r = t1' - t0`.
>
> 关于`r`的取值, 我们假设设备运行在100MHz的环境下,
> 你可以根据`yosys-sta`的综合报告计算出`r`.
> 至于`s`, 理论上当然是越大越好, 不过你只要选择一个实际中够用的`s`即可.
> 至于多少够用, 就交给你来观察了, 这其实也是一种profiling.
>
> 实现后, 尝试取不同的`r`, 在波形中观察上述等式是否成立 -->

> #### todo::Calibrate memory access delay  
> As described above, implement the APB delay module in ysyxSOC to calibrate the memory access delay in the simulation environment.  
> Specifically, if you choose Verilog, you need to implement the corresponding code in `ysyxSOC/perip/amba/apb_delayer.v`;
> If you choose Chisel, you need to implement the corresponding code in the `APBDelayerChisel` module in `ysyxSOC/src/amba/APBDelayer.scala`,
> and modify `Module(new apb_delayer)` in `ysyxSOC/src/amba/APBDelayer.scala` to instantiate the `APBDelayerChisel` module.
>
> To implement the APB delay module, you need to determine when an APB transaction begins and ends based on the definition of the APB protocol.
> Assuming an APB transaction starts at time `t0`, the device returns the APB response at time `t1`,
> and the APB delay module returns the APB response to the upstream at time `t1'`, then the equation `(t1 - t0) * r = t1' - t0` should hold.
>
> Regarding the value of `r`, we assume that the device is running in a 100MHz environment.
> You can calculate `r` based on the synthesis report from `yosys-sta`.
> As for `s`, theoretically, the larger the better, but you only need to choose an `s` that is sufficient for practical use.
> As for how much is sufficient, that is up to you to observe, which is essentially a form of profiling.
>
> After implementation, try different values of `r` and observe in the waveform whether the above equation holds true.

<!-- -->
<!-- > #### todo::寻找最高的综合频率
> 除了频率之外, 面积也是电路的一个评价指标.
> 但在工艺库的标准单元层次, 这两者本质上是互相制约的:
> 功能相同的一类单元, 如果希望单元的逻辑延迟低,
> 就需要通过更多的晶体管让其具备更强的驱动能力, 从而增大单元的面积.
>
> 鉴于面积和频率之前的制约关系, 综合器一般会用尽可能少的面积来达到给定的目标频率,
> 而不会去单独考虑给定的电路最高能达到多少主频.
> 如果你的电路质量比较高, 可能会出现综合报告的频率随目标频率提升而提升的现象,
> 不过电路的综合面积也会随之增加.
>
> 因此, 如果你暂时不考虑面积的开销, 可以设置一个比较高的目标频率, 让综合器尝试去达到.
> 在处理器设计中, 有一些因素会成为处理器频率的上限:
> 1. 寄存器堆的读延迟. 通常寄存器堆的读操作需要在一个周期内完成,
>    无法拆分到多个周期中, 因此处理器的频率不会超过寄存器堆的最大工作频率.
> 1. 位宽和处理器字长相等的加法器延迟. 通常EXU的加法操作需要在一个周期内完成,
>    如果这个加法操作需要多个周期完成, 将会大幅降低所有包含加法操作的指令的执行效率,
>    包括加法指令, 减法指令, 访存指令(需要计算访存地址), 分支指令(需要计算目标地址),
>    甚至是`PC + 4`的计算, 从而使得程序的IPC大幅降低.
>    因此处理器的频率不会超过这个加法器的最大工作频率.
> 1. SRAM的读写延迟. SRAM作为一种全定制单元, 无法通过逻辑设计来优化其读写延迟,
>    因此只要使用了SRAM, 处理器的频率不会超过SRAM的最大工作频率.
>
> 你可以编写一些简单的小模块来单独评估这些部件的最大工作频率,
> 为了避免I/O端口的影响, 你需要在这些部件的输入端和输出端都插入一些触发器.
> SRAM作为一种全定制单元, 其最大工作频率通常会记录在相应的手册中,
> 而且目前我们暂不使用SRAM, 你可以先不开展SRAM的评估工作.
>
> 评估后, 你可以将处理器的综合目标频率设置得比上述部件的最大工作频率更高,
> 来指导综合器尽可能地综合出更高频率的结果.
> 当然你也可以直接将目标主频设置成一个很难达到的值, 例如5000MHz,
> 不过我们还是推荐你通过上述评估过程了解这些部件的最大工作频率. -->

> #### todo::Finding the highest synthesis frequency
> In addition to frequency, area is another evaluation metric for circuits.
> However, at the standard cell level of the process library, these two metrics are inherently mutually constraining:
> For a class of cells with the same functionality, if you want the cell to have low logic delay,
> you need to use more transistors to give it stronger drive capability, thereby increasing the cell's area.
>
> Given the trade-off between area and frequency, the synthesizer generally aims to achieve the target frequency with the minimum possible area,
> rather than considering the maximum clock frequency the circuit can achieve.
> If your circuit quality is high, you may observe that the frequency in the synthesis report increases as the target frequency is raised,
> though the synthesis area will also increase accordingly.
>
> Therefore, if you temporarily disregard area constraints, you can set a higher target frequency and allow the synthesizer to attempt to achieve it.
> In processor design, there are some factors that act as upper limits on processor frequency:
> 1. Register file read latency. Typically, register file read operations must be completed within a single cycle,  
>    and cannot be split across multiple cycles, so the processor's frequency cannot exceed the maximum operating frequency of the register file.
> 1. Addition unit latency with bit width and processor word length equal. Typically, addition operations in the EXU must be completed within a single cycle,  
>    if this addition operation requires multiple cycles to complete, it will significantly reduce the execution efficiency of all instructions containing addition operations,  
>    including addition instructions, subtraction instructions, memory access instructions (requiring calculation of the memory access address), branch instructions (requiring calculation of the target address),
>    or even the calculation of `PC + 4`, thereby significantly reducing the program's IPC.
>    Therefore, the processor's frequency will not exceed the maximum operating frequency of this adder.
> 1. SRAM read/write latency. As a fully customizable unit, SRAM cannot be optimized for read/write latency through logical design,
>    so as long as SRAM is used, the processor's frequency will not exceed the maximum operating frequency of SRAM.
>
> You can write some simple small modules to evaluate the maximum operating frequency of these components separately,  
> to avoid the influence of I/O ports, you need to insert some flip-flops at the input and output ends of these components.  
> As a fully customizable unit, the maximum operating frequency of SRAM is typically recorded in the corresponding manual,  
> and since we are not currently using SRAM, you can temporarily skip the evaluation of SRAM.
>
> After evaluation, you can set the processor's synthesis target frequency higher than the maximum operating frequencies of the above components,
> to guide the synthesizer to achieve the highest possible frequency.
> Of course, you can also directly set the target clock frequency to a value that is difficult to achieve, such as 5000MHz,
> but we still recommend that you first understand the maximum operating frequencies of these components through the above evaluation process.


<!-- -->
<!-- > #### comment::可编程的计数器增量
> 上文提到的`r`对RTL设计来说是个常数, 但在可以动态调频的复杂处理器中并不是这样.
> 对于这种复杂处理器, 我们需要把`r`设计成可编程的, 通过一个设备寄存器来存放`r`,
> 软件进行动态调频后, 就将新的`r`写入到设备寄存器中.
> 因此, 我们还需要将这个设备寄存器映射到处理器的地址空间中,
> 使其可以被处理器通过SoC访问到. 当然, `s`也可以设计成可编程的.
>
> 不过, 这需要对ysyxSoC进行较多的改动, 所以我们就不要求大家实现上述可编程功能了. -->

> #### comment::Programmable counter increment
> The `r` mentioned above is a constant for RTL design, but this is not the case for complex processors with dynamic frequency adjustment.
> For such complex processors, we need to design `r` as programmable, storing it in a device register,
> After dynamic frequency adjustment by the software, the new `r` is written to the device register.
> Therefore, we also need to map this device register to the processor's address space,
> so that it can be accessed by the processor through the SoC. Of course, `s` can also be designed to be programmable.
>
> However, this requires significant modifications to the ysyxSOC, so we will not require everyone to implement the above programmable functionality.

<!-- -->
<!-- > #### todo::重新寻找优化瓶颈
> 添加延迟模块后, 重新运行一些测试并收集性能计数器的统计结果,
> 然后根据Amdahl's law寻找性能瓶颈. -->
> #### todo::Re-identify performance bottlenecks
> After adding the delay module, rerun some tests and collect performance counter statistics,
> then use Amdahl's law to identify performance bottlenecks.
<!-- -->
<!-- > #### todo::评估NPC的性能
> 添加延迟模块后, 运行microbench的train规模测试, 记录各种性能数据,
> 包括主频信息和各种性能计数器.
>
> 校准访存延迟后, 在ysyxSoC中运行microbench的train规模测试预计需要花费数小时,
> 但我们将得到与流片环境非常接近的性能数据.
> 后续你可以在每次添加一个特性后, 就重新评估并记录性能数据,
> 来帮助你梳理每一个特性带来的性能收益. -->

> #### todo::Evaluate NPC performance
> After adding the delay module, run the microbench train-scale test, record various performance data,
> including clock frequency information and various performance counters.
>
> After calibrating memory access latency, running the microbench train-scale test in ysyxSOC is expected to take several hours,
> but we will obtain performance data very close to that of the tape-out environment.
> Going forward, you can reassess and record performance data after adding each feature,
> to help you analyze the performance gains brought by each feature.

<!-- -->
<!-- > #### danger::记录性能数据
> 接下来, 我们要求你记录每一次评估后的性能数据.
> 如果你申请第六期的流片, 你将会提交这部分记录,
> 如果记录的情况与实际开发过程不符, 你可能无法获得流片机会.
> 我们希望通过这种方式强迫你去认识并理解NPC的性能变化情况,
> 锻炼出处理器体系结构设计的基本素养,
> 而不是仅仅将参考书籍中的架构图翻译成RTL代码.
>
> 具体地, 你可以按照如下方式记录:
> | commit           | 说明            | 仿真周期数 | 指令数 | IPC | 综合频率 | 综合面积 | 性能计数器1 | 性能计数器2 | ... |
> | ---              | ---             | ---        | ---    | --- | ---      | ---      | ---         | ---         | --- |
> | 0123456789abcdef | 示例, 实现cache | 200000     | 10000  | 0.05|  750MHz  |  16000   |   3527      |   8573      | ... |
>
> 其中:
> * 我们要求你在NPC工程目录下的`Makefile`中添加一条规则`make perf`,
>   使得将来执行
>   ```bash
>   git checkout 表中的commit
>   make perf
>   ```
>   后, 可以复现出对应表格中的性能数据.
>   如果复现的情况与表中记录的数据严重不符, 在无法合理说明的情况下, 可能会被判定为违反学术诚信.
>   * 你可以假设你正在进行一项科学研究, 你需要对研究过程中的实验数据负责:
>     实验数据需要是可复现的, 能够在公开场合下经得起推敲.
> * 你可以在学习记录中新建一个名为`NPC性能评估结果`的工作表, 来记录这些性能数据
> * 你可以将`性能计数器1`和`性能计数器2`更换成相应性能计数器的实际名称
> * 你可以根据实际情况, 记录更多的性能计数器
> * 你也可以将你对性能数据的分析记录到说明一栏
> * 我们鼓励你尽可能多地记录性能数据的条目, 从而帮助你量化地分析NPC的性能变化情况 -->

> #### danger::Record performance data
> Next, we require you to record performance data after each evaluation.
> If you apply for the tape-out, you will submit this record.
> If the recorded data does not match the actual development process, you may not be eligible for tape-out.
> We hope to use this method to force you to recognize and understand the performance changes of NPC,
> develop the fundamental skills required for processor architecture design,
> rather than merely translating architecture diagrams from reference books into RTL code.
>
> Specifically, you can record the data as follows:
> | Commit           | Description            | Simulation cycles | Instructions | IPC | Synthesis frequency | Synthesis area | Performance counter 1 | Performance counter 2 | ... |
> | ---              | ---             | ---        | ---    | --- | ---      | ---      | ---         | ---         | --- |
> | 0123456789abcdef | Example, implement cache | 200000     | 10000  | 0.05|  750MHz  |  16000   |   3527      |   8573      | ... |
>
> Note:
> * We require you to add a rule `make perf` to the `Makefile` in the NPC project directory,
>   so that when you execute
> ```bash
>   git checkout the commit in the table
>   make perf
>   ```
>   will reproduce the performance data in the corresponding table.
>   If the reproduced results significantly deviate from the data recorded in the table and cannot be reasonably explained, it may be deemed a violation of academic integrity.
>   * You can assume that you are conducting a scientific research project and are responsible for the experimental data throughout the research process:
>     Experimental data must be reproducible and able to withstand scrutiny in public settings.
> * You can create a new worksheet named `NPC Performance Evaluation Results` in your learning records to record these performance data.
> * You can replace `Performance Counter 1` and `Performance Counter 2` with the actual names of the corresponding performance counters.
> * You can record additional performance counters based on actual circumstances.
> * You can also record your analysis of the performance data in the description column.  
> * We encourage you to record as many performance data entries as possible to help you quantitatively analyze changes in NPC performance.

<!-- -->
<!-- > #### question::优化主频是否值得?
> 根据主频校准访存延迟后, 你会发现IPC大幅下降.
> 可以预料到的是, 如果主频进一步提升, 访存延迟的周期数也会增加, 从而导致IPC降低.
> 那么, 主频是否值得我们去优化呢?
> 如果值得, 优化主频带来的性能收益具体是来自于哪里?
> 如果不值得, 优化主频带来的性能倒退具体又体现在哪处?
> 尝试结合性能计数器分析你的猜想. -->

> #### question::Is it worth optimizing the CPU clock frequency?  
> After calibrating the CPU clock frequency to memory access latency, you will find that IPC drops significantly.
> As expected, if the clock frequency is further increased, the number of cycles for memory access latency will also increase, leading to a decrease in IPC.  
> So, is it worth optimizing the clock frequency?  
> If it is worth it, where exactly does the performance gain from optimizing the clock frequency come from?  
> If it is not worth it, where exactly does the performance degradation from optimizing the clock frequency manifest itself?
> Try to analyze your hypothesis using performance counters.

<!-- -->
<!-- > #### comment::校准FPGA上的访存延迟
> 现代的FPGA上一般含有DDR内存控制器.
> 但受限于FPGA的实现原理, PL部分的CPU频率也与ASIC流程有很大差距,
> 甚至CPU频率比内存控制器的频率还低. 例如, 内存控制器可以跑在200MHz,
> 但CPU只能在FPGA上以数百MHz甚至数十MHz的频率运行,
> 而在真实芯片中, CPU通常能运行在1GHz以上(例如, 第三代香山的目标主频是3GHz).
> 显然, 在这样的评估环境中得到的性能数据, 对将流片作为目标的CPU性能测试来说是大幅失真的.
> 通过校准访存延迟解决上述内存频率倒挂的情况,
> 是处理器企业使用FPGA进行CPU性能评估之前必须解决的问题.
>
> 事实上, 由于真实的DDR是一个复杂的系统, 即使采用上文介绍的延迟模块的方案,
> 也还需要考虑更多问题:
> * 由于模拟电路部件的差异, FPGA中内存控制器的phy模块与ASIC的内存控制器不同,
>   这可能会影响访存延迟
> * 受限于FPGA的实现原理和FPGA上PLL的可配置范围,
>   DDR控制器运行的频率也低于ASIC的内存控制器,
>   但DDR颗粒无法等比例降频, 导致访存延迟不准确
> * DDR控制器降频后, 其刷新频率等参数也与ASIC的内存控制器不一致
>
> 因此, 要在FPGA上很好地解决内存频率倒挂问题, 在业界也是一项不小的挑战.
> 例如, 香山团队成立了一个小组, 由工程师带队来解决这一问题.
>
> 事实上, 是否需要校准FPGA的访存延迟, 取决于FPGA的使用场景和目标:
> * 教学: 仅仅将FPGA作为功能测试的环境. 此时FPGA起到的作用就是加速仿真过程,
>   无论处理器和内存控制器之间的运行频率比例如何, 理论上都不影响功能测试的结果.
> * 比赛或科研项目: 将FPGA作为性能测试的环境, 同时也作为目标平台.
>   此时不以流片为目标, 故不必校准访存延迟.
> * 企业产品研发: 将FPGA作为性能测试的环境, 但同时把流片作为目标.
>   我们会期望通过FPGA得出的性能数据尽可能与真实芯片一致.
>   这时, 校准FPGA的访存延迟将是不可或缺的.
>
> "一生一芯"虽然作了诸多简化, 但总体上还是希望大家能体会到企业产品研发的大致流程,
> 同时考虑到真实DDR控制器的校准存在诸多工程挑战, 因此我们不要求大家使用FPGA.
> 相比之下, 在仿真环境中校准访存延迟, 要比在FPGA中容易很多,
> 因此我们还是推荐大家在仿真环境中进行性能评估和优化. -->

> #### comment::Calibrating memory access latency on an FPGA  
> Modern FPGAs typically include DDR memory controllers.  
> However, due to the implementation principles of FPGAs, the CPU frequency in the PL section differs significantly from that in the ASIC process,  
> and in some cases, the CPU frequency is even lower than that of the memory controller. For example, the memory controller can operate at 200MHz,  
> but the CPU can only run at hundreds of MHz or even tens of MHz on the FPGA,  
> while in real chips, the CPU typically operates at over 1GHz (e.g., the target clock speed for the third-generation Xiangshan is 3GHz).  
> Clearly, performance data obtained in such an evaluation environment is significantly distorted for CPU performance testing targeting tape-out.
> Addressing the aforementioned memory frequency mismatch through calibration of memory access latency  
> is a problem that processor companies must resolve before using FPGAs for CPU performance evaluation.  
>  
> In fact, since real DDR is a complex system, even when adopting the latency module solution described above,  
> additional considerations are necessary:
> * Due to differences in analog circuit components, the PHY module of the memory controller in the FPGA differs from that of the ASIC's memory controller,  
> which may affect memory access latency.  
> * Constrained by the implementation principles of FPGAs and the configurable range of PLLs on FPGAs,  
> the operating frequency of the DDR controller is lower than that of the ASIC's memory controller,  
> but DDR memory chips cannot be proportionally downclocked, leading to inaccurate memory access latency.
> * After the DDR controller is downclocked, its refresh frequency and other parameters also differ from those of the ASIC memory controller.  
>  
> Therefore, effectively addressing the memory frequency inversion issue on an FPGA remains a significant challenge in the industry.  
> For example, the Xiangshan team formed a task force led by engineers to tackle this problem.
>
> In fact, whether FPGA memory access latency needs to be calibrated depends on the FPGA's use case and objectives:
> * Education: Using the FPGA solely as an environment for functional testing. In this case, the FPGA serves to accelerate the simulation process,
>   and theoretically, the frequency ratio between the processor and memory controller does not affect the results of functional testing.
> * Competitions or research projects: Using the FPGA as an environment for performance testing, as well as the target platform.
>   In this case, since the goal is not to tape out, there is no need to calibrate memory access latency.
> * Enterprise product development: Using the FPGA as an environment for performance testing, but also aiming to tape out.
> We expect the performance data obtained from the FPGA to be as consistent as possible with the actual chip.
> At this point, calibrating the FPGA's memory access latency will be indispensable.
>
> Although “One Student One Chip” has been simplified in many ways, we still hope that everyone can get a general idea of the product development process in a company.
> At the same time, considering the many engineering challenges involved in calibrating a real DDR controller, we do not require everyone to use an FPGA.
> In contrast, calibrating memory access latency in a simulation environment is much easier than in an FPGA,
> so we still recommend that you perform performance evaluation and optimization in a simulation environment.


<!-- -->
<!-- > #### todo::提升功能测试的效率
> 将校准访存延迟后的ysyxSoC仿真环境用于性能评估是很合适的,
> 但你也会感觉到, 这一环境的仿真效率明显低于之前的`riscv32e-npc`:
> 从microbench的train规模测试的运行时间来看,
> `riscv32e-npc`的仿真效率是`riscv32e-ysyxsoc`的数十倍甚至上百倍.
> 这其实反映出一种权衡关系: 要得到更准确的性能数据,
> 就要仿真越多的细节(例如SDRAM控制器和SDRAM颗粒),
> 从而需要在仿真1周期的过程中花费更多的时间, 最终导致仿真效率越低.
> 相应地, 仿真效率较高的`riscv32e-npc`, 得到的性能数据则是不准确的.
>
> 那`riscv32e-npc`是否没有意义呢?
> 事实上, 我们可以把`riscv32e-npc`作为一个功能测试的环境.
> 如果在`riscv32e-npc`中存在一个功能bug,
> 那么这个bug也很大概率存在于`riscv32e-ysyxsoc`之中,
> 但显然在仿真效率更高的`riscv32e-npc`中调试这个bug是一个更合适的方案.
> 这样, 我们就可以充分发挥两个仿真环境各自的优势, 取长补短,
> 从整体上提升开发和测试的效率.
>
> 尝试修改相关的仿真流程, 从而支持NPC在`riscv32e-npc`和`riscv32-ysyxsoc`中仿真.
> 其中, `riscv32e-npc`仍然采用`0x8000_0000`作为复位时的PC值. -->

> #### todo::Improve the efficiency of functional testing
> Using the ysyxSoC simulation environment with calibrated memory access latency for performance evaluation is appropriate,
> but you will also notice that the simulation efficiency of this environment is significantly lower than that of the previous `riscv32e-npc`:  
> From the runtime of the microbench train-scale test,  
> the simulation efficiency of `riscv32e-npc` is tens or even hundreds of times higher than that of `riscv32e-ysyxsoc`.
> This actually reflects a trade-off: to obtain more accurate performance data,  
> more details need to be simulated (e.g., SDRAM controller and SDRAM chips),  
> which requires more time to simulate one cycle, ultimately leading to lower simulation efficiency.  
> Correspondingly, the `riscv32e-npc` environment with higher simulation efficiency yields inaccurate performance data.
>  
> Does this mean `riscv32e-npc` is useless?  
> In fact, we can use `riscv32e-npc` as a functional testing environment.  
> If there is a functional bug in `riscv32e-npc`,
> it is highly likely that the same bug also exists in `riscv32e-ysyxsoc`,  
> but clearly debugging this bug in the more efficient `riscv32e-npc` is a more appropriate approach.  
> This way, we can leverage the strengths of both simulation environments, complementing each other's weaknesses,  
> thereby improving overall development and testing efficiency.
>  
> Attempt to modify the relevant simulation workflow to support NPC simulation in both `riscv32e-npc` and `riscv32-ysyxsoc`.  
> In this case, `riscv32e-npc` continues to use `0x8000_0000` as the PC value during reset.


<!-- ## 经典体系结构的4类优化方法

找到性能瓶颈后, 我们就可以考虑如何对其进行优化.
经典体系结构中主要有4类优化方法:
1. 局部性 - 利用数据访问的性质提升指令供给和数据供给能力. 代表性技术是缓存
1. 并行 - 多个实例同时工作, 提升系统整体的处理能力. 并行方法又有很多分类:
   * 指令级并行 - 同时执行多条指令, 相关技术包括流水线, 多发射, VLIW和乱序执行
   * 数据级并行 - 同时访问多个数据, 相关技术包括SIMD, 向量指令/向量机
   * 任务级并行 - 同时执行多个任务, 相关技术包括多线程, 多核, 多处理器和多进程;
     GPU属于SIMT, 是一种介于数据级并行和任务级并行之间的并行方法
1. 预测 - 在不知道正确选择时, 先投机地执行一个选择, 在后续检查选择是否正确, 如果预测正确, 就能降低等待的延迟, 从而获得性能收益. 如果预测错误, 就需要通过额外的机制进行恢复. 代表性技术包括分支预测和缓存预取
1. 加速器 - 使用专门的硬件部件来执行特定任务, 从而提升该任务的执行效率, 一些例子包括:
   * AI加速器 - 对AI负载的计算过程进行加速, 通常通过总线访问
   * 自定义扩展指令 - 将加速器集成到CPU内部, 通过通过新增的自定义扩展指令来访问
   * 乘除法器 - 可以视为一类加速器, 将RVM视为RVI的扩展,
     通过乘除法指令控制专门的乘除法硬件模块, 来加速乘除法的计算过程 -->

## Four Optimization Methods for Classic Computer Architectures

After identifying performance bottlenecks, we can consider how to optimize them.  
Classic computer architectures primarily employ four optimization methods:
1. Locality - Leveraging the nature of data access to enhance instruction and data supply capabilities. Representative technology includes caching.
1. Parallelism - Multiple instances working simultaneously to enhance the system's overall processing capability. Parallel methods further categorize into:
   * Instruction-level parallelism - Execute multiple instructions simultaneously. Related technologies include pipelining, multiple issue, VLIW, and out-of-order execution.  
   * Data-level parallelism - Access multiple data simultaneously. Related technologies include SIMD, vector instructions/vector machines.  
   * Task-level parallelism - Execute multiple tasks simultaneously. Related technologies include multithreading, multicore, multiprocessing, and multiprocessing;
     GPUs belong to SIMT, a parallel method that lies between data-level parallelism and task-level parallelism
1. Prediction - When the correct choice is unknown, speculatively execute one choice first, then verify whether the choice was correct in subsequent steps. If the prediction is correct, it reduces waiting latency and improves performance. If the prediction is incorrect, additional mechanisms are required for recovery. Representative technologies include branch prediction and cache prefetching
1. Accelerators - Use specialized hardware components to execute specific tasks, thereby improving the execution efficiency of those tasks. Some examples include:
   * AI accelerators - Accelerate AI workload computations, typically via bus access
   * Custom extended instructions - Integrate accelerators into the CPU and access them via newly added custom extended instructions
   * Multiplier-Divider - Can be considered a type of accelerator, treating RVM as an extension of RVI,
     controlling dedicated multiplier-divider hardware modules via multiplier-divider instructions to accelerate the computation process

<!-- > #### caution::重新审视处理器体系结构设计
> 很多电子类专业的同学一开始很可能会把处理器体系结构设计理解成"用RTL开发一款处理器".
> 但RTL编码只是处理器设计流程中的其中一个环节, 而且严格来说并不属于处理器体系结构设计的范畴.
>
> 事实上, 一名合格的处理器架构师应该具备如下能力:
> 1. 理解程序如何在处理器上运行
> 1. 对于支撑程序运行的特性, 能判断它们适合在硬件层次实现, 还是适合在软件层次实现
> 1. 对于适合在硬件层次实现的特性, 能提出一套在各种因素的权衡之下仍然满足目标要求的设计方案
>
> 这些能力的背后其实反映出人们使用计算机的根本目的: 通过程序解决真实需求.
> 如果在硬件层次添加的特性给程序带来的收益很低, 甚至程序根本不会使用这一特性,
> 那相关方案的决策者确实算不上是专业的架构师.
>
> 事实上, 这些能力是需要刻意去锻炼的.
> 我们见过不少同学, 能根据一些参考资料将流水线处理器的框图翻译成RTL代码,
> 但却无法评估一个程序的运行时间是否符合预期, 也不知道如何进一步优化或实现新需求;
> 还有一些同学设计出一个乱序超标量处理器, 但性能却比不上教科书上的五级流水线.
> 这说明, 体系结构设计能力并不等同于RTL编码能力,
> 也许这些同学在开发过程中确实理解了流水线和乱序超标量的基本概念,
> 但却缺少全局的视野和理解, 只将关注点放在提升后端的计算效率上,
> 很少甚至没有关注过指令供给和数据供给,
> 导致处理器的访存能力远低于计算能力, 总体的性能表现不佳.
> 因此, 即使能设计出一个正确的乱序超标量处理器, 也算不上是一个好的处理器,
> 从某种程度上来说, 这些同学也还不具备处理器体系结构设计能力.
>
> "一生一芯"尝试从另一个思路锻炼大家的处理器体系结构设计能力:
> 首先将程序运行起来, 从软硬件协同的视角理解程序运行的每一处细节;
> 然后学习处理器性能评估的基本原理, 理解程序行为在硬件层次的微观表现;
> 最后才是学习各种体系结构优化方法, 使用科学的评估手段理解这些优化方法对程序运行带来的真实收益.
>
> 这一学习方案和教科书有很大区别,
> 这是因为处理器体系结构设计能力只能通过实践来锻炼,
> 但采用教科书的理论课堂受限于课程体制, 无法考察学生的体系结构设计能力.
> 因此, 你很容易通过教科书或者参考书籍入门,
> 但如果你想成为这个方向的专业人士, 就要明白这些书籍的上限,
> 在必要的阶段跨越书籍的边界, 通过针对性的训练锻炼出真正的体系结构设计能力. -->

> #### Caution: Re-evaluate processor architecture design
> Many electronics engineering students may initially misunderstand processor architecture design as “developing a processor using RTL.”
> However, RTL coding is merely one stage in the processor design process and, strictly speaking, does not fall within the scope of processor architecture design.  
>  
> In fact, a qualified processor architect should possess the following capabilities:  
> 1. Understand how programs run on processors  
> 1. For features that support program execution, be able to determine whether they are suitable for implementation at the hardware level or the software level
> 1. For features suitable for implementation at the hardware level, propose a design solution that meets the target requirements after balancing various factors  
>  
> These capabilities fundamentally reflect the core purpose of using computers: to solve real-world problems through programs.  
> If the benefits of adding features at the hardware level are minimal for a program, or the program does not utilize such features at all,  
> then the decision-makers behind such solutions cannot be considered professional architects.
>
> In fact, these capabilities require deliberate cultivation.
> We have seen many students who can translate the block diagram of a pipelined processor into RTL code based on reference materials,
> but are unable to assess whether a program's runtime meets expectations, nor do they know how to further optimize or implement new requirements;
> Some students design an out-of-order superscalar processor, yet its performance falls short of the five-stage pipeline described in textbooks.  
> This indicates that architectural design capability is not equivalent to RTL coding capability.  
> While these students may have understood the basic concepts of pipelines and out-of-order superscalar processing during development,  
> they lack a holistic perspective and understanding, focusing solely on improving computational efficiency in the backend,
> paying little to no attention to instruction supply and data supply,  
> resulting in the processor's memory access capability being far below its computational capability, leading to poor overall performance.  
> Therefore, even if one can design a correct out-of-order superscalar processor, it does not qualify as a good processor,  
> and in a sense, these students still lack processor architecture design capabilities.
>
> “One Student, One Chip” aims to cultivate students' processor architecture design capabilities from a different perspective:
> First, run the program and understand every detail of its execution from a software-hardware collaboration perspective;
> Then, learn the basic principles of processor performance evaluation and understand how program behavior manifests at the hardware level;
> Finally, study various architectural optimization methods and use scientific evaluation tools to understand the actual benefits these optimizations bring to program execution.
>
> This learning approach differs significantly from textbooks,
> as processor architecture design capabilities can only be developed through practice,
> but textbook-based theoretical classrooms are constrained by the curriculum system and cannot assess students' computer architecture design capabilities.
> Therefore, you can easily get started with textbooks or reference books,  
> but if you want to become a professional in this field, you must understand the limitations of these books,  
> and at the necessary stage, go beyond the boundaries of the books and develop true computer architecture design capabilities through targeted training.


<!-- ## 存储层次结构和局部性原理

校准ysyxSoC的访存延迟后, 你应该发现性能瓶颈在于指令供给:
取一条指令都要等待数十上百个周期, 流水线根本没法流水.
要提升指令供给能力, 最合适的就是使用缓存技术.
不过在介绍缓存之前, 我们需要先了解计算机的存储层次结构和局部性原理. -->

## Memory Hierarchy and Locality Principle

After calibrating the memory access latency of the ysyxSoC, you should find that the performance bottleneck lies in instruction supply:  
retrieving a single instruction requires waiting for dozens or even hundreds of cycles, rendering the pipeline ineffective.  
To enhance instruction supply capabilities, the most suitable approach is to utilize cache technology.  
However, before introducing caches, we must first understand the computer's memory hierarchy and locality principle.

<!-- ### 存储层次结构(Memory Hierarchy)

计算机中存在不同的存储介质, 如寄存器, 内存, 硬盘和磁带,
它们具有不同的物理特性, 因而各种指标也有所不同.
可以从访问时间, 容量和成本这几个方面评估它们.

```
access time     /\        capacity    price
               /  \
   ~1ns       / reg\        ~1KB     $$$$$$
             +------+
   ~10ns    /  DRAM  \      ~10GB     $$$$
           +----------+
   ~10ms  /    disk    \    ~1TB       $$
         +--------------+
   ~10s /      tape      \  >10TB       $
       +------------------+
```

* 寄存器. 寄存器的访问时间很短, 基本与CPU的主频一致.
  目前的商业级高性能CPU主频大约3GHz, 因此寄存器访问时间小于1ns.
  寄存器的容量很小, 通常小于1KB.
  例如, RV32E有16个32位寄存器, 大小是512b.
  此外，寄存器的制造成本较为昂贵, 若使用大量寄存器, 则将占用不少流片面积.
* DRAM. DRAM的访问时间大约为10ns的量级, 其容量比寄存器大得多, 常用于内存.
  其成本也低很多, 某电商平台上16GB内存条的价格是329元, 约20元/GB.
* 机械硬盘. 机械硬盘的访问时间受限于其机械部件, 如盘片需要旋转, 通常需要10ms的量级.
  相对地, 机械硬盘也拥有更大的容量, 通常能达到数TB;
  其成本也更便宜, 某电商平台上4TB机械硬盘的价格是569元, 约0.139元/GB.
* 固态硬盘. 固态硬盘也是目前流行的存储介质, 其存储单元采用NAND flash,
  基于电的特性进行工作, 因此访问速度比机械硬盘快得多, 其读延迟接近DRAM,
  但受限于flash单元的特性, 写延迟还是比DRAM高很多.
  其成本稍高于机械硬盘, 某电商平台上1TB固态硬盘的价格是699元, 约0.683元/GB.
* 磁带. 磁带的存储容量非常大, 成本也非常低, 但访问时间很长, 约10s的量级,
  因此目前已很少使用, 通常用在数据备份的场景.
  某电商平台上30TB磁带机的价格是1000元, 约0.033元/GB.

可见, 由于存储介质物理特性的限制, 没有一种存储器能同时满足容量大, 速度快, 成本低等各种指标.
因此, 计算机通常集成多种存储器, 并通过一定的技术将它们有机组织起来, 形成存储层次结构,
在整体上达到容量大, 速度快, 成本低的综合指标.
这听上去有点不可思议, 不过关键在于如何把各种存储器有机组织起来. -->

### Memory Hierarchy

Computers utilize various storage media, such as registers, memory, hard disks, and tapes,  
which possess distinct physical characteristics and thus exhibit differing performance metrics.  
These can be evaluated based on access time, capacity, and cost.

```
access time     /\        capacity    price
               /  \
   ~1ns       / reg\        ~1KB     $$$$$$
             +------+
   ~10ns    /  DRAM  \      ~10GB     $$$$
           +----------+
   ~10ms  /    disk    \    ~1TB       $$
         +--------------+
   ~10s /      tape      \  >10TB       $
       +------------------+
```

* Registers. Register access time is very short, essentially matching the CPU's clock speed.
  Current commercial-grade high-performance CPUs have clock speeds of approximately 3GHz, so register access time is less than 1ns.
  Register capacity is very small, typically less than 1KB.
  For example, the RV32E has 16 32-bit registers, each 512b in size.
  Additionally, the manufacturing cost of registers is relatively high, and using a large number of registers will occupy a significant amount of chip area.
* DRAM. DRAM access time is approximately 10 ns, and its capacity is much larger than that of registers, commonly used for memory.
  Its cost is also much lower; a 16GB memory module on a certain e-commerce platform costs 329 CNY, approximately 20 CNY per GB.
* Mechanical hard drives. The access time of mechanical hard drives is limited by their mechanical components, such as the need for the platters to rotate, typically requiring around 10ms.  
  In contrast, mechanical hard drives also have much larger capacities, typically reaching several TB;  
  their cost is also cheaper, with a 4TB mechanical hard drive priced at 569 CNY on a certain e-commerce platform, approximately 0.139 CNY per GB.
* Solid-state drives (SSDs). SSDs are also a popular storage medium today, using NAND flash memory cells,  
which operate based on electrical properties, resulting in significantly faster access speeds than mechanical hard drives, with read latency approaching that of DRAM,  
but write latency remains much higher than DRAM due to the characteristics of flash memory cells.
  Their cost is slightly higher than that of mechanical hard drives. On a certain e-commerce platform, the price of a 1TB solid-state drive is 699 CNY, approximately 0.683 CNY per GB.
* Magnetic tape. Magnetic tape has a very large storage capacity and very low cost, but its access time is very long, approximately 10 seconds,  
so it is rarely used today and is typically employed in data backup scenarios.
  On a certain e-commerce platform, a 30TB tape drive costs 1,000 CNY, approximately 0.033 CNY per GB.

As can be seen, due to the physical limitations of storage media, no single storage device can simultaneously meet all criteria such as large capacity, high speed, and low cost.
Therefore, computers typically integrate multiple storage devices and organize them through certain technologies to form a storage hierarchy,  
achieving a balanced combination of large capacity, high speed, and low cost overall.  
This may seem somewhat unbelievable, but the key lies in how to effectively organize various storage devices.

<!-- ### 局部性原理

其实上述的组织方式是有讲究的, 其中的奥秘就是程序的局部性原理.
计算机架构师发现, 程序在一段时间内对存储器的访问通常集中在一个很小的范围:
* 时间局部性 - 访问一个存储单元后, 短时间内可能再次访问它
* 空间局部性 - 访问一个存储单元后, 短时间内可能访问它的相邻存储单元

上述现象与程序的结构和行为有关:
* 程序大多数时候将顺序执行或循环执行, 二者分别遵循空间局部性和时间局部性
* 编写程序时, 相关的变量在源代码中的位置相距不远, 或者采用结构体来组织,
  编译器也会为其分配分配相近的存储空间, 从而呈现出空间局部性
* 程序执行过程中访问变量的次数通常不小于变量的数量(否则将存在未被使用的变量),
  因此必定有变量会被多次访问, 从而呈现出时间局部性
* 对于数组, 程序通常使用循环来遍历, 从而呈现出空间局部性 -->


### Locality Principle

In fact, the above organizational method is carefully designed, and the secret lies in the locality principle of programs.
Computer architects have discovered that a program's access to memory over a period of time is typically concentrated within a very small range:
* Temporal locality - After accessing a memory unit, it may be accessed again shortly thereafter
* Spatial locality - After accessing a memory unit, its adjacent memory units may be accessed shortly thereafter

These phenomena are related to the structure and behavior of programs:
* Programs typically execute sequentially or in loops, with sequential execution following spatial locality and loop execution following temporal locality
* When writing programs, related variables are often located close to each other in the source code or organized using structures,  
  and compilers also allocate adjacent memory spaces for them, thereby exhibiting spatial locality
* During program execution, the number of times a variable is accessed is typically no less than the number of variables (otherwise there would be unused variables),  
  so there must be variables that are accessed multiple times, thereby exhibiting temporal locality
* For arrays, programs typically use loops to iterate through them, thereby exhibiting spatial locality

<!-- > #### option::观察程序的局部性
> 程序的局部性和内存访问有关, 很自然地, 我们可以通过mtrace来观察它!
> 在NEMU中运行一些程序, 并获取mtrace.
> 之后, 你需要对mtrace的输出进行二次处理, 尝试借助一些绘图工具呈现你的结果. -->

> #### option::Observing program locality
> Program locality is related to memory access, so naturally, we can observe it using mtrace!
> Run some programs in NEMU and obtain mtrace.
> Afterward, you need to process the mtrace output further and try to visualize your results using some plotting tools.

<!-- -->
<!-- > #### question::链表的局部性
> 遍历链表的过程中是否呈现出局部性?
> 尝试比较访问数组元素和链表元素时, 何者的局部性更优.

局部性原理告诉我们, 程序对存储器的访问表现出集中的特性.
这说明, 即使慢速存储器的容量很大, 程序在一段时间内只会访问很小的一部分数据.
既然如此, 我们可以先将这部分数据从慢速存储器中搬运到快速存储器中, 然后在快速存储器中访问它们.

这就是存储层次结构中各种存储器之间组织方式的诀窍:
将各种存储器按层次排列, 上层存储器速度快但容量小, 下层存储器容量大但速度慢;
访问数据时, 先访问速度较快的上层存储器,
如果数据在当前层级(称为命中), 则直接访问当前层级的数据;
否则(称为缺失), 就在下一层级寻找, 下层将目标数据及其相邻数据传递给上层.
其中, "把目标数据传递给上层存储器"利用了时间局部性, 期望下次访问目标数据时能在速度快的存储器中命中;
而"把相邻数据传递给上层存储器"则利用了空间局部性, 期望下次访问相邻数据时也能在速度快的存储器中命中.

例如, 在访问DRAM时, 如果数据不存在, 则访问机械硬盘, 并将目标数据及其相邻数据搬运到DRAM中,
下次访问这些数据时, 即可在DRAM中命中, 从而直接访问DRAM中数据.
通过这种方式, 我们近似得到了一个访问速度接近DRAM, 容量接近机械硬盘的存储器!
在成本方面, 以上文电商平台的报价为例, 一根16GB的内存条和一块4TB的机械硬盘的总价格不到900元,
但如果要采购4TB的内存条, 则需要`329 * (4TB / 16GB) = 84224`元!

当然, 天下没有免费的午餐, 要实现上文的效果是有条件的, 计算机系统的设计需要满足局部性原理:
一方面, 计算机系统需要设计并实现存储层次结构;
另一方面, 程序员也需要开发出局部性较好的程序, 使其在存储层次结构中能获得较好的性能.
如果程序的局部性较差, 访问的数据不具备集中的特性,
将导致大部分访问都无法在快速存储器中命中, 从而使得整个系统的表现接近于访问慢速存储器. -->

> #### question::Locality of linked lists
> Does traversing a linked list exhibit locality?
> Try comparing the locality of accessing array elements versus linked list elements to determine which exhibits better locality.

The principle of locality tells us that program access to memory exhibits concentrated characteristics.  
This means that even if slow memory has a large capacity, the program will only access a small portion of the data over a period of time.  
Given this, we can first move this portion of data from slow memory to fast memory, and then access it from fast memory.  

This is the key to how different types of memory are organized within the memory hierarchy:
memories are arranged in a hierarchical structure, with upper-level memories being faster but smaller in capacity, and lower-level memories being larger in capacity but slower;  
when accessing data, the faster upper-level memory is accessed first;  
if the data is found in the current level (referred to as a hit), it is directly accessed from the current level;  
otherwise (referred to as a miss), it is searched for in the next lower level, with the lower level passing the target data and its adjacent data to the upper level.
Here, “passing the target data to the upper-level storage” leverages temporal locality, aiming to achieve a hit in the faster storage when accessing the target data again;  
while “passing adjacent data to the upper-level storage” leverages spatial locality, aiming to achieve a hit in the faster storage when accessing adjacent data again.

For example, when accessing DRAM, if the data is not present, access the mechanical hard drive and move the target data and its adjacent data to DRAM.  
When accessing these data again, they can be fetched from DRAM, enabling direct access to the data in DRAM.  
Through this approach, we approximate a memory with access speed close to DRAM and capacity close to a mechanical hard drive!
In terms of cost, using the pricing example from the e-commerce platform mentioned earlier, the total cost of a 16GB memory module and a 4TB mechanical hard drive is less than 900 CNY,  
but if you were to purchase a 4TB memory module, it would cost `329 * (4TB / 16GB) = 84,224` CNY!

Of course, there's no such thing as a free lunch. Achieving the above results comes with conditions: the computer system design must adhere to the principle of locality:  
On one hand, the computer system must design and implement a storage hierarchy;  
On the other hand, programmers must develop programs with good locality to achieve optimal performance within the storage hierarchy.
If a program has poor locality and the data being accessed lacks centralized characteristics,  
most accesses will fail to hit in fast memory, resulting in system performance approaching that of accessing slow memory.  

<!-- ## 简易缓存

### 缓存介绍

回到上文的性能瓶颈, 为了优化指令供给, 实际上我们需要做的是提升访问DRAM的效率.
为此, 我们只需要按照计算机存储层次结构的思想, 在寄存器和DRAM之间添加一层存储器即可.
这就是缓存(cache)的思想.
也即, 在访问DRAM之前, 先访问cache, 若命中, 则直接访问;
若缺失, 则先将数据从DRAM读入cache, 然后再访问cache中的数据.

上面的cache属于狭义范畴, 指的是处理器缓存(CPU cache).
事实上, 广义的cache并不仅仅是指代访存通路上的那个硬件模块,
在计算机系统中, cache无处不在:
磁盘控制器中也包含cache, 用于缓存从磁盘中读出的数据;
我们之前介绍的SDRAM中的行缓冲, 本质上也是SDRAM存储阵列的缓存;
操作系统也会通过软件的方式为磁盘等存储设备维护一个缓存,
用于存储最近访问的数据, 这个缓存本质上是一个大型结构体数组,
它在内存中分配, 因此操作系统负责在磁盘和内存之间进行数据搬运;
缓存对分布式系统也是至关重要, 如果要访问的数据不在本地缓存,
就需要访问远端, 浏览器的网页缓存和视频内容缓存都属于这种情况.

回到CPU cache, 谁来将数据从DRAM读入cache呢?
事实上, 谁来完成上下两个存储层次之间的数据搬运, 取决于谁能访问这两个存储层次.
计算机中的主体只有软件程序和硬件电路, 而软件程序的本质是指令序列.
虽然指令可以访问DRAM, 但指令集手册中定义的编程模型通常不包含cache,
也就是说从功能上来说, cache对软件程序不可见,
因此只能是硬件电路来完成上述读入操作(后面我们会看到, 本质上还是状态机).
而对于DRAM和磁盘这两个存储层次, 指令可以访问DRAM, 也可以通过MMIO访问作为设备的磁盘,
因此可以由软件负责将数据从磁盘读入DRAM, 上文提到的操作系统就是如此.
当然, 原则上你也可以设计一个硬件模块, 专门完成DRAM和磁盘之间的数据搬运工作,
但这个硬件模块通常只能适配一种磁盘, 其灵活性远不如操作系统中的驱动程序. -->
## Simple Cache  

### Cache Introduction  

Returning to the performance bottleneck mentioned earlier, to optimize instruction supply, what we actually need to do is improve the efficiency of accessing DRAM.
To achieve this, we simply need to add an intermediate storage layer between the registers and DRAM, following the principles of the computer storage hierarchy.
This is the concept of a cache (cache).
That is, before accessing DRAM, first access the cache; if a hit occurs, access directly;
if a miss occurs, first read the data from DRAM into the cache, then access the data in the cache.

The cache mentioned above falls under the narrow definition, referring specifically to the processor cache (CPU cache).  
In fact, the broader concept of cache extends beyond the hardware module on the memory access path.  
Caches are ubiquitous in computer systems:  
disk controllers also include caches to store data read from disks;
The row buffer in SDRAM, which we previously discussed, is essentially also a cache for the SDRAM memory array;
Operating systems also maintain a cache for storage devices like disks through software,
used to store recently accessed data. This cache is essentially a large array of structures,
allocated in memory, so the operating system is responsible for moving data between the disk and memory;
Caches are also critical for distributed systems. If the data to be accessed is not in the local cache,  
it must be accessed from a remote location. Browser webpage caches and video content caches fall into this category.

Returning to the CPU cache, who reads data from DRAM into the cache?  
In fact, who performs the data transfer between the two storage levels depends on who can access both levels.
The only entities in a computer are software programs and hardware circuits, and the essence of software programs is a sequence of instructions.  
Although instructions can access DRAM, the programming model defined in the instruction set manual typically does not include the cache,  
meaning that, functionally speaking, the cache is invisible to software programs,  
so it must be the hardware circuits that perform the aforementioned read operation (as we will see later, it is essentially a state machine).
For the two storage levels of DRAM and disk, instructions can access DRAM and can also access the disk as a device via MMIO,  
so software can be responsible for reading data from the disk into DRAM, as mentioned earlier in the operating system.
Of course, in principle, you could design a hardware module specifically to handle data transfer between DRAM and disk,  
but such a hardware module typically only supports one type of disk and lacks the flexibility of a driver in an operating system.

<!-- > #### comment::对软件程序可见的cache
> 在一些处理器中, cache是对软件程序可见的.
> 例如CUDA GPU编程模型中的shared memory, 在组织层次上和CPU cache一样,
> 都是寄存器和DRAM之间的一层存储器;
> 但和CPU cache不同的是, GPU提供了专门用于访问shared memory的访存指令,
> 因此GPU程序可以通过指令来将数据从内存中读入shared memory的指定位置.

为了方便描述, 我们将从DRAM读入的数据称为一个数据块,
cache中存放的数据块称为cache块(有的教科书也称为cache行, 英文为cache line).
自然地, 设计cache需要关注如下问题:
* 数据块的大小应该是多少?
* 如何检查访存请求是否在cache中命中?
* cache的容量通常比DRAM小, 如何维护cache块和DRAM中数据块之间的映射关系? cache满了后怎么办?
* CPU可能会执行写操作, 从而更新数据块中的数据, cache应如何维护? -->

> #### comment::Cache visible to software programs  
> In some processors, the cache is visible to software programs.
> For example, shared memory in the CUDA GPU programming model is organized at the same level as CPU cache,  
> serving as an intermediate layer of storage between registers and DRAM;  
> however, unlike CPU cache, the GPU provides specialized memory access instructions for accessing shared memory,  
> enabling GPU programs to read data from memory into specific locations in shared memory via instructions.

For convenience, we refer to data read from DRAM as a data block,  
and data blocks stored in the cache as cache blocks (some textbooks also refer to them as cache lines).  
Naturally, cache design must address the following issues:
* What should the size of a data block be?
* How should cache hits be determined for memory access requests?
* Since cache capacity is typically smaller than DRAM, how should the mapping relationship between cache blocks and DRAM data blocks be maintained? What happens when the cache is full?
* The CPU may perform write operations to update data in data blocks; how should the cache be maintained in such cases?

<!-- ### 简易指令缓存

我们先来考虑指令缓存(instruction cache, 简称icache),
由于IFU的取指过程无需写入内存, 因此icache是只读的,
我们可以先不考虑如何处理CPU写入数据块的情况.
至于块大小, 我们先取一条指令的长度, 即4B.
这也许不是一个最好的设计, 但对icache来说, 小于4B肯定不是一个好的设计,
否则取出一条新指令时, 还需要进行多次访存.
至于大于4B是否更好, 我们后面再来评估.

为了检查访存请求是否在cache中命中, 很自然地,
除了存储数据块本身, cache还需要记录块的一些属性.
最直接的方式就是记录这个块的一种唯一编号,
不过我们还希望, 这个唯一编号的计算方式要足够简单.
既然数据块是从内存来的, 我们可以按块大小对内存进行编号,
内存地址`addr`对应的数据块的编号即为`addr / 4`, 这个编号称为块的标签(tag).
这样, 我们只要计算出访存地址的tag, 然后和每个cache块的tag对比,
就能知道目标块是否在cache中. -->

## Simple Instruction Cache

Let's first consider the instruction cache (ICache).
Since the IFU's instruction fetch process does not require writing to memory, the ICache is read-only.
We can temporarily disregard how to handle CPU writes to data blocks.
Regarding block size, we will initially use the length of a single instruction, which is 4B.
This may not be the best design, but for the icache, anything less than 4B is definitely not a good design,  
as retrieving a new instruction would require multiple memory accesses.  
Whether a size greater than 4B is better will be evaluated later.

To check if a memory access request is a cache hit, it is natural that,  
in addition to storing the data block itself, the cache must also record some attributes of the block.
The most direct way is to record a unique identifier for the block,  
but we also want the calculation method for this unique identifier to be sufficiently simple.  
Since the data block comes from memory, we can number the memory based on block size,  
and the identifier for the data block corresponding to memory address `addr` is `addr / 4`, which is called the block's tag.
Thus, we only need to calculate the tag of the memory access address and compare it with the tags of each cache block  
to determine whether the target block is in the cache.

<!-- 然后来考虑cache块的组织问题.
根据存储层次结构, cache的容量不可能和DRAM一样大, 通常也不可能小到只有1个cache块,
因此需要考虑读入一个新块时, 应该将其读入到哪个cache块中.
由于cache中有多个cache块, 因此我们也可以给cache块进行编号.
最简单的组织方式就是将一个新块读入到固定的cache块中, 这种组织方式称为直接映射(direct-mapped).
为此, 我们需要明确从内存地址`addr`到cache块号的映射关系.
假设cache可存放`k`个cache块, 一种简单的映射关系是`cache块号 = (addr / 4) % k`,
也即, 对于内存地址为`addr`的数据块, 它将被读入编号为`(addr / 4) % k`的cache块.

显然, 多个数据块可能会映射到相同的cache块,
这时需要决定应该保留已有的cache块, 还是往cache块中读入新块.
根据程序的局部性原理, 将来访问新块的概率更大,
因此读入新块时, 应该将已有的cache块替换为新块,
使得接下来一段时间内访问新块时能都在cache中命中.

我们可以把所有cache块看作一个数组, cache块号就是数组的索引(index), 因此cache块号也称为块索引.
对于块大小是`b`字节, 共`k`个cache块的直接映射cache, 有`tag = addr / b`, `index = (addr / b) % k`.
为了方便计算, 通常取`b`和`k`为2的幂, 假设`b = 2^m`, `k = 2^n`.
假设`addr`为32位, 则有`tag = addr / 2^m = addr[31:m]`, `index = (addr / 2^m) % 2^n = addr[m+n-1:m]`,
可见, index实际上是tag中的低`n`位.
由于在直接映射的cache中, index不同的数据块必定会被映射到不同的cache块中,
即使它们的tag的高`m`位(即`addr[31:m+n]`)相同,
因此, 记录tag时只需要记录`addr[31:m+n]`即可. -->

Next, we consider the organization of cache blocks.  
According to the storage hierarchy, the cache capacity cannot be as large as DRAM, and it is also unlikely to be as small as a single cache block.  
Therefore, we need to consider which cache block to read a new block into when it is read into the cache.
Since there are multiple cache blocks in the cache, we can also assign numbers to the cache blocks.  
The simplest organization method is to read a new block into a fixed cache block, which is called direct mapping.  
For this, we need to clearly define the mapping relationship between the memory address `addr` and the cache block number.
Assuming the cache can store `k` cache blocks, a simple mapping relationship is `cache block number = (addr / 4) % k`,  
meaning that for a data block with memory address `addr`, it will be read into the cache block numbered `(addr / 4) % k`.

Obviously, multiple data blocks may map to the same cache block.  
In this case, we need to decide whether to keep the existing cache block or read the new block into the cache block.  
According to the principle of program locality, the probability of accessing the new block in the future is higher.  
Therefore, when reading the new block, we should replace the existing cache block with the new block,  
so that subsequent accesses to the new block can be cached.

We can view all cache blocks as an array, where the cache block number serves as the array index (index), hence the cache block number is also referred to as the block index.
For a direct mapping cache with a block size of `b` bytes and `k` cache blocks, we have `tag = addr / b` and `index = (addr / b) % k`.
For convenience in calculation, `b` and `k` are typically chosen as powers of 2, assuming `b = 2^m` and `k = 2^n`.
Assuming `addr` is 32 bits, we have `tag = addr / 2^m = addr[31:m]` and `index = (addr / 2^m) % 2^n = addr[m+n-1:m]`.  
It can be seen that the index is actually the lower `n` bits of the tag.
In a direct-mapped cache, data blocks with different indices are always mapped to different cache blocks,  
even if their upper `m` bits of the tag (i.e., `addr[31:m+n]`) are the same.  
Therefore, when recording the tag, it is sufficient to record only `addr[31:m+n]`.


<!-- 一个访存地址可以划分成以下3部分: tag, index, offset.
其中tag部分作为数据块在cache中的唯一编号, index部分作为数据块在cache中的索引,
offset部分属于块内偏移, 指示需要访问数据块中的哪部分数据.

```
 31    m+n m+n-1   m m-1    0
+---------+---------+--------+
|   tag   |  index  | offset |
+---------+---------+--------+
```

最后, 在系统复位时, cache中无任何数据, 此时所有cache块均无效.
为了标识一个cache块是否有效, 需要为每个cache块添加一个有效位(valid).
valid和tag统称为cache块的元数据(metadata), 其含义是用于管理数据的数据, 此处被管理的数据就是cache块. -->

A memory access address can be divided into the following three parts: tag, index, and offset.
The tag part serves as the unique identifier for the data block in the cache, the index part serves as the index for the data block in the cache,
and the offset part is the block-internal offset, indicating which part of the data block needs to be accessed.

```
 31    m+n m+n-1   m m-1    0
+---------+---------+--------+
|   tag   |  index  | offset |
+---------+---------+--------+
```

Finally, when the system is reset, there is no data in the cache, and all cache blocks are invalid.
To identify whether a cache block is valid, a validity bit (valid) must be added to each cache block.
The valid and tag bits are collectively referred to as the cache block's metadata, which is data used to manage data, with the data being managed here being the cache block.

<!-- 综上, 上述icache的工作流程如下:
1. IFU向icache发送取指请求
1. icache获得取指请求的地址后, 根据index部分索引出一个cache块,
   判断其tag与请求地址的tag是否相同, 并检查该cache块是否有效.
   若同时满足上述条件, 则命中, 跳转到第5步
1. 通过总线在DRAM中读出请求所在的数据块
1. 将该数据块填入相应cache块中, 更新元数据
1. 向IFU返回取出的指令

整理出工作流程后, 你应该知道如何实现icache了: 还是状态机!
上述工作流程甚至包含了一次总线的访问, 因此icache的实现也可以看成是总线状态机的扩展.
你已经很熟悉总线的实现了, 因此如何实现icache的状态机, 就交给你来梳理吧! -->

In summary, the workflow of the ICache is as follows:
1. The IFU sends an instruction fetch request to the ICache
1. After obtaining the address of the fetch request, the icache uses the index portion to index a cache block,  
checks whether its tag matches the tag of the requested address, and verifies the validity of the cache block.  
If all these conditions are met, it is a hit, and the process jumps to step 5.  
1. Reads the requested data block from DRAM via the bus.  
1. Fills the data block into the corresponding cache block and updates the metadata.
1. Return the fetched instruction to the IFU.

After organizing the workflow, you should know how to implement the icache: it's still a state machine!
The above workflow even includes a bus access, so the implementation of the icache can also be seen as an extension of the bus state machine.
You are already familiar with the implementation of the bus, so how to implement the icache state machine is up to you to figure out!

<!-- > #### todo::实现icache
> 根据上述流程, 实现一个简单的icache, 块大小为4B, 共16个cache块.
> 一般来说, cache的存储阵列(包括数据和元数据)都通过SRAM来实现,
> 但在ASIC流程中使用SRAM涉及到选型和实例化, 其中SRAM的选型可能会影响到数据和元数据的存放.
> 作为第一个cache的练习, 为简单起见, 此处先通过触发器来实现存储阵列, 提高实现的灵活性.
>
> 实现时, 建议将相关参数实现成可配置的, 以便于后续评估不同配置参数的性能表现.
> 实现后, 尝试评估其性能表现. -->

> #### todo::Implement the icache
> Based on the above workflow, implement a simple icache with a block size of 4B and a total of 16 cache blocks.
> Generally, the cache storage array (including data and metadata) is implemented using SRAM,
> but using SRAM in the ASIC process involves selection and instantiation, where the selection of SRAM may affect the storage of data and metadata.
> As the first cache exercise, for simplicity, the storage array is first implemented using flip-flops to enhance implementation flexibility.
>
> During implementation, it is recommended to design relevant parameters as configurable to facilitate subsequent performance evaluation of different configuration parameters.
> After implementation, attempt to evaluate its performance.

<!-- -->
<!-- > #### todo::适合缓存的地址空间
> 并不是所有的地址空间都适合缓存, 只有存储器类型的地址空间才适合缓存.
> 此外, SRAM的访问延迟本身就只有1周期, 因此也无需缓存,
> 将缓存块留给其他的地址空间是一个更合适的方案. -->

> #### todo::Address spaces suitable for caching
> Not all address spaces are suitable for caching; only memory-type address spaces are suitable for caching.
> Additionally, SRAM access latency is only 1 cycle, so caching is unnecessary.
> Allocating cache blocks to other address spaces is a more appropriate solution.

<!-- -->
<!-- > #### todo::估算dcache的理想收益
> 通常, LSU也有与其配对的缓存, 称为数据缓存(data cache, 简称dcache).
> 在实现dcache之前, 我们可以先估计它在理想情况下的性能收益.
> 假设dcache的容量无限大, 访问dcache的命中率为100%, 且dcache的访问延迟为1周期,
> 尝试根据性能计数器估算添加这样一个dcache带来的性能收益.
>
> 如果你的估算正确, 你应该发现此时添加dcache是不值得的, 我们将在下文继续讨论这个问题. -->

> #### todo::Estimating the ideal performance gain of the dcache
> Typically, LSUs are paired with a cache called the data cache (dcache).
> Before implementing the dcache, we can first estimate its performance gain under ideal conditions.
> Assuming the dcache has infinite capacity, a 100% hit rate when accessing the dcache, and a latency of 1 cycle,  
> attempt to estimate the performance benefit of adding such a dcache based on performance counters.  
>  
> If your estimate is correct, you should find that adding the dcache is not worthwhile at this point. We will continue to discuss this issue in the following sections.

<!-- ## 形式化验证

借助DiffTest, 你应该很容易保证接入icache后, 给定的程序仍然可以正确运行.
但如何保证icache对于任意程序都能正确运行呢?

这看上去是一个很困难的问题, 相信你一定碰到过这种情况:
代码运行给定的测试用例都是正确的, 但哪天运行一个其他的测试, 就会出错.
无论是从原理上分析还是从实践中总结, 光靠测试是无法证明一个模块的正确性的,
除非这些测试用例覆盖了所有程序, 或者是覆盖了被测试模块的所有输入情况.
程序的数量是无限的, 要把所有程序都测试一遍并不现实,
不过被测试模块的输入是有限的, 至少遍历所有输入在理论上是可行的.

如果要遍历一个模块的所有输入, 一方面要生成可以覆盖所有输入情况的测试集,
另一方面还需要有方法能判断一个具体的输入是否正确.
即使这些能做到, 要把所有测试全部运行完, 也需要很长时间, 这通常是难以忍受的.
软件测试理论中的等价类测试方法可以把本质行为相似的测试进行归类,
从等价类中选择一个测试来代表整个等价类的测试, 从而降低测试集的大小.
但等价类应该如何划分, 是需要根据被测模块的逻辑人工决定的.
不过, 根据软件领域另一则广泛流传的忠告, 需要人工干预的流程, 都会存在出错的风险. -->

## Formal Verification

With DiffTest, you should be able to easily ensure that the given program still runs correctly after integrating icache.  
But how can we ensure that icache runs correctly for any program?  

This seems like a difficult problem, and I'm sure you've encountered this situation before:  
the code runs correctly for the given test cases, but one day when running another test, it fails.
Whether analyzed from a theoretical perspective or summarized from practical experience, testing alone cannot prove the correctness of a module,  
unless the test cases cover all programs or all input scenarios of the tested module.  
The number of programs is infinite, so testing all programs is impractical,  
but the inputs of the tested module are finite, so at least iterating through all inputs is theoretically feasible.

To traverse all inputs of a module, one must generate a test set that covers all input scenarios,  
and also have a method to determine whether a specific input is correct.  
Even if these are achievable, running all tests would take a significant amount of time, which is often intolerable.
The equivalence class testing method in software testing theory can categorize tests with similar essential behaviors,  
select one test from the equivalence class to represent the entire equivalence class, thereby reducing the size of the test set.  
However, how to divide equivalence classes requires manual decision-making based on the logic of the tested module.  
However, according to another widely circulated piece of advice in the software field, any process requiring manual intervention carries the risk of errors.

<!-- ### 形式化验证的基本原理

那么, 能不能让工具帮我们自动寻找测试用例呢? 还真有这种工具!
求解器(Solver)是一类在给定约束条件下寻找可行解的数学工具,
其本质类似求解方程组或线性规划等数学问题.
例如, [Z3][z3]是一个[可满足性模理论(Satisfiablity Modulo Theories, SMT)][smt]问题的求解器,
它可以求解一个包含实数, 整数, 比特, 字符, 数组, 字符串等内容的命题是否成立.
事实上, 只要能将问题表达成一阶逻辑语言的某个子集, 就能交给SMT求解器来求解,
因此SMT求解器也可以用来求解类似数独的复杂问题.
SMT求解器广泛应用于定理自动证明, 程序分析, 程序验证和软件测试等领域.
下面是一个在python中使用Z3求解方程组的例子.

```python
#!/usr/bin/python
from z3 import *

x = Real('x')  # 定义变量
y = Real('y')
z = Real('z')
s = Solver()
s.add(3*x + 2*y - z == 1)    # 定义约束条件
s.add(2*x - 2*y - 4*z == -2)
s.add(-x + 0.5*y - z == 0)
print(s.check())  # 求是否存在可行解: sat
print(s.model())  # 输出可行解: [y = 14/25, x = 1/25, z = 6/25]
```

[z3]: https://github.com/Z3Prover/z3
[smt]: https://en.wikipedia.org/wiki/Satisfiability_modulo_theories

在测试验证领域, 有一种称为形式化验证(formal verification)的验证方法,
该方法的一种技术路线是模型检测(model checking),
相应的验证工具称为模型检测器(model checker).
模型检测的核心是求解器, 具体地, 模型检测器将设计作为约束条件,
将输入作为变量, 将"至少一个验证条件不成立"作为求解目标,
把这些内容用一阶逻辑语言表达出来, 并转换成求解器识别的语言, 然后尝试让求解器寻找是否存在可行解.
例如, 若某设计中有`assert(cond1)`和`assert(cond2)`这两个验证条件,
则尝试让求解器寻找是否存在输入, 使得`!cond1 || !cond2`成立.
若可行解存在, 则说明求解器找到了一个违反验证条件的测试用例, 这个反例可以帮助我们调试并改进设计;
若可行解不存在, 则说明所有输入都不会违反验证条件, 从而证明了设计的正确性!
可见, 无论求解器能否找到可行解, 对设计者来说都是极好的消息.

通常来说, 一个系统的状态还跟时间有关,
例如, 处理器每经过一个周期, 其状态都可能会发生改变.
如果支持在所有时间长度下验证某条件, 则该方法称为无界模型检测(Unbounded Model Checking).
但由于无界模型检测的检测开销很大, 因此实际上更多采用的是有界模型检测(Bounded Model Checking, BMC).
BMC通常需要一个参数`k`, 它只验证某条件在经过最多时间`k`的情况下是否成立, 这个参数称为BMC的界(bound). -->

### Basic Principles of Formal Verification

Can tools help us automatically find test cases? There are indeed such tools!
Solvers are mathematical tools that find feasible solutions under given constraints,
similar in essence to solving systems of equations or linear programming problems.
For example, [Z3][z3] is a solver for [Satisfiability Modulo Theories (SMT)][smt] problems,  
which can determine whether a proposition containing real numbers, integers, bits, characters, arrays, strings, etc., is valid.
In fact, as long as a problem can be expressed as a subset of first-order logic, it can be solved by an SMT solver,  
so SMT solvers can also be used to solve complex problems like Sudoku.  
SMT solvers are widely used in fields such as automated theorem proving, program analysis, program verification, and software testing.  
Below is an example of using Z3 to solve a system of equations in Python.

```python
#!/usr/bin/python
from z3 import *

x = Real(‘x’)  # Define variables
y = Real(‘y’)
z = Real(‘z’)
s = Solver()
s.add(3*x + 2*y - z == 1)    # Define constraints
s.add(2*x - 2*y - 4*z == -2)
s.add(-x + 0.5*y - z == 0)
print(s.check())  # Check if a feasible solution exists: sat
print(s.model())  # Output feasible solution: [y = 14/25, x = 1/25, z = 6/25]
```

[z3]: https://github.com/Z3Prover/z3
[smt]: https://en.wikipedia.org/wiki/Satisfiability_modulo_theories

In the field of testing and verification, there is a verification method called formal verification.
One technical approach to this method is model checking.
The corresponding verification tool is called a model checker.
The core of model checking is the solver. Specifically, the model checker treats the design as constraints,
the input as variables, and “at least one verification condition is not satisfied” as the solution objective.
These elements are expressed using first-order logic and converted into a language recognizable by the solver, which then attempts to find a feasible solution.
For example, if a design contains the verification conditions `assert(cond1)` and `assert(cond2)`,  
the solver attempts to find an input such that `!cond1 || !cond2` holds.
If a feasible solution exists, it means the solver has found a test case that violates the verification condition, and this counterexample can help us debug and improve the design;  
if no feasible solution exists, it means no input will violate the verification condition, thereby proving the correctness of the design!  
As can be seen, whether or not the solver finds a feasible solution, it is excellent news for the designer.

Generally speaking, the state of a system is also related to time.  
For example, the state of a processor may change after each cycle.  
If a condition can be verified at all time intervals, the method is called unbounded model checking (UMC).  
However, due to the high computational cost of UMC, bounded model checking (BMC) is more commonly used in practice.
BMC typically requires a parameter `k`, which only verifies whether a condition holds after a maximum of `k` time units. This parameter is called the bound of BMC.

<!-- > #### caution::不要迷信UVM测试的100%覆盖率报告
> 如果你了解UVM, 你应该知道UVM的目标是提升覆盖率.
> 但如果你认为提升覆盖率是测试验证的最终目标, 那你很有可能还不了解测试验证.
>
> 事实上, 测试验证的最终目标, 是证明设计的正确性, 或者是找出设计中的所有bug.
> 但有经验的工程师都知道, 即使做到了100%的覆盖率, 还是可能会有一些bug没有被测出来,
> 而且还估算不了这些没被测出来的bug还有多少.
>
> 之所以覆盖率的目标被企业广泛采用, 一方面是因为覆盖率是一个容易量化和统计的指标.
> 如果用严格的语言来描述"一个事件被覆盖", 就是
> ```
> 存在一个测试用例, 其运行成功, 且在运行过程中触发了该事件.
> ```
> 这里的事件可以是执行到某行代码(行覆盖率), 某信号发生翻转(翻转覆盖率),
> 某状态机的状态发生转移(状态机覆盖率), 自定义条件被满足(功能覆盖率)等.
> 而"覆盖率达到100%", 则是
> ```
> 对于每个事件, 都存在一个测试用例, 其运行成功, 且在运行过程中触发了该事件.
> ```
> 注意我们可以通过不同的测试用例覆盖不同的事件.
> 按照这个定义, 只需要在仿真过程中添加一些标志即可统计出覆盖率.
> 甚至大多数RTL仿真器(包括verilator)都提供了自动统计覆盖率的功能,
> 如果你想学习如何统计覆盖率, 只需要RTFM.
>
> 另一方面, 从上述定义也可以看出, 提升覆盖率其实是验证工作的下限,
> 覆盖率太低, 只能说明验证工作做得还不充分, 这和"未测试代码永远是错的"是一致的.
> 但测试验证的最终目标是
> ```
> 对于所有测试用例, 均运行成功.
> ```
> 相比之下, "覆盖率达到100%"其实是"设计正确"的必要不充分条件, 是非常宽松的,
> 甚至我们很容易就可以举出一个反例:
> 某模块有两个功能, 针对每个功能, 已分别被各自的测试用例覆盖, 此时功能覆盖率达到100%;
> 但在运行那些需要两个功能进行交互的测试用例时, 就出错.
>
> 和低覆盖率的验证工作相比, 达到更高的覆盖率固然能提升设计正确的概率,
> 但我们想说的是, 即使覆盖率达到100%, 还是远远不够的. 尤其是对复杂系统来说,
> 一些藏得很深的bug通常在多个边界条件同时满足的情况下才会触发.
> 与其坚持以100%覆盖率作为验证目标, 我们更鼓励大家积极思考如何通过其他方法和技术寻找更多潜在的bug,
> 对验证工作的实际意义也更大. -->

> #### Caution: Do not blindly trust UVM test 100% coverage reports.  
> If you understand UVM, you should know that its goal is to improve coverage.  
> However, if you believe that improving coverage is the ultimate goal of test verification, then you likely do not fully understand test verification.  
>  
> In fact, the ultimate goal of test verification is to prove the correctness of the design or to identify all bugs in the design.
> But experienced engineers know that even with 100% coverage, there may still be some bugs that are not detected,
> and it is impossible to estimate how many of these undetected bugs there are.
>
> The reason why coverage targets are widely adopted by companies is, on the one hand, because coverage is an easy-to-quantify and statistically measurable metric.
> If we describe “an event being covered” in strict terms, it is
> ```
> There exists a test case that runs successfully and triggers the event during execution.  
> ```  
> Here, the event can be executing a specific line of code (line coverage), a signal flipping (flip coverage),  
> a state transition in a state machine (state machine coverage), or a custom condition being met (functional coverage), etc.
> Meanwhile, “coverage reaching 100%” means
> ```
> For each event, there exists a test case that runs successfully and triggers the event during execution.
> ```
> Note that we can use different test cases to cover different events.
> According to this definition, we only need to add some flags during simulation to calculate the coverage.
> Most RTL simulators (including Verilator) even provide automatic coverage statistics functionality.
> If you want to learn how to calculate coverage, simply RTFM.
>
> On the other hand, as can be seen from the above definition, improving coverage is actually the lower limit of verification work.
> If coverage is too low, it only indicates that verification work is insufficient, which aligns with the principle that “untested code is always wrong.”
> However, the ultimate goal of testing and verification is  
> ```  
> For all test cases, they all run successfully.  
> ```  
> In contrast, “coverage reaching 100%” is actually a necessary but insufficient condition for “correct design,” and it is very lenient.  
> In fact, we can easily provide a counterexample:
> A module has two functions, and each function is covered by its own test case, resulting in 100% functional coverage;
> However, when running test cases that require interaction between the two functions, errors occur.
>
> Compared to verification work with low coverage, achieving higher coverage certainly increases the probability of correct design,
> but what we want to emphasize is that even with 100% coverage, it is still far from sufficient. Especially for complex systems,  
> some deeply hidden bugs typically only trigger when multiple boundary conditions are simultaneously met.  
> Rather than insisting on 100% coverage as the verification target, we encourage everyone to actively explore other methods and techniques to identify more potential bugs,  
> which holds greater practical significance for verification work.


<!-- ### 形式化验证的简单示例

#### 基于Chisel的形式化验证流程

Chisel的测试框架[chiseltest][chiseltest]已经集成了形式化验证的功能,
可以将FIRRTL代码翻译成能被BMC识别的语言, 并让BMC证明给定的`assert()`是否正确.
若能找到反例, 则生成该反例的波形辅助调试, 非常方便.
有了形式化验证工具, 我们再也不需要为测试用例覆盖不全面而苦恼, 甚至连测试用例也不需要编写了.
一个字, 香!

[chiseltest]: https://github.com/ucb-bar/chiseltest

下面给出一个对Chisel模块进行形式化验证的例子:
```scala
import chisel3._
import chisel3.util._
import chiseltest._
import chiseltest.formal._
import org.scalatest.flatspec.AnyFlatSpec

class Sub extends Module {
  val io = IO(new Bundle {
    val a = Input(UInt(4.W))
    val b = Input(UInt(4.W))
    val c = Output(UInt(4.W))
  })
  io.c := io.a + ~io.b + Mux(io.a === 2.U, 0.U, 1.U)

  val ref = io.a - io.b
  assert(io.c === ref)
}

class FormalTest extends AnyFlatSpec with ChiselScalatestTester with Formal {
  "Test" should "pass" in {
    verify(new Sub, Seq(BoundedCheck(1), BtormcEngineAnnotation))
  }
}
``` -->
### A simple example of formal verification

#### Formal verification process based on Chisel

Chisel's testing framework [chiseltest][chiseltest] has integrated formal verification functionality,  
which can translate FIRRTL code into a language recognizable by BMC and allow BMC to prove whether the given `assert()` is correct.
If a counterexample is found, the waveform generating that counterexample is generated to assist in debugging, which is very convenient.  
With formal verification tools, we no longer need to worry about incomplete test case coverage, and we may not even need to write test cases at all.  
In a word, it's fantastic!  

[chiseltest]: https://github.com/ucb-bar/chiseltest

Below is an example of formal verification of a Chisel module:
```scala
import chisel3._
import chisel3.util._
import chiseltest._
import chiseltest.formal._
import org.scalatest.flatspec.AnyFlatSpec

class Sub extends Module {
  val io = IO(new Bundle {
    val a = Input(UInt(4.W))
    val b = Input(UInt(4.W))
    val c = Output(UInt(4.W))
  })
  io.c := io.a + ~io.b + Mux(io.a === 2.U, 0.U, 1.U)

  val ref = io.a - io.b
  assert(io.c === ref)
}

class FormalTest extends AnyFlatSpec with ChiselScalatestTester with Formal {
  “Test” should “pass” in {
    verify(new Sub, Seq(BoundedCheck(1), BtormcEngineAnnotation))
  }
}
```

<!-- > #### danger::不再使用Utest
> 随着Chisel版本的演进, Utest已不被支持, 因此我们也建议不再使用Utest.
> 如果你在2024/04/11 01:00:00之前获取`chisel-playground`的代码,
> 请参考[新版本的`build.sc`][new build.sc]中的`object test`修改你的`build.sc`.

[new build.sc]: https://github.com/OSCPU/chisel-playground/blob/master/build.sc

上述例子的`Sub`模块通过"取反加1"实现了补码减法的功能.
为了验证`Sub`模块实现的正确性, 代码将"取反加1"的计算结果与通过减法运算符得到的结果进行对比,
我们预期`assert()`应该对任意输入都成立.
为了展示形式化验证的效果, 我们在`Sub`模块的实现中注入了一个bug:
在`io.a`为`2`时不进行"加1"操作, 此时补码减法的结果是错误的.

在调用chiseltest提供的形式化验证功能时, 上述代码还需要传入一个`BoundedCheck(1)`的参数作为BMC的界,
用来指定需要证明的周期数.
例如, `BoundedCheck(4)`表示让BMC尝试证明被测模块在复位之后的4个周期内,
在任意输入信号下都不违反`assert()`.
对于组合逻辑电路, 我们只需要让BMC在1周期内求解即可.

此外, 上述代码还传入了一个`BtormcEngineAnnotation`的参数, 表示调用模型检测器[BtorMC][btormc].
BtorMC基于一个不同于Z3的SMT求解器[Boolector][boolector], 其基本原理与Z3类似,
但根据实测情况, 其求解效率通常Z3的数倍.
在运行上述测试之前, 你还需要获得BtorMC工具.
具体地, 你需要从[这个链接][oss release]下载相应的工具.
解压缩后, 将`path-to-oss-cad-suite/bin`加入到环境变量`PATH`中, 即可调用BtorMC.

[btormc]: https://cs.stanford.edu/~niemetz/publications/2018/NiemetzPreinerWolfBiere-CAV18.pdf
[boolector]: https://github.com/Boolector/boolector
[oss release]: https://github.com/YosysHQ/oss-cad-suite-build/releases -->

> #### danger::No longer use Utest
> With the evolution of Chisel versions, Utest is no longer supported, so we also recommend that you no longer use Utest.
> If you obtained the `chisel-playground` code before 2024/04/11 01:00:00,
> please refer to the `object test` section in the [new version of `build.sc`][new build.sc] to modify your `build.sc`.

[new build.sc]: https://github.com/OSCPU/chisel-playground/blob/master/build.sc

The `Sub` module in the above example implements two's complement subtraction using “invert and add 1.”
To verify the correctness of the `Sub` module implementation, the code compares the result of “invert and add 1” with the result obtained using the subtraction operator.
We expect `assert()` to hold true for any input.
To demonstrate the effectiveness of formal verification, we injected a bug into the implementation of the `Sub` module:  
when `io.a` is `2`, the “add 1” operation is not performed, resulting in an incorrect two's complement subtraction result.

When calling the formal verification functionality provided by chiseltest, the above code also needs to pass in a `BoundedCheck(1)` parameter as the BMC boundary,  
to specify the number of cycles to be proven.
For example, `BoundedCheck(4)` indicates that BMC should attempt to prove that the tested module does not violate `assert()` within 4 cycles after reset,  
under any input signal.  
For combinational logic circuits, we only need BMC to solve within 1 cycle.  

Additionally, the above code passes a `BtormcEngineAnnotation` parameter, indicating the invocation of the model detector [BtorMC] [btormc].
BtorMC is based on an SMT solver [Boolector][boolector] that is different from Z3. Its basic principle is similar to Z3,
but according to actual testing, its solving efficiency is typically several times that of Z3.
Before running the above test, you also need to obtain the BtorMC tool.
Specifically, you need to download the corresponding tool from [this link][oss release].
After unzipping, add `path-to-oss-cad-suite/bin` to the environment variable `PATH` to invoke BtorMC.

[btormc]: https://cs.stanford.edu/~niemetz/publications/2018/NiemetzPreinerWolfBiere-CAV18.pdf
[boolector]: https://github.com/Boolector/boolector
[oss release]: https://github.com/YosysHQ/oss-cad-suite-build/releases

<!-- 完成上述配置后, 通过``mill -i __.test``运行测试, 输出信息如下:
```
Assertion failed
    at SubTest.scala:16 assert(io.c === ref)
- should pass *** FAILED ***
  chiseltest.formal.FailedBoundedCheckException: [Sub] found an assertion violation 0 steps after reset!
  at chiseltest.formal.FailedBoundedCheckException$.apply(Formal.scala:26)
  at chiseltest.formal.backends.Maltese$.bmc(Maltese.scala:92)
  at chiseltest.formal.Formal$.executeOp(Formal.scala:81)
  at chiseltest.formal.Formal$.$anonfun$verify$2(Formal.scala:61)
  at chiseltest.formal.Formal$.$anonfun$verify$2$adapted(Formal.scala:61)
  at scala.collection.immutable.List.foreach(List.scala:333)
  at chiseltest.formal.Formal$.verify(Formal.scala:61)
  at chiseltest.formal.Formal.verify(Formal.scala:34)
  at chiseltest.formal.Formal.verify$(Formal.scala:32)
  at FormalTest.verify(SubTest.scala:19)
  ...
```
上述信息说明BMC找到了一个在复位后第0个周期就违反`assert()`的测试用例.
进一步地, 开发者可以通过波形文件`test_and_run/Test_should_pass/Sub.vcd`辅助调试.
修正`Sub`模块中的错误后, 重新运行上述测试后将不再输出错误信息,
表示BMC找不到反例, 也即证明了代码的正确性. -->

After completing the above configuration, run the test using ``mill -i __.test``, and the output will be as follows:
```
Assertion failed
    at SubTest.scala:16 assert(io.c === ref)
- should pass *** FAILED ***
  chiseltest.formal.FailedBoundedCheckException: [Sub] found an assertion violation 0 steps after reset!
  at chiseltest.formal.FailedBoundedCheckException$.apply(Formal.scala:26)
  at chiseltest.formal.backends.Maltese$.bmc(Maltese.scala:92)
  at chiseltest.formal.Formal$.executeOp(Formal.scala:81)
  at chiseltest.formal.Formal$.$anonfun$verify$2(Formal.scala:61)
  at chiseltest.formal.Formal$.$anonfun$verify$2$adapted(Formal.scala:61)
  at scala.collection.immutable.List.foreach(List.scala:333)
  at chiseltest.formal.Formal$.verify(Formal.scala:61)
  at chiseltest.formal.Formal.verify(Formal.scala:34)
  at chiseltest.formal.Formal.verify$(Formal.scala:32)
  at FormalTest.verify(SubTest.scala:19)
  ...
```
The above information indicates that BMC found a test case that violates `assert()` in the 0th cycle after reset.
Furthermore, developers can use the waveform file `test_and_run/Test_should_pass/Sub.vcd` to assist in debugging.
After correcting the error in the `Sub` module, rerunning the above test will no longer output error messages,
indicating that BMC cannot find counterexamples, thereby proving the correctness of the code.

<!-- #### 基于Verilog的形式化验证流程

chiseltest的形式化验证流程是将FIRRTL代码转换成能被BMC识别的语言,
不涉及Verilog, 因此上述流程不支持基于Verilog开发的项目.
如果你使用Verilog进行开发, 可以使用基于Yosys的形式化验证流程,
其中[SymbiYosys][symbiyosys]是这一流程的前端工具.

[symbiyosys]: https://symbiyosys.readthedocs.io/en/latest/

下面给出一个对Verilog模块进行形式化验证的例子:

```verilog
// Sub.sv
`define FORMAL

module Sub(
  input [3:0] a,
  input [3:0] b,
  output [3:0] c
);

  assign c = a + ~b + (a == 4'd2 ? 1'b0 : 1'b1);

`ifdef FORMAL
  always @(*) begin
    c_assert: assert(c == a - b);
  end
`endif  // FORMAL

endmodule
``` -->

#### Formal verification process based on Verilog

The formal verification process of chiseltest converts FIRRTL code into a language that can be recognized by BMC.
It does not involve Verilog, so the above process does not support projects developed based on Verilog.
If you are developing using Verilog, you can use the formal verification process based on Yosys,  
where [SymbiYosys][symbiyosys] is the front-end tool for this process.  

[symbiyosys]: https://symbiyosys.readthedocs.io/en/latest/  

The following is an example of formal verification for a Verilog module:

```verilog
// Sub.sv
`define FORMAL

module Sub(
  input [3:0] a,
  input [3:0] b,
  output [3:0] c
);

  assign c = a + ~b + (a == 4'd2 ? 1'b0 : 1'b1);

`ifdef FORMAL
  always @(*) begin
    c_assert: assert(c == a - b);
  end
`endif  // FORMAL

endmodule
```


<!-- 上述例子的`Sub`模块通过"取反加1"实现了补码减法的功能.
为了验证`Sub`模块实现的正确性, 代码将"取反加1"的计算结果与通过减法运算符得到的结果进行对比,
我们预期`assert()`应该对任意输入都成立.
为了展示形式化验证的效果, 我们在`Sub`模块的实现中注入了一个bug:
在`a`为`2`时不进行"加1"操作, 此时补码减法的结果是错误的.

编写上述`Sub.sv`文件后, 你还需要编写SymbiYosys的配置文件``*.sby``, 这个文件一般由以下几个部分组成:

* task: 可选项, 用于指定所需执行的任务
* options: 必须项, 用于将代码中的`assert`, `cover`等语句和模型相对应
* engines: 必须项, 用于指定求解的模型
* script: 必须项, 包含测试需要的Yosys脚本
* files: 必须项, 用于指定测试的文件

以下是配置文件`Sub.sby`的示例:

```sby
[tasks]
basic bmc
basic: default

[options]
bmc:
mode bmc
depth 1

[engines]
smtbmc

[script]
read -formal Sub.sv
prep -top Sub

[files]
Sub.sv
``` -->

The `Sub` module in the above example implements two's complement subtraction using “invert and add 1”.
To verify the correctness of the `Sub` module's implementation, the code compares the result of “invert and add 1” with the result obtained using the subtraction operator.
We expect `assert()` to hold true for any input.
To demonstrate the effectiveness of formal verification, we have introduced a bug into the implementation of the `Sub` module:
When `a` is `2`, the “add 1” operation is not performed, resulting in an incorrect two's complement subtraction result.

After writing the above `Sub.sv` file, you also need to write the SymbiYosys configuration file ``*.sby``, which generally consists of the following parts:

* task: optional, used to specify the tasks to be executed
* options: required, used to map statements such as `assert` and `cover` in the code to the model
* engines: required, used to specify the model to be solved
* script: Required, contains the Yosys script needed for testing
* files: Required, used to specify the files for testing

The following is an example of the configuration file `Sub.sby`:

```sby
[tasks]
basic bmc
basic: default

[options]
bmc:
mode bmc
depth 1

[engines]
smtbmc

[script]
read -formal Sub.sv
prep -top Sub

[files]
Sub.sv
```


<!-- 上述配置中的`depth`选项即为BMC的界, 用来指定需要证明的周期数.
例如, `depth 4`表示让BMC尝试证明被测模块在复位之后的4个周期内,
在任意输入信号下都不违反`assert()`.
对于组合逻辑电路, 我们只需要让BMC在1周期内求解即可.

在进行形式化验证之前, 你需要从[这个链接][oss release]下载相应的工具.
解压缩后, 执行命令行`path-to-oss-cad-suite/bin/sby -f Sub.sby`进行形式化验证, 输出信息如下:
```
SBY 16:52:19 [Sub_basic] engine_0: ##   0:00:00  Checking assumptions in step 0..
SBY 16:52:19 [Sub_basic] engine_0: ##   0:00:00  Checking assertions in step 0..
SBY 16:52:19 [Sub_basic] engine_0: ##   0:00:00  BMC failed!
SBY 16:52:19 [Sub_basic] engine_0: ##   0:00:00  Assert failed in Sub: c_assert
SBY 16:52:19 [Sub_basic] engine_0: Status returned by engine: FAIL
SBY 16:52:19 [Sub_basic] summary: Elapsed clock time [H:MM:SS (secs)]: 0:00:00 (0)
SBY 16:52:19 [Sub_basic] summary: Elapsed process time [H:MM:SS (secs)]: 0:00:00 (0)
SBY 16:52:19 [Sub_basic] summary: engine_0 (smtbmc) returned FAIL
SBY 16:52:19 [Sub_basic] summary: counterexample trace: Sub_basic/engine_0/trace.vcd
SBY 16:52:19 [Sub_basic] summary:   failed assertion Sub.c_assert at Sub.sv:11.9-11.37 in step 0
SBY 16:52:19 [Sub_basic] DONE (FAIL, rc=2)
SBY 16:52:19 The following tasks failed: ['basic']
``` -->

The `depth` option in the above configuration is the boundary of BMC, used to specify the number of cycles to be proven.
For example, `depth 4` means that BMC will attempt to prove that the tested module does not violate `assert()` in any input signal within 4 cycles after reset.
For combinational logic circuits, we only need BMC to solve within 1 cycle.

Before performing formal verification, you need to download the corresponding tools from [this link][oss release].
After unzipping, execute the command line `path-to-oss-cad-suite/bin/sby -f Sub.sby` to perform formal verification, and the output information is as follows:
```
SBY 16:52:19 [Sub_basic] engine_0: ##   0:00:00  Checking assumptions in step 0..
SBY 16:52:19 [Sub_basic] engine_0: ##   0:00:00  Checking assertions in step 0..
SBY 16:52:19 [Sub_basic] engine_0: ##   0:00:00  BMC failed!
SBY 16:52:19 [Sub_basic] engine_0: ##   0:00:00  Assert failed in Sub: c_assert
SBY 16:52:19 [Sub_basic] engine_0: Status returned by engine: FAIL
SBY 16:52:19 [Sub_basic] summary: Elapsed clock time [H:MM:SS (secs)]: 0:00:00 (0)
SBY 16:52:19 [Sub_basic] summary: Elapsed process time [H:MM:SS (secs)]: 0:00:00 (0)
SBY 16:52:19 [Sub_basic] summary: engine_0 (smtbmc) returned FAIL
SBY 16:52:19 [Sub_basic] summary: counterexample trace: Sub_basic/engine_0/trace.vcd
SBY 16:52:19 [Sub_basic] summary:   failed assertion Sub.c_assert at Sub.sv:11.9-11.37 in step 0
SBY 16:52:19 [Sub_basic] DONE (FAIL, rc=2)
SBY 16:52:19 The following tasks failed: [‘basic’]
```

<!-- 上述信息说明BMC找到了一个在复位后第0个周期就违反`assert()`的测试用例.
进一步地, 开发者可以通过波形文件`Sub_basic/engine_0/trace.vcd`辅助调试.
修正`Sub`模块中的错误后, 重新运行上述命令后将输出成功信息,
表示BMC找不到反例, 也即证明了代码的正确性. -->

The above information indicates that BMC found a test case that violated `assert()` in the 0th cycle after reset.
Furthermore, developers can use the waveform file `Sub_basic/engine_0/trace.vcd` to assist in debugging.
After correcting the error in the `Sub` module, rerunning the above command will output a success message,
indicating that BMC cannot find a counterexample, thereby proving the correctness of the code.

<!-- ### 通过形式化验证测试icache

我们的目标是通过形式化验证的方式来证明icache的正确性,
因此首先我们需要设计相应的REF, 并确定验证条件.
由于cache属于提升访存效率的技术, 它不应该影响访存结果的正确性,
也即, 无论是否有cache, 访存请求的行为应当一致.
因此, 我们可以将一个最简单的访存系统作为REF,
它接收来自CPU的访存请求, 然后直接访问内存;
与之相对的DUT, 则是让访存请求经过cache.
对于验证条件, 我们只要检查读请求返回的结果是否一致即可. -->

### Verifying the icache through formal verification

Our goal is to prove the correctness of the icache through formal verification,
so we first need to design the corresponding REF and determine the verification conditions.
Since the cache is a technology designed to improve memory access efficiency, it should not affect the correctness of memory access results,
i.e., the behavior of memory access requests should be consistent regardless of whether the cache is present.
Therefore, we can use the simplest memory access system as the REF,  
which receives memory access requests from the CPU and directly accesses memory;  
the corresponding DUT, on the other hand, routes memory access requests through the cache.  
For the verification conditions, we only need to check whether the results returned by read requests are consistent.


<!-- 根据上文的分析, 我们很容易编写出验证顶层模块的伪代码.
这里我们用Chisel作为伪代码, 如果你使用Verilog开发, 你仍然可以借鉴相关思路来编写验证顶层模块.
```scala
class CacheTest extends Module {
  val io = IO(new Bundle {
    val req = new ...
    val block = Input(Bool())
  })

  val memSize = 128  // byte
  val mem = Mem(memSize / 4, UInt(32.W))
  val dut = Module(new Cache)

  dut.io.req <> io.req

  val dutData = dut.io.rdata
  val refRData = mem(io.req.addr)
  when (dut.io.resp.valid) {
    assert(dutData === refData)
  }
}
``` -->

Based on the above analysis, we can easily write pseudocode for the verification top-level module.
Here we use Chisel as pseudocode, but if you are developing in Verilog, you can still draw on the relevant ideas to write the verification of the top-level module.
```scala
class CacheTest extends Module {
  val io = IO(new Bundle {
    val req = new ...
    val block = Input(Bool())
  })

  val memSize = 128  // byte
  val mem = Mem(memSize / 4, UInt(32.W))
  val dut = Module(new Cache)

  dut.io.req <> io.req

  val dutData = dut.io.rdata
  val refRData = mem(io.req.addr)
  when (dut.io.resp.valid) {
    assert(dutData === refData)
  }
}
```

<!-- 上述伪代码只给出了大致的框架, 你需要根据你的具体实现来补充部分细节:
* 屏蔽写操作, 你可以将写使能相关的信号置为`0`
* 让cache缺失时从`mem`中读出数据, 由于测试对象不产生写操作,
  因此DUT和REF可以使用相同的存储器
* 由于REF直接从`mem`中读出数据, 没有任何延迟,
  而DUT从cache中读出数据时需要经历若干周期, 因此需要对`assert()`的时机进行同步:
  REF读出数据后, 需要等待DUT返回读结果后才能进行检查, 显然, 这很容易通过状态机来实现
* 由于形式化验证工具会遍历每个周期的所有输入情况, 因此输入信号每个周期都会变化,
  你可能需要借助寄存器暂存一些结果
* 借助"形式化验证工具会遍历每个周期的所有输入情况"的特性,
  我们可以在测试的顶层定义一些`block`信号,
  用于测试AXI相关的代码能否在随机延迟的场景下工作,
  例如`dut.io.axi.ar.ready := arready_ok & ~block1`,
  `dut.io.axi.r.valid := rvalid_ok & ~block2` -->

  The above pseudocode only provides a rough framework. You need to add some details based on your specific implementation:
* Block write operations by setting the write enable signal to `0`
* When the cache is missing, read data from `mem`. Since the test object does not generate write operations,
  the DUT and REF can use the same memory
* Since REF reads data directly from `mem` with no delay,
  while DUT reads data from the cache and needs to go through several cycles, the timing of `assert()` needs to be synchronized:
  After REF reads the data, it must wait for the DUT to return the read result before performing the check. This can be easily achieved using a state machine.
* Since formal verification tools traverse all input conditions for each cycle, the input signals change every cycle.  
You may need to use registers to temporarily store some results.
* Leveraging the feature that “formal verification tools iterate through all input conditions for each cycle,”  
  we can define some `block` signals at the top level of the test,  
  to verify whether AXI-related code can function under random delay scenarios,  
  for example, `dut.io.axi.ar.ready := arready_ok & ~block1`,
  `dut.io.axi.r.valid := rvalid_ok & ~block2`


<!-- > #### option::通过形式化验证测试icache的实现
> 尽管这不是必须的, 我们还是强烈建议你尝试这一现代的测试验证方法,
> 体会"使用正确的工具解决问题"带来的爽感.
> 关于BMC的界(`BoundedCheck()`或`depth`), 你可以挑选一个合适的参数,
> 使cache能在形式化验证工具证明的周期内处理3~4个请求,
> 从而测试cache能否正确处理任意连续的请求.

形式化验证看上去全是好处, 但其实形式化验证有一个致命的缺点, 就是状态空间爆炸问题.
随着设计规模的增长和界的上升, 求解器需要遍历的空间也越大.
事实上, 一阶逻辑语言在理论上是不可判定的;
即使是可判定的子集, 在算法复杂度的意义上通常也是NP-Hard问题.
这意味着, 求解器的运行时间很可能随设计规模呈指数增长.
因此, 形式化验证一般在单元测试中使用. -->

> #### option::Verify the implementation of icache through formal verification
> Although this is not mandatory, we strongly recommend that you try this modern testing and verification method,
> and experience the satisfaction of “solving problems with the right tools.”
> Regarding the boundary of BMC (`BoundedCheck()` or `depth`), you can select an appropriate parameter  
> to ensure that the cache can process 3–4 requests within the cycle proven by the formal verification tool,  
> thereby testing whether the cache can correctly handle any consecutive requests.

Formal verification seems to have only advantages, but it actually has a fatal flaw: the state space explosion problem.
As the design scale grows and the bounds increase, the space that the solver needs to traverse also grows.  
In fact, first-order logic languages are theoretically undecidable;  
even decidable subsets are typically NP-Hard problems in terms of algorithmic complexity.  
This means that the solver's runtime is likely to grow exponentially with the design scale.  
Therefore, formal verification is generally used in unit testing.

<!-- ## 缓存的优化

由于缓存技术主要用于提升访存效率, 因此很自然地, 我们应该通过访存相关的指标来评价缓存的性能表现.
通常通过AMAT(Average Memory Access Time)来评估缓存的性能表现, 假设缓存的命中率为`p`:
```
AMAT = p * access_time + (1 - p) * (access_time + miss_penalty)
     = access_time + (1 - p) * miss_penalty
```
其中`access_time`为cache的访问时间, 即从cache接收访存请求到得出命中结果所需的时间,
`miss_penalty`为cache缺失时的代价, 此处即访问DRAM的时间.

这个等式给优化缓存的性能表现提供了指导方向:
减少访问时间`access_time`, 提升命中率`p`, 或者减少缺失代价`miss_penalty`.
在目前的NPC中, 访问时间在架构设计上能优化的空间不多, 更多是受具体实现的影响, 如周期数和关键路径.
因此, 后续我们重点讨论命中率和缺失代价的优化. -->

## Cache Optimization

Since cache technology is primarily used to improve memory access efficiency, it is natural to evaluate cache performance using memory access-related metrics.
Typically, AMAT (Average Memory Access Time) is used to assess cache performance, assuming a cache hit rate of `p`:
```
AMAT = p * access_time + (1 - p) * (access_time + miss_penalty)
     = access_time + (1 - p) * miss_penalty
```
where `access_time` is the cache access time, i.e., the time required from receiving a memory access request to obtaining a hit result,
and `miss_penalty` is the cost of a cache miss, which in this case is the time required to access DRAM.

This equation provides guidance for optimizing cache performance:
reduce access time `access_time`, increase hit rate `p`, or reduce miss penalty `miss_penalty`.
In the current NPC, there is limited room for optimization of access time in architectural design, as it is more influenced by specific implementations, such as cycle count and critical path.
Therefore, we will focus on optimizing hit rate and miss penalty in the following sections.

<!-- > #### todo::统计AMAT
> 在NPC中添加合适的性能计数器, 统计icache的AMAT.

### 优化命中率

要提升命中率, 也就是要降低缺失率, 为此, 我们需要先了解cache的缺失有哪些原因. -->

> #### todo::Statistics on AMAT  
> Add appropriate performance counters to the NPC to track the AMAT of the icache.

### Optimizing Hit Rate

To improve the hit rate, we need to reduce the miss rate. To do this, we first need to understand the causes of cache misses.

<!-- #### cache缺失的3C模型

计算机科学家[Mark Hill][mark mill]在其1987年的[博士论文][mark mill phd thesis]中提出3C模型,
刻画了cache缺失的3种类型:
1. Compulsory miss, 强制缺失, 定义为在一个容量无限大的cache中所发生的缺失,
   表现为在第一次访问一个数据块时所发生的缺失
1. Capacity miss, 容量缺失, 定义为不扩大cache容量就无法消除的缺失,
   表现为因cache无法容纳所有所需访问的数据而发生的缺失
1. Conflict miss, 冲突缺失, 定义为除上述两种原因外引起的缺失,
   表现为因多个cache块之间相互替换而发生的缺失

[mark mill]: https://pages.cs.wisc.edu/~markhill/
[mark mill phd thesis]: https://www2.eecs.berkeley.edu/Pubs/TechRpts/1987/CSD-87-381.pdf

有了3C模型, 我们就可以为每种类型的缺失提出针对性的方案, 来降低相应的缺失率了. -->

#### The 3C Model of Cache Misses

Computer scientist [Mark Hill][mark mill] proposed the 3C model in his 1987 [PhD thesis][mark mill phd thesis],
which describes three types of cache misses:
1. Compulsory miss, defined as a miss that occurs in a cache with infinite capacity,
   manifested as a miss during the first access to a data block
1. Capacity miss, defined as a miss that cannot be eliminated without expanding the cache capacity,  
   manifested as a miss occurring because the cache cannot accommodate all the required data.
1. Conflict miss, defined as a miss caused by reasons other than the above two,  
   manifested as a miss occurring due to the replacement of multiple cache blocks.

[Mark Mill]: https://pages.cs.wisc.edu/~markhill/
[Mark Mill PhD thesis]: https://www2.eecs.berkeley.edu/Pubs/TechRpts/1987/CSD-87-381.pdf

With the 3C model, we can propose targeted solutions for each type of miss to reduce the corresponding miss rate.

<!-- #### 降低Compulsory miss

为了降低Compulsory miss, 原则上需要在访问一个数据块之前就将其读入cache中,
但上述cache工作流程并不支持这一功能, 因此需要添加新的机制才能实现, 这种机制称为预取(prefetch).
不过从理论上来说, Compulsory miss只指代首次访问一个数据块时发生的缺失,
在访存次数较多的场合, Compulsory miss的占比并不高,
因此在这里我们暂不深入讨论如何减少Compulsory miss, 感兴趣的同学可以搜索并阅读预取的相关资料.

#### 降低Capacity miss

为了降低Capacity miss, 根据其定义, 只能扩大cache容量, 从而更好地利用时间局部性.
但cache容量并非越大越好, 一方面, cache容量越大, 意味着流片面积越大, 从而增加了流片成本;
另一方面, 存储阵列越大, 意味着其访问延迟越高, 从而增加了cache的访问时间, 降低cache的性能表现.
因此, 在实际的项目中, 一味增大cache容量并不是一个合理的方案, 需要综合各方因素后做出权衡. -->

#### Reducing Compulsory Misses

To reduce compulsory misses, in principle, a data block should be read into the cache before it is accessed.
However, the aforementioned cache workflow does not support this functionality, so a new mechanism must be added to achieve it. This mechanism is called prefetching (prefetch).
However, theoretically, Compulsory miss only refers to the miss that occurs during the first access to a data block.
In scenarios with a high number of memory accesses, the proportion of Compulsory miss is not high.
Therefore, we will not delve into how to reduce Compulsory miss here. Interested readers can search for and read relevant materials on prefetch.

#### Reducing Capacity Misses

To reduce capacity misses, according to its definition, the only option is to increase cache capacity to better utilize temporal locality.
However, larger cache capacity is not always better. On one hand, larger cache capacity means larger chip area, thereby increasing chip production costs;
on the other hand, larger storage arrays mean higher access latency, thereby increasing cache access time and reducing cache performance.
Therefore, in actual projects, blindly increasing cache capacity is not a reasonable solution; a balanced decision must be made after considering all factors.

<!-- #### 降低Conflict miss

为了降低Conflict miss, 需要考虑如何减少多个cache块之间相互替换的情况.
上文介绍了cache中直接映射的组织方式, 每个数据块只能读入到固定index的cache块中,
如果多个数据块的index相同, 那么后读入的数据块将会替换先读入的数据块.
因此, 一个减少替换情况的想法就是采用新的cache块组织方式, 允许数据块读入到多个cache块中.

一种极端的情况是, 每个数据块都可以存放到任意cache块中, 这种组织方式称为全相联(fully-associative).
而具体存放到哪一个cache块中, 首先当然是存放到无效的cache块中;
若所有的cache块均有效, 则由替换算法来决定.
不同的替换算法会影响cache访问的命中率,
一般来说, 替换算法需要选择一个将来最不可能被访问的cache块.
对于给定的访存序列, 我们可以设计出一套最优的替换算法, 使得Conflict miss最少;
但在实际情况中, 我们无法提前得知将来的访存序列,
因此替换算法的设计变成一个"根据过去预测未来"的问题,
即需要根据过去每个cache块的访问情况, 预测出一个将来最不可能被访问的cache块.
常见的替换算法有如下几种:
* FIFO, 先进先出, 替换最旧读入的cache块
* LRU, 最近最少用, 替换在最近一段时间内访问次数最少的cache块
* random, 随机替换

配合合适的替换算法, 全相联组织方式能以更大的概率替换掉一个将来一段时间内最不可能被访问的cache块,
从而能够在最大程度上降低Conflict miss.
但由于全相联组织方式可以将数据块存放到任意cache块中, 这需要付出两点代价.
首先, 访存地址无需划分出index部分, 故除了offset部分外, 剩余的均为tag部分.
因此, 我们需要在存储阵列中花费更多的存储开销来存储cache块的tag部分. -->

#### Reducing Conflict Misses

To reduce conflict misses, it is necessary to consider how to minimize the replacement of multiple cache blocks.  
As mentioned earlier, the direct mapping organization in the cache means that each data block can only be read into a cache block with a fixed index.  
If multiple data blocks have the same index, the later-read data block will replace the earlier-read data block.
Therefore, one approach to reduce replacements is to adopt a new cache block organization scheme that allows data blocks to be read into multiple cache blocks.

An extreme case is where each data block can be stored in any cache block, which is referred to as a fully-associative organization scheme.  
The specific cache block to store in is first determined by selecting an invalid cache block;
if all cache blocks are valid, the replacement algorithm determines the destination.  
Different replacement algorithms affect cache access hit rates,  
generally, the replacement algorithm should select the cache block least likely to be accessed in the future.  
For a given memory access sequence, we can design an optimal replacement algorithm to minimize conflict misses;
However, in practice, we cannot predict future memory access sequences in advance,  
so the design of the replacement algorithm becomes a problem of “predicting the future based on the past,”  
i.e., predicting the cache block that is least likely to be accessed in the future based on the access history of each cache block.  
Common replacement algorithms include the following:  
* FIFO (First-In, First-Out): Replace the oldest read cache block  
* LRU (Least Recently Used): Replaces the cache block with the fewest accesses in the recent period.  
* Random: Replaces cache blocks randomly.

When combined with an appropriate replacement algorithm, the fully associative organization can replace the cache block least likely to be accessed in the near future with a higher probability,  
thereby minimizing conflict misses to the greatest extent possible.  
However, since the fully associative organization can store data blocks in any cache block, this incurs two costs.
First, the memory access address does not need to be divided into an index part, so except for the offset part, the rest are tag parts.
Therefore, we need to spend more storage overhead in the storage array to store the tag part of the cache block.

<!-- ```
 31                m m-1    0
+-------------------+--------+
|        tag        | offset |
+-------------------+--------+
```

其次, 判断命中时, 需要与所有cache块检查其tag是否匹配, 这需要使用很多比较器, 从而增加面积开销.
由于这些代价, 全相联组织方式一般只在cache块数量较少的场景下使用.

组相联(set-associative)是直接映射和全相联的折中, 其思想是给所有cache块分组,
在组间通过直接映射方式选出一个组, 然后在组内通过全相联方式选出一个cache块,
也即, 每个数据块都可以存放到组号为`tag % 组数`中的任意一个cache块.
如果每个组中有`w`个cache块, 则称为`w路组相联`(`w`-way set-associative). -->

```
 31                m m-1    0
+-------------------+--------+
|        tag        | offset |
+-------------------+--------+
```

Second, when determining a hit, it is necessary to check whether the tags of all cache blocks match, which requires the use of many comparators, thereby increasing area overhead.  
Due to these costs, the fully associative organization method is generally only used in scenarios with a small number of cache blocks.

Set-associative is a compromise between direct mapping and fully associative. The idea is to group all cache blocks,
select one group among the groups using direct mapping, and then select one cache block within the group using fully associative mapping.
That is, each data block can be stored in any cache block with a group number of `tag % number of groups`.
If each group contains `w` cache blocks, it is referred to as `w-way set-associative`.

<!-- ```
   +----------------------+-------------------------+
   |                      |                         |
+--+--+-------+--------+  |                         |
| tag | index | offset |  |                         |
+-----+---+---+--------+  |                         |
          |               |                         |
+---------+               |                         |
|                         |                         |
|  V   tag      data      |   V   tag      data     |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+----------+ |
+>+-+-------+----------+  |  +-+-------+----------+ |
  +++-----+-+----------+  |  +++-----+-+----------+ |
   |      |               |   |      |              |
 +-v-+  +-v--+            | +-v-+  +-v--+           |
 | & |<-+ == |<-----------+ | & |<-+ == |<----------+
 +-+-+  +----+              +-+-+  +----+
   |                          |
   +------------+--------------+
                |
                v
             hit way
```

在组相联组织方式中, 一个访存地址可以划分成以下3部分: tag, index, offset,
其中index部分作为组索引, 故其位宽为`n = log2(cache块总数/w)`.

```
 31    m+n m+n-1   m m-1    0
+---------+---------+--------+
|   tag   |  index  | offset |
+---------+---------+--------+
```

判断命中时, 只需要与组内所有cache块检查其tag是否匹配即可, 在`w`不大时, 比较器的面积开销是可以接受的.

实际上, 全相联和直接映射都可以视为组相联的特例: `w=1`时即为直接映射, `w=cache块总数`时即为全相联.
现代CPU通常采用8或16路组相联. -->

```
   +----------------------+-------------------------+
   |                      |                         |
+--+--+-------+--------+  |                         |
| tag | index | offset |  |                         |
+-----+---+---+--------+  |                         |
          |               |                         |
+---------+               |                         |
|                         |                         |
|  V   tag      data      |   V   tag      data     |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+--- -------+ |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+--- -------+ |
| +-+-------+----------+  |  +-+-------+----------+ |
| +-+-------+----------+  |  +-+-------+-------- --+ |
| +-+-------+----------+  |  +-+-------+----------+ |
+>+-+-------+----------+  |  +-+-------+----------+ |
  +++-----+-+----------+  |  +++-----+-+----------+ |
   |      |               |   |      |              |
 +-v-+  +-v--+            | +-v-+  +-v--+           |
 | & |<-+ == |<-----------+ | & |<-+ == |<----------+
 +-+-+  +----+              +-+-+  +----+
   |                          |
   +------------+----------- ---+
                |
                v
             hit way
```

In the set associative organization method, a memory access address can be divided into the following three parts: tag, index, and offset.
The index part is used as the set index, so its bit width is `n = log2(total number of cache blocks/w)`.

```
 31    m+n m+n-1   m m-1    0
+---------+---------+--------+
|   tag   |  index  | offset |
+---------+---------+--------+
```

When determining a hit, it is only necessary to check whether the tag matches all cache blocks within the group. When `w` is not large, the area overhead of the comparator is acceptable.

In fact, fully associative and direct mapping can both be regarded as special cases of set associative: when `w=1`, it is direct mapping; when `w=total number of cache blocks`, it is fully associative.
Modern CPUs typically use 8 or 16-way set associative.

<!-- #### 块大小的选择

块大小是一个特殊的参数.
如果cache块较大, 一方面能够降低tag的存储开销,
另一方面则能够在cache块中存放更多的相邻数据, 从而能更好地捕捉到程序的空间局部性, 降低Conflict miss.
但为了读出更多的相邻数据, cache的缺失代价也会随之上升;
同时, 对于一定的cache容量, 更大的cache块也意味着cache块的数量更少,
如果程序的空间局部性不明显, 则意味着需要更多更小的cache块,
此时较大的cache块反而会增加Conflict miss.

```txt
// 空间局部性较差的程序
// 2个大小为4的cache块
               1111       2222        cache
|--------------oooo-------oooo-----|  memory, `o`为程序访问的热点数据

// 1个大小为8的cache块
               11111111               cache
|--------------oooo-------oooo-----|  memory



// 空间局部性较好的程序
// 2个大小为4的cache块
               11112222               cache
|--------------oooooooo------------|  memory

// 1个大小为8的cache块
               11111111               cache
|--------------oooooooo------------|  memory
``` -->

#### Block Size Selection

Block size is a special parameter.
If the cache block is larger, on one hand, it reduces the storage overhead of tags,
on the other hand, it can store more adjacent data within the cache block, thereby better capturing the spatial locality of the program and reducing conflict misses.
However, to read more adjacent data, the cost of cache misses also increases;
Additionally, for a given cache capacity, larger cache blocks mean fewer cache blocks,  
and if the program's spatial locality is not pronounced, more smaller cache blocks are needed,  
in which case larger cache blocks may actually increase conflict misses.  

```txt
// Program with poor spatial locality
// 2 cache blocks of size 4
               1111       2222        cache
|------------- -oooo-------oooo-----|  memory, `o` represents the program's hotspot data

// 1 cache block of size 8
               11111111               cache
|--------------oooo-------oooo-----|  memory



// Program with good spatial locality
// 2 cache blocks of size 4
               11112222               cache
|--------------oooooooo------------|  memory

// 1 cache block of size 8
               11111111               cache
|--------------oooooooo------------|  memory
```


<!-- ### 设计空间探索

上文提到了非常多和cache相关的参数,
如何选择一组合适的参数, 使得在给定的资源下达到较好的表现,
则属于cache的设计空间探索(Design Space Exploration, DSE)问题.
目前, 我们关心的表现包括IPC, 主频, 面积,
其中, 主频和面积可以通过`yosys-sta`项目快速评估,
而对于IPC, 通常需要在校准过访存延迟的ysyxSoC环境中运行完整的程序后才能获得.

但不同参数之间的组合情况太多了, 如果针对每一种参数组合,
都要花费小时级别的时间才能得到其IPC, 设计空间探索的效率将会十分低下.
上文提到, 数据精度和仿真效率是权衡关系,
因此为了提升设计空间探索的效率, 一种方式是牺牲IPC的统计精度,
通过较低的开销来统计一个能体现IPC变化趋势的指标.

当我们调整cache的各种参数时, 直接影响的是AMAT,
因此我们可以认为CPU中其他部分的执行开销保持不变.
而根据AMAT的定义, 调整上述参数并不影响cache的访问时间, 故可以将其看作一个常数.
因此我们真正需要关注的, 只有程序在发生cache缺失时需要等待的总时间,
我们将其称为总缺失时间(Total Miss Time, TMT).
事实上, TMT可以代表IPC的变化趋势: TMT越小, 每条指令访存所需的周期数也越小, 从而IPC越大.

你之前通过性能计数器统计过AMAT时, 应该也统计过TMT了, 但这需要在ysyxSoC中运行完成的程序.
为了低开销地统计TMT, 我们从`缺失次数 * 缺失代价`这个角度,
考虑如何低开销地统计缺失次数和缺失代价. -->

### Design Space Exploration

The above text mentions many cache-related parameters.
How to select a suitable set of parameters to achieve good performance under given resources
belongs to the cache design space exploration (DSE) problem.
Currently, the performance metrics we are concerned with include IPC, clock frequency, and area.
Among these, clock frequency and area can be quickly evaluated using the `yosys-sta` project,
while IPC typically requires running the entire program in a ysyxSoC environment calibrated for memory access latency to obtain.

However, the number of possible parameter combinations is too large. If we were to spend hours evaluating each combination,  
the efficiency of design space exploration would be extremely low.  
As mentioned earlier, data accuracy and simulation efficiency are trade-offs.  
Therefore, to improve the efficiency of design space exploration, one approach is to sacrifice the statistical accuracy of IPC,  
and instead use a metric that reflects the trend of IPC changes with lower computational overhead.

When adjusting various cache parameters, the direct impact is on AMAT,
so we can assume that the execution overhead of other parts of the CPU remains constant.
According to the definition of AMAT, adjusting the aforementioned parameters does not affect cache access time, so it can be treated as a constant.
Therefore, what we truly need to focus on is the total time the program must wait when a cache miss occurs,
which we refer to as the total miss time (TMT).
In fact, TMT can represent the trend of IPC changes: the smaller the TMT, the fewer cycles required per instruction for memory access, resulting in a higher IPC.

When you previously used performance counters to measure AMAT, you should have also measured TMT, but this requires running the program in ysyxSoC.
To measure TMT with low overhead, we consider the angle of “miss count * miss cost,”
and explore how to measure miss count and miss cost with low overhead.


<!-- 对于缺失次数的统计, 我们有如下观察:
* 对于给定的程序, cache的访问次数是固定的.
  只要获得程序运行的itrace, 将其输入到icache中, 就可以模拟icache工作的过程,
  从而统计icache的缺失次数, 因此不必仿真整个ysyxSoC, 甚至连NPC都不需要.
* NPC执行程序时, 需要通过当前指令的执行结果来得知下一条需要执行的指令.
  但itrace已经包含了完整的指令流, 因此在统计TMT时, 我们只需要指令流的PC值, 而不需要指令本身.
* icache的数据部分是用来作为指令回复给NPC的IFU的,
  但由于统计TMT并不需要NPC, 因此icache的数据部分也不需要, 只需要保留元数据部分即可.
  事实上, 对于给定的访存地址序列, cache的缺失次数与访存内容无关,
  通过维护元数据即可统计出正确的缺失次数. -->

For counting miss counts, we have the following observations:
* For a given program, the number of cache accesses is fixed.  
  By obtaining the program's execution trace (itrace) and inputting it into the icache, we can simulate the icache's operation process,  
  thereby counting the icache's miss counts, so there is no need to simulate the entire ysyxSoC, or even the NPC.
* When the NPC executes a program, it needs to determine the    next instruction to execute based on the execution result of the current instruction.  
However, the itrace already contains the complete instruction stream, so when counting TMT, we only need the PC value of the instruction stream, not the instruction itself.
* The data portion of the icache is used as the IFU to be returned to the NPC,  
but since TMT counting does not require the NPC, the data portion of the icache is also unnecessary, and only the metadata portion needs to be retained.  
In fact, for a given memory access address sequence, the number of cache misses is independent of the memory access content,  
and the correct number of misses can be counted by maintaining the metadata.

<!-- 因此, 为了统计icache的缺失次数, 我们完全没有必要每次都把程序完整运行一遍.
我们真正需要的是一个简单的cache功能模拟器, 我们称它为cachesim.
cachesim接收指令流的PC序列(简化版的itrace), 通过维护元数据来统计这一PC序列的缺失次数.
至于指令流的PC序列, 我们可以通过NEMU来快速生成.

而对于缺失代价, 由于cachesim不包含ysyxSoC中的访存细节, 因此原则上我们也无法准确地获取这个值.
但根据上文的讨论, 只有块大小这个参数会影响缺失代价,
因此我们可以在ysyxSoC中统计出一个平均缺失代价, 并将其作为一个常数来估算TMT. -->

Therefore, to count the number of misses in the icache, there is no need to run the program in full each time.  
What we truly need is a simple cache functionality simulator, which we refer to as cachesim.  
Cachesim receives the PC sequence of the instruction stream (a simplified version of itrace) and counts the number of misses for this PC sequence by maintaining metadata.  
As for the PC sequence of the instruction stream, we can quickly generate it using NEMU.

As for the miss cost, since cachesim does not include memory access details from ysyxSoC, we cannot accurately obtain this value in principle.  
However, as discussed earlier, only the block size parameter affects the miss cost,  
so we can calculate an average miss cost in ysyxSoC and use it as a constant to estimate TMT.

<!-- > #### todo::实现cachesim
> 根据上文的介绍, 实现一个简单的cache模拟器.
>
> 借助cachesim, 我们可以来对icache进行性能测试的DiffTest.
> 具体地, 我们可以把cachesim作为性能测试的REF,
> 在NPC中运行一个程序得到的性能计数器结果,
> 应该与cachesim执行相应PC序列统计得到的缺失次数等数据完全一致.
> 如果不一致, 则可能是RTL实现有性能bug,
> 这种性能bug是无法通过和NEMU进行功能测试的DiffTest或形式化验证发现的.
> 例如, 即使icache一直缺失, 程序仍然可以在NPC上正确运行.
> 当然, 也有可能是作为REF的cachesim有bug.
> 但不管怎样, 能有REF作为对比, 总归是不会吃亏的.
>
> 不过为了得到一致的itrace, 你可能需要对NEMU做一些改动,
> 使其能运行`riscv32e-ysyxsoc`的镜像文件. -->

> #### todo::Implement cachesim
> Based on the above introduction, implement a simple cache simulator.  
>  
> Using cachesim, we can perform performance testing DiffTest on the icache.  
> Specifically, we can use cachesim as the performance testing REF,  
> and the performance counter results obtained by running a program in NPC  
> should be completely consistent with the miss count data and other statistics obtained by cachesim executing the corresponding PC sequence.
> If there is a discrepancy, it may indicate a performance bug in the RTL implementation,  
> which cannot be detected through functional testing with NEMU or formal verification.  
> For example, even if the iCache is consistently missing, the program may still run correctly on the NPC.
> Of course, it is also possible that cachesim, which is used as a REFERENCE, has a bug.
> However, regardless of the cause, having a REFERENCE for comparison is always beneficial.
>
> However, to obtain consistent itrace results, you may need to make some modifications to NEMU
> to enable it to run the `riscv32e-ysyxsoc` ISO file.

<!-- -->
<!-- > #### option::压缩trace
> 如果你得到的itrace非常大, 可以考虑从以下方面进行压缩:
> * 采用二进制方式存储itrace, 而不是文本方式
> * 大部分时候指令是顺序执行的, 对于一段连续的PC序列,
>   我们可以只记录第一个PC和连续执行的指令数量
> * 通过`bzip2`相关工具对生成的itrace文件进一步压缩,
>   然后在cachesim的代码中通过`popen("bzcat 压缩后的文件路径", "r")`得到一个可读的文件指针.
>   关于`popen()`的使用方式, 请RTFM -->

> #### option::Compressing the trace
> If the itrace you obtain is very large, you can consider compressing it in the following ways:
> * Store the itrace in binary format instead of text format
> * Most of the time, instructions are executed sequentially. For a continuous PC sequence,
>   we can only record the first PC and the number of continuously executed instructions
> * Further compress the generated itrace file using `bzip2`-related tools,
>   then obtain a readable file pointer in the cachesim code via `popen(“bzcat compressed file path”, “r”)`.
> For information on how to use `popen()`, please RTFM

<!-- -->
<!-- > #### todo::使用cachesim进行设计空间探索
> 有了cachesim, 我们就可以快速评估不同cache参数组合的预期收益了.
> 对于一个参数组合, cachesim的评估效率比ysyxSoC能快几千倍甚至上万倍.
>
> 除此之外, 我们还能利用多核同时评估多个参数组合:
> 具体地, 首先让cachesim通过命令行方式传入各种cache参数,
> 然后通过脚本启动多个cachesim, 分别传入不同的参数组合即可.
> 通过这种方式, 我们可以在数分钟内获得几十组参数组合的评估结果,
> 从而帮助我们快速选择出合适的参数组合.
>
> 尝试打通这一快速评估的流程, 并评估若干组参数组合.
> 不过, 我们还没有对缺失代价进行优化, 从而评估出更合理的TMT;
> 另外, 我们也还没有给出面积大小的限制.
> 这些因素都会影响最终的决策, 因此目前你不必做出最终的设计选择. -->

> #### todo::Using cachesim for design space exploration
> With cachesim, we can quickly evaluate the expected benefits of different cache parameter combinations.
> For a given parameter combination, cachesim's evaluation efficiency is thousands or even tens of thousands of times faster than ysyxSoC.
>
> Additionally, we can utilize multiple cores to evaluate multiple parameter combinations simultaneously:
> Specifically, first pass various cache parameters to cachesim via the command line,
> then use a script to launch multiple instances of cachesim, each with different parameter combinations.
> Through this approach, we can obtain evaluation results for dozens of parameter combinations within minutes,
> thereby helping us quickly select suitable parameter combinations.
>
> Attempt to establish this rapid evaluation workflow and assess several parameter combinations.
> However, we have not yet optimized the miss cost to evaluate a more reasonable TMT;
> additionally, we have not yet imposed any area size constraints.
> These factors will influence the final decision, so you do not need to make a final design choice at this stage.

<!-- ### 优化缺失代价

如果cache发生缺失, 将要到下一层存储层次中访问数据, 因此缺失代价就是下一层存储层次的访问时间.
降低缺失代价的技术有很多, 目前我们先讨论其中一种: 总线传输的突发访问.
如果cache块的大小和总线的数据位宽相同, 那么只需要一次总线传输事务就可以完成数据块的访问, 此时优化的空间并不大.
但对于更大的cache块, 原则上需要多次总线传输事务才能完成数据块的访问, 其中就存在优化的机会.

#### 块大小和缺失代价

对当前的cache设计来说, 下一层存储层次就是SDRAM.
为了进一步分析, 我们先给SDRAM的访问时间建立如下的简单模型,
把SDRAM的访问时间分成4段, 因此对于一个独立的总线传输事务来说, 开销是`a+b+c+d`.
假设总线的数据位宽是4字节, cache的块大小是16字节.
若采用4个独立的总线传输事务, 则所需开销是`4(a+b+c+d)`.

```txt
+------------------------ arvalid有效
|   +-------------------- AR通道握手, 接收读请求
|   |       +------------ 状态机转移到READ状态, 并向SDRAM颗粒发送READ命令
|   |       |     +------ SDRAM颗粒返回读数据
|   |       |     |   +-- R通道握手, 返回读数据
V a V   b   V  c  V d V
|---|-------|-----|---|
``` -->

### Optimizing Miss Cost

If a cache miss occurs, data must be accessed from the next storage layer, so the miss cost is the access time of the next storage layer.
There are many techniques to reduce the miss cost, and we will first discuss one of them: burst access over the bus.
If the cache block size matches the bus data width, only a single bus transaction is needed to access the data block, leaving little room for optimization.
However, for larger cache blocks, multiple bus transactions are theoretically required to access the data block, presenting opportunities for optimization.

#### Block Size and Miss Cost

For the current cache design, the next storage layer is SDRAM.  
To further analyze this, we first establish the following simple model for SDRAM access time:  
SDRAM access time is divided into four segments, so for an independent bus transfer transaction, the overhead is `a+b+c+d`.
Assuming the bus data width is 4 bytes and the cache block size is 16 bytes,  
if four independent bus transfer transactions are used, the required overhead is `4(a+b+c+d)`.

```txt
+------------------------ arvalid valid
|   +------------ -------- AR channel handshake, receive read request
|   |       +------------ State machine transitions to READ state and sends READ command to SDRAM die
|   |       |     +------ SDRAM die returns read data
|   |       |     |   +-- R channel handshake, return read data
V a V   b   V  c  V d V
|---|-------|-----|---|
```

<!-- > #### todo::支持更大的块大小
> 修改cachesim中的块大小, 使其为总线数据位宽的4倍, 并通过独立的总线传输事务方式来估算缺失代价.
>
> 实现后, 与之前的评估结果进行对比, 并尝试分析原因. -->

> #### todo::Support larger block sizes
> Modify the block size in cachesim to be four times the bus data width, and estimate the missing cost using an independent bus transaction method.
>
> After implementation, compare with previous evaluation results and attempt to analyze the causes.

<!-- -->
<!-- > #### todo::支持更大的块大小(2)
> 修改icache的实现, 使其支持更大的块大小.
> 实现时, 建议将块大小的参数实现成可配置的, 以便于后续的评估工作. -->

<!-- #### 总线的突发传输

与SDRAM颗粒类似, AXI总线也支持"突发传输"(burst transfer),
即在一次总线传输事务中包含多次连续的数据传输, 一次数据传输称为一个"节拍"(beat).
在AXI总线协议中, 需要通过`AR`通道的`arbusrt`信号指示此次传输是否采用突发传输,
若是, 则还需要通过`arlen`信号指示此次突发传输的节拍数量.

当然, 光有总线协议的支持还不够, 还需要AXI的master端发起突发传输事务,
以及slave端支持突发传输事务的处理.
icache作为master端, 我们将发起突发传输事务作为实验内容留给大家.
另一方面, ysyxSoC提供的SDRAM控制器确实支持突发传输事务的处理,
可以将上述4次总线传输包含在一次总线事务中, 从而有效降低完整读出一个数据块的开销:
首先, 通过突发传输, `AR`通道只需要进行一次握手, 与上文的方案相比, 开销可节省`3a`;
其次, SDRAM控制器可以将一次突发传输事务拆分成多个发往SDRAM颗粒的READ命令,
当SDRAM颗粒返回一次读数据时, 如果还有地址连续的READ命令,
SDRAM控制器的状态机则直接转移到READ状态继续发送READ命令,
与上文的方案相比, 开销可节省`3b`;
最后, SDRAM控制器对`R`通道的回复可与下一次READ命令的发送同时进行,
从而将`R`通道握手的开销隐藏起来, 与上文的方案相比, 开销可节省`3d`. -->

#### Bus burst transfers

Similar to SDRAM chips, the AXI bus also supports “burst transfers,”
which involve multiple consecutive data transfers within a single bus transaction, with each data transfer referred to as a “beat.”
In the AXI bus protocol, the `arbusrt` signal on the `AR` channel is used to indicate whether the transfer is a burst transfer.  
If so, the `arlen` signal is also used to indicate the number of beats in the burst transfer.

Of course, bus protocol support alone is insufficient; the AXI master must initiate the burst transfer transaction,  
and the slave must support processing of burst transfer transactions.
As the master end, we will leave the initiation of burst transfer transactions as an experimental task for you to explore.
On the other hand, the SDRAM controller provided by ysyxSoC does support the processing of burst transfer transactions,  
allowing the aforementioned four bus transfers to be included in a single bus transaction, thereby effectively reducing the overhead of fully reading a data block:  
First, through burst transfer, the `AR` channel only needs to perform a single handshake, saving `3a` in overhead compared to the previous scheme;
Second, the SDRAM controller can split a single burst transfer transaction into multiple READ commands sent to the SDRAM chips.  
When the SDRAM chips return a read data, if there are still READ commands with consecutive addresses,  
the state machine of the SDRAM controller directly transitions to the READ state to continue sending READ commands,  
saving `3b` in overhead compared to the previous scheme;
Finally, the SDRAM controller can respond to the `R` channel while sending the next READ command,  
thus hiding the overhead of the `R` channel handshake. Compared to the previous scheme, this saves `3d` in overhead.

<!-- ```txt
  a     b      c    d
|---|-------|-----|---| <-------------------- 第1个节拍
                  |-----|---| <-------------- 第2个节拍
                        |-----|---| <-------- 第3个节拍
                              |-----|---| <-- 第4个节拍
```

综上, 采用突发传输所需开销为`a+b+4c+d`, 和上文的方案相比, 可以节省`3(a+b+d)`的开销.
推广到一般情况, 若cache块的大小是总线数据位宽的`n`倍, 则采用突发传输可以节省`n(a+b+d)`的开销.
虽然表面上看`a`, `b`, `d`都不大, 但别忘了这是SDRAM控制器视角的开销,
校准访存延迟后, 对CPU来说有可能节省数十甚至上百周期的开销. -->


```txt
  a     b      c    d
|---|-------|-----|---| <-------------------- 1st beat
                  |-----|---| <-------------- 2rd beat
                        |-----|---| <-------- 3rd beat
                              |-----|---| <-- 4th beat
```
In summary, the overhead required for burst transmission is `a+b+4c+d`, which saves `3(a+b+d)` compared to the previous scheme.
Extending this to a general case, if the cache block size is `n` times the bus data width, burst transmission can save `n(a+b+d)` overhead.
Although `a`, `b`, and `d` appear small at first glance, remember that this is the overhead from the SDRAM controller's perspective.
After calibrating the memory access latency, this could save dozens or even hundreds of cycles for the CPU.

<!-- #### 突发传输的实现

可以看到, 要获得突发传输的收益, 首先需要让cache使用更大的块.
但根据上文的讨论, 更大的cache块也可能会增加Conflict miss,
从而带来负收益, 具体取决于程序的空间局部性.
至于何者更优, 就要通过benchmark进行评估了.
要在cachesim中评估TMT, 我们还需要获得采用突发传输时的缺失代价.
为此, 我们需要先在ysyxSoC环境中实现AXI的突发传输.
但这一改动涉及较多细节, 因此我们分多个步骤来进行.

之前我们为了方便测试, 先接入了APB接口的SDRAM控制器.
但APB协议不支持突发传输, 要采用突发传输, 首先我们需要将SDRAM控制器更换成AXI接口的版本.

> #### danger::切换到32位ysyxSoC
> SoC团队已经提供了32位的流片SoC, 我们也在2024/07/26 13:00:00把`ysyxSoC`修改成32位,
> 以帮助大家在接入流片SoC之前在本地环境中进行测试.
> 如果你在上述时间之前获取`ysyxSoC`的代码, 请参考本页面最开始处的内容进行操作.


> #### todo::集成AXI接口的SDRAM控制器
> 你需要在`ysyxSoC/src/Top.scala`的`Config`对象中将`sdramUseAXI`变量修改为`true`.
> 修改后重新生成`ysySoCFull.v`, 并尝试运行一些测试程序.


> #### todo::让icache支持突发传输
> 修改icache的实现, 使其采用突发传输方式访问SDRAM中的数据块.
>
> 实现正确后, 记录突发传输的波形, 并与未采用突发传输时的波形进行对比,
> 你应该能观察到, 采用突发传输确实能提升效率. -->

#### Implementation of Burst Transmission

As can be seen, to achieve the benefits of burst transmission, the cache must first use larger blocks.
However, as discussed above, larger cache blocks may also increase conflict misses,
which may result in negative benefits, depending on the spatial locality of the program.
To determine which is better, benchmarking is required.
To evaluate TMT in cachesim, we also need to obtain the missing cost when using burst transmission.
To do this, we first need to implement burst transmission in the ysyxSoC environment.
However, this change involves many details, so we will proceed in several steps.

Previously, for testing convenience, we first integrated an SDRAM controller with an APB interface.
However, the APB protocol does not support burst transfers. To adopt burst transfers, we first need to replace the SDRAM controller with an AXI interface version.

> #### danger::Switch to 32-bit ysyxSoC
> The SoC team has provided a 32-bit production SoC. We also updated `ysyxSoC` to 32-bit on 2024/07/26 13:00:00,
> to help everyone test locally before integrating the production SoC.
> If you obtained the `ysyxSoC` code before the aforementioned time, please refer to the content at the beginning of this page for instructions.

<!-- -->
> #### todo:: Integrate the AXI interface for the SDRAM controller
> You need to modify the `sdramUseAXI` variable in the `Config` object of `ysyxSoC/src/Top.scala` to `true`.
> After modification, regenerate `ysySoCFull.v` and try running some test programs.

<!-- -->
> #### todo:: Enable burst transfer for icache
> Modify the implementation of icache so that it uses burst transfer to access data blocks in SDRAM.
>
> After the implementation is correct, record the waveform of the burst transfer and compare it with the waveform when burst transfer is not used.
> You should be able to observe that using burst transfer does indeed improve efficiency.

<!-- #### 校准AXI接口SDRAM的的访存延迟

虽然目前突发传输方式已经可以工作, 但并未对相应的访存延迟进行校准,
得到的性能数据是不准确的.
和之前实现的APB延迟模块类似, 我们需要一个AXI延迟模块来校准相应的访存延迟.

由于AXI协议比APB协议复杂, AXI延迟模块的实现还需要额外考虑如下问题:
* AXI的读写通道是独立的, 因此原则上需要为读事务和写事务分别设计用于控制延迟的计数器.
  不过当前的NPC是多周期的, 不会同时发送读请求和写请求,
  因此当前你也可以设计一套统一的计数器来进行控制.
  但接下来实现流水线的时候, 你还是需要采用两套独立的计数器.
* AXI有完整的握手信号, 等待握手也涉及到设备的状态,
  因此这段时间也应该属于校准范围内, 也即, 应该将valid信号有效的时刻视为事务的开始.
* AXI支持突发传输, 因此传输模式和APB不同.
  * 以读事务为例, 突发传输可能包含多次数据的接收, 都需要分别校准.
    假设一个AXI突发读事务从`t0`时刻开始, 设备端分别在`t1`, `t2`时刻返回数据,
    AXI延迟模块在`t1'`, `t2'`时刻向上游返回数据,
    则应有等式`(t1 - t0) * r = t1' - t0`和`(t2 - t0) * r = t2' - t0`.
  * 写事务的突发传输包含多次数据的发送, 由于设备接收一次数据也需要至少花费一个周期,
    这在CPU看来已经经过了`r`个周期, 因此也需要对数据的发送时刻进行校准.
    不过目前我们还没有实现dcache, LSU不会发起突发写事务,
    因此目前可暂不实现突发写事务的校准, 但对于单个写事务, 则仍需校准. -->

#### Calibrate the memory access delay of the AXI interface SDRAM

Although the burst transfer mode is currently working, the corresponding memory access delay has not been calibrated,
so the performance data is inaccurate.
Similar to the previously implemented APB delay module, we need an AXI delay module to calibrate the corresponding memory access delay.

Since the AXI protocol is more complex than the APB protocol, the implementation of the AXI delay module requires additional consideration of the following issues:
* AXI read and write channels are independent, so in principle, separate counters should be designed to control the delay for read and write transactions.
However, the current NPC is multi-cycle and does not send read and write requests simultaneously,
so you can currently design a unified counter to control both.
However, when implementing the pipeline later, you will still need to use two independent counters.
* AXI has complete handshake signals, and waiting for the handshake also involves the device's state,
so this period should also be included in the calibration range, i.e., the moment when the valid signal is active should be considered the start of the transaction.
* AXI supports burst transfers, so the transfer mode differs from APB.
* Taking a read transaction as an example, burst transfers may involve multiple data receptions, each of which requires separate synchronization.
Assume an AXI burst read transaction starts at time `t0`. The device receives data at times `t1`, `t2`, and the AXI delay module sends data upstream at `t1'` and `t2'`.
Then, the equations `(t1 - t0) * r = t1' - t0` and `(t2 - t0) * r = t2' - t0` should hold.
* Burst transmission of write transactions involves multiple data transmissions. Since the device needs at least one cycle to receive data,
the CPU considers that `r` cycles have already elapsed, so the data transmission time also needs to be calibrated.
However, since we have not yet implemented dcache, LSU does not initiate burst write transactions,
so burst write transaction calibration can be temporarily omitted. However, calibration is still required for single write transactions.

<!-- > #### todo::实现AXI延迟模块
> 按照上文的介绍, 在ysyxSoC中实现AXI延迟模块.
> 具体地, 如果你选择Verilog, 你需要在`ysyxSoC/perip/amba/axi4_delayer.v`中实现相应代码;
> 如果你选择Chisel, 你需要在`ysyxSoC/src/amba/AXI4Delayer.scala`的`AXI4DelayerChisel`模块中实现相应代码,
> 并将`ysyxSoC/src/amba/AXI4Delayer.scala`中的`Module(new axi4_delayer)`修改为实例化`AXI4DelayerChisel`模块.
>
> 为了简化实现, 目前你可以认为突发传输的节拍数量不会大于8.
> 实现后, 尝试取不同的`r`, 在波形中观察上述等式是否成立. -->

> #### todo::Implement AXI delay module
> Based on the previous description, implement the AXI delay module in ysyxSoC.
> Specifically, if you choose Verilog, you need to implement the corresponding code in `ysyxSoC/perip/amba/axi4_delayer.v`;
> If you choose Chisel, you need to implement the corresponding code in the `AXI4DelayerChisel` module of `ysyxSoC/src/amba/AXI4Delayer.scala`,
> and modify `Module(new axi4_delayer)` in `ysyxSoC/src/amba/AXI4Delayer.scala` to instantiate the `AXI4DelayerChisel` module.
>
> To simplify the implementation, you can currently assume that the number of burst transmission cycles will not exceed 8.
> After implementation, try different values of `r` and observe whether the above equation holds true in the waveform.

<!-- -->
<!-- > #### todo::评估突发传输方式的性能表现
> 校准突发传输方式的访存延迟后, 运行microbench的train规模测试, 并与之前的记录结果进行对比. -->

> #### todo::Evaluate the performance of the burst transmission method
> After calibrating the memory access delay of the burst transmission method, run the microbench train scale test and compare the results with the previous records.

<!-- #### 快速评估缺失代价

根据上文的讨论, 当前icache的缺失代价只与块大小和总线的传输方式相关, 与cache的其他参数无关.
因此, 我们可以提前评估各种块大小和总线传输方式组合下的缺失代价,
将来直接代入这个缺失代价, 即可计算出TMT, 从而估算出不同cache参数组合的预期收益.
但根据上文的分析, 采用突发传输方式的性能表现必定优于独立传输方式,
所以实际上我们只需要提前评估各种块大小在采用突发传输方式时的缺失代价即可.

至于如何提前评估缺失代价, 又有两种方法:
1. 建模. 根据SDRAM控制器状态机的工作流程, 拟合出SDRAM访问时间的计算公式.
   代入块大小, 即可计算出相应的缺失代价.
   这种方式比较直接, 但建模的精确度是一个挑战,
   例如SDRAM的row buffer和刷新操作也会影响SDRAM的访问时间,
   但我们很难刻画这些因素在一次SDRAM访问中带来的开销.
1. 统计. 通过适当的性能计数器, 计算出icache缺失时访问SDRAM的TMT, 从而计算出平均缺失代价.
   作为一种统计方法, 它可以通过采样后平均的方式,
   将SDRAM的row buffer和刷新操作等难以建模的因素考虑进来.
   但row buffer的本质也是一种cache, 其性能表现也会受到程序局部性的影响,
   因此测试程序还需要具备代表性:
   直接运行microbench的train规模测试的代表性是最好的,
   虽然需要花费较多时间, 但只需要运行一次, 即可统计出平均缺失代价;
   test规模测试与train规模测试的行为不完全相同, 但也有一定的代表性, 从而可以快速统计出平均缺失代价;
   但hello程序的代表性就比较弱了, 用其统计出的平均缺失代价进行估算, 可能会带来较大的误差.

在实际的项目中, 考虑到项目的复杂性, 一般很少采用建模方式,
因而此处也推荐大家采用统计方式来提前评估缺失代价. -->

#### Quickly evaluate the cost of missing

Based on the previous discussion, the current missing cost of the ICache is only related to the block size and the bus transmission mode, and is independent of other cache parameters.
Therefore, we can pre-evaluate the missing cost for various combinations of block sizes and bus transmission modes,
and directly substitute this missing cost into the formula to calculate the TMT, thereby estimating the expected benefits of different cache parameter combinations.
However, based on the above analysis, burst transmission mode will definitely perform better than independent transmission mode.
Therefore, in practice, we only need to evaluate the miss cost of various block sizes in burst transmission mode in advance.

There are two methods for evaluating the miss cost in advance:
1. Modeling. Based on the workflow of the SDRAM controller state machine, derive a formula for calculating the SDRAM access time.
Substitute the block size to calculate the corresponding miss cost.
This method is relatively straightforward, but the accuracy of the model is a challenge.
For example, the row buffer and refresh operations of SDRAM also affect SDRAM access time,
but it is difficult to quantify the overhead these factors introduce in a single SDRAM access.
1. Statistical analysis. By using appropriate performance counters, calculate the TMT when accessing SDRAM with an ICache miss, and then calculate the average miss cost.
As a statistical method, it can consider factors that are difficult to model, such as SDRAM row buffers and refresh operations, by averaging the results of a sample.
However, the row buffer is essentially a cache, and its performance is also affected by program locality.
Therefore, the test program must be representative:
Running the representative train-scale test of microbench is the best way to ensure representativeness, although it takes more time.
Running the train-scale test of the microbenchmark directly provides the best representativeness,
though it requires more time, it only needs to be run once to calculate the average miss cost;
The behavior of the test-scale test is not entirely the same as that of the train-scale test, but it still has some representativeness, allowing for a quick calculation of the average missing cost;
However, the representativeness of the hello program is relatively weak, and using it to estimate the average missing cost may result in significant errors.

In actual projects, considering the complexity of the project, modeling is rarely used.
Therefore, we also recommend using the statistical method to pre-assess the missing cost.



<!-- > #### todo::快速评估缺失代价
> 根据上述内容实现缺失代价的快速评估.
> 你已经可以通过cachesim计算出缺失次数,
> 后续你将使用这些缺失代价来估算不同cache参数组合的预期收益. -->

> #### todo::Quickly assess the missing cost
> Implement a quick assessment of the miss cost based on the above content.
> You can already calculate the number of misses using cachesim,
> and you will use these miss costs to estimate the expected benefits of different cache parameter combinations.



<!-- ### 程序的内存布局

程序的内存布局有时换也会明显影响cache的性能表现.
以icache为例, 当icache的块大小超过4字节时,
有时候程序中的一些热点循环可能没有对齐到cache块的边界,
使得热点循环中的指令多占用了一些cache块.
例如, 某程序的热点循环位于地址`[0x1c, 0x34)`, 假设某icache的块大小是16字节,
将这个热点循环的指令全部读入icache, 需要占用3个cache块.

```txt
         [0x1c, 0x34)           + 0x4   =   [0x20, 0x38)
+------+------+------+------+      +------+------+------+------+
|      |      |      | 0x1c |      | 0x20 | 0x24 | 0x28 | 0x2c |
+------+------+------+------+      +------+------+------+------+
| 0x20 | 0x24 | 0x28 | 0x2c |      | 0x30 | 0x34 |      |      |
+------+------+------+------+      +------+------+------+------+
| 0x30 |      |      |      |
+------+------+------+------+
```

但如果我们在程序代码前填充一些空白字节, 就可以改变热点循环的位置,
从而使得热点循环的指令占用更少的cache块.
例如, 在上述例子中, 我们只需要在程序代码前填充4字节的空白内容,
就可以将热点循环的位置改变为`[0x20, 0x38)`.
此时热点循环的指令只需要占用2个cache块, 节省出来的1个cache块可用于存放其他指令,
从而提升程序的整体性能表现. -->

### Program memory layout

The memory layout of a program can also significantly affect cache performance.
Take icache as an example. When the icache block size exceeds 4 bytes,
some hot loops in the program may not be aligned with the cache block boundaries,
causing the instructions in the hot loops to occupy additional cache blocks.
For example, suppose a hot loop is located at address `[0x1c, 0x34)`, and the block size of an icache is 16 bytes.
To read all the instructions in this hot loop into the icache, three cache blocks are required.

```txt
         [0x1c, 0x34)           + 0x4   =   [0x20, 0x38)
+------+------+------+------+      +------+------+------+------+
|      |      |      | 0x1c |      | 0x20 | 0x24 | 0x28 | 0x2c |
+------+------+------+------+      +------+------+------+------+
| 0x20 | 0x24 | 0x28 | 0x2c |      | 0x30 | 0x34 |      |      |
+------+------+------+------+      +------+------+------+------+
| 0x30 |      |      |      |
+------+------+------+------+
```

However, if we fill in some blank bytes before the program code, we can change the position of the hotspot loop,
so that the instructions in the hotspot loop occupy fewer cache blocks.
For example, in the above example, we only need to fill in 4 bytes of blank content before the program code,
to change the position of the hotspot loop to `[0x20, 0x38)`.
At this point, the hotspot loop instructions only occupy 2 cache blocks, and the 1 cache block saved can be used to store other instructions,
thereby improving the overall performance of the program.

<!-- > #### todo::优化程序的内存布局
> 尝试按照上文的方法, 在程序代码前填充若干空白字节.
> 具体地, 你可以通过修改代码, 或者修改链接脚本来实现这个功能.
> 实现后, 尝试评估填充的空白字节是否能优化程序的性能表现.

事实上, 上述例子的cache容量很小, 因此节省出来的1个cache块在整个cache中占比较高.
但现代处理器的cache的容量相对充足, 节省出来的1个cache块可能不会给程序的性能带来明显的提升.
不过我们想说的是, 程序的优化也是利用局部性原理的一个重要方向,
有的程序在优化后甚至能提升数倍的性能.

在企业中, 对于目标场景中的关键应用程序, 工程师团队通常会采用各种方法来提升其性能表现;
如果企业有能力设计自己的处理器, 除了提升处理器的性能之外,
还会根据处理器的参数对编译器进行定制化.
和采用公共版本的编译器(如开源社区的gcc)编译出来的可执行文件相比,
采用定制化编译器编译出来的可执行文件能够在目标处理器上运行得更快.
特别地, [SPEC CPU中定义了两种衡量标准`base`和`peak`][spec run rules],
其中`peak`标准允许不同的子项采用不同的编译优化选项来编译,
使得整个benchmark在目标平台上达到比`base`更优的性能.
如果希望在`peak`标准中获得更高的分数, 软件层次的优化是不可或缺的.

[spec run rules]: https://www.spec.org/cpu2006/Docs/runrules.html#rule_1.5 -->

> #### todo::Optimize the memory layout of the program
> Try filling the program code with a few blank bytes using the method described above.
> Specifically, you can achieve this by modifying the code or modifying the link script.
> After implementation, try evaluating whether the filled blank bytes optimize the program's performance.

In fact, the cache capacity in the above example is very small, so the 1 cache block saved accounts for a high proportion of the entire cache.
However, modern processors have relatively large cache capacities, so saving one cache block may not significantly improve program performance.
Nevertheless, we would like to point out that program optimization is an important direction for utilizing the principle of locality.
Some programs can even achieve several times the performance after optimization.

In enterprises, for key applications in target scenarios, engineering teams usually use various methods to improve their performance.
If an enterprise has the capability to design its own processors, in addition to improving processor performance,
it will also customize the compiler based on the processor parameters.
Compared to executable files compiled using public versions of compilers (such as gcc from the open-source community),
executable files compiled using customized compilers can run faster on the target processor.
Specifically, [SPEC CPU defines two metrics, `base` and `peak`][spec run rules],
where the `peak` metric allows different sub-items to be compiled with different compilation optimization options,
enabling the entire benchmark to achieve better performance on the target platform than the `base` metric.
If you want to achieve a higher score in the `peak` metric, software-level optimization is indispensable.

[spec run rules]: https://www.spec.org/cpu2006/Docs/runrules.html#rule_1.5

<!-- ### 设计空间探索(2)

我们上面介绍了cache的很多参数, 甚至还包含程序的内存布局,
这些都会影响程序在处理器上运行的性能.
现在我们就可以来综合考虑这些参数, 选择一组性能表现较好的参数组合了.
当然, 设计空间探索还需要满足面积大小的约束.

> #### danger::面积大小的限制
> 你的NPC需要在`yosys-sta`项目默认提供的nangate45工艺下,
> 综合面积不超过25000$um^2$(综合工具报告的面积数据默认单位为$um^2$), 这也作为B阶段流片的面积限制.
> 考虑到后续任务还需要实现流水线, 因此我们推荐此时NPC的综合面积不超过23000$um^2$.
>
> 这个面积算不上很大, 一方面,
> 采用这一面积限制有助于凸显其他参数在设计空间探索中的贡献,
> 否则你只需要一味提升cache容量, 就可以获得很好的性能表现,
> 其他参数对性能提升的影响将难以得到体现;
> 另一方面, 项目组预期较多同学会基于B阶段流片,
> 更小的面积有助于项目组节省流片成本.
>
> 目前你不必严格满足上述面积限制, 如果超过的部分不到5%,
> 你可以选择在完成流水线后再统一进行优化;
> 但如果当前的综合面积已远超上述限制, 你可能需要对你的设计进行较大调整,
> 我们建议你马上开展一些面积相关的优化工作.
>
> 关于流片成本, 我们可以做一些简单但不太严谨的估算.
> 假设某晶圆厂提供nangate45工艺的流片服务, 每个block的尺寸是`2mm X 3mm`, 价格是50万RMB,
> 则每$um^2$的价格为`500000 / (2 * 3 * 1000000) = 0.0834`元.
> 通常来说, 连接标准单元的线网也需要占用一定的面积,
> 同时为了让走线不至于过度拥塞, 标准单元之间也会多留一些空隙.
> 因此, 最终的芯片面积通常要大于综合面积, 根据经验, 综合面积一般是最终芯片面积的70%.
> 按上述估算方式, 一个综合面积为25000$um^2$的设计,
> 最终需要花费的价格是`25000 / 0.7 * 0.0834 = 2978`元.
>
> 上述估算给大家带来的最大启发就是, 给CPU添加功能特性, 并不是免费的.
> 这一点与面向FPGA的设计有很大区别: 在一些以FPGA作为目标平台的比赛中,
> 参赛者通常会尽最大可能将FPGA上的资源转化成CPU的性能,
> 这一过程中不会产生任何经济上的成本.
>
> 但真实的流片并不是这样.
> 你可以将B阶段的目标看成是设计一款低成本的嵌入式处理器(rv32e就是面向嵌入式场景的基础指令集):
> 假设你是某嵌入式CPU厂商的架构师, 你需要在面积预算有限的情况下想办法提升处理器的性能.
> 如果面积超出预期, 芯片成本就会上升, 在市场上的竞争力也会有所下降.
> 在这样的条件下, 你需要估算添加某个功能特性的性价比:
> 假设某功能特性可以带来10%的性能提升, 它是否值得你为它多支付500元? -->

### Design Space Exploration (2)

We have introduced many cache parameters above, including the memory layout of the program,
which all affect the performance of the program running on the processor.
Now we can comprehensively consider these parameters and select a set of parameter combinations with better performance.
Of course, design space exploration also needs to satisfy area size constraints.

> #### danger::Area size constraints
> Your NPC needs to be implemented on the nangate45 process provided by the `yosys-sta` project by default,
> with a total area not exceeding 25,000 $um^2$ (the default unit for area data reported by the synthesis tool is $um^2$), which also serves as the area constraint for the B-stage tape-out.
> Considering that subsequent tasks will require the implementation of pipelining, we recommend that the total area of the NPC does not exceed 23,000 $um^2$ at this stage.
>
> This area is not particularly large, on the one hand,
> adopting this area limit helps highlight the contribution of other parameters in the design space exploration,
> otherwise, you could simply increase the cache capacity to achieve good performance,
> and the impact of other parameters on performance improvements would be difficult to discern;
> on the other hand, the project team expects many students to base their work on the B-stage tape-out,
> a smaller area helps the project team save tape-out costs.
>
> Currently, you do not need to strictly adhere to the above area constraints. If the excess is less than 5%,
> you can choose to perform unified optimization after completing the pipeline;
> however, if the current total area significantly exceeds the above constraints, you may need to make significant adjustments to your design,
> and we recommend that you immediately initiate area-related optimization efforts.
>
> Regarding wafer fabrication costs, we can make some simple but not very rigorous estimates.
> Assuming a foundry offers nangate45 process technology with a block size of `2mm x 3mm` at a price of 500,000 CNY,
> the cost per $um^2$ is `500,000 / (2 * 3 * 1,000,000) = 0.0834` CNY.
> Typically, the interconnecting lines between standard cells also occupy a certain amount of area,
> and to prevent excessive congestion, additional space is left between standard cells.
> Therefore, the final chip area is usually larger than the synthesis area. Based on experience, the synthesis area is generally 70% of the final chip area.
> Using the above estimation method, a design with a synthesis area of 25,000 $um^2$
> would require a final cost of `25,000 / 0.7 * 0.0834 = 2,978` CNY.
>
> The key takeaway from this estimation is that adding functional features to a CPU is not free.
> This is very different from FPGA-based designs: In some competitions targeting FPGA platforms,
> participants typically strive to convert as much FPGA resources into CPU performance as possible,
> without incurring any economic costs in the process.
>
> However, real-world chip production is not like this.
> You can view the goal of Phase B as designing a low-cost embedded processor(rv32e is a basic instruction set for embedded scenarios): 
> Assume you are an architect at an embedded CPU manufacturer. You need to find ways to improve processor performance within a limited area budget.
> If the area exceeds expectations, the chip cost will increase, and its competitiveness in the market will decrease.
> Under these conditions, you need to estimate the cost-effectiveness of adding a feature:
> Assuming that a certain feature can improve performance by 10%, is it worth paying an additional 500 CNY for it?

<!-- -->
<!-- > #### todo::icache的设计空间探索
> 按照上述介绍, 探索icache的设计空间, 并确定一个设计方案, 使其在给定约束下达到较好的性能表现.
> 确定方案后, 通过RTL实现它, 并评估其在ysyxSoC中的性能表现. -->

> #### todo::Explore the design space of icache
> Based on the above introduction, explore the design space of icache and determine a design plan that achieves good performance under the given constraints.
> After determining the solution, implement it in RTL and evaluate its performance in ysyxSoC.

<!-- -->
<!-- > #### hint::一些优化面积的思路
> 如果你的评估面积大幅超过上述要求, 你大概率需要对你的设计进行优化.
> 面积的优化并没有什么诀窍, 但整体上来说可以从以下方向考虑:
> 1. 优化逻辑开销: 考虑哪些逻辑功能是冗余的, 可以和其他现有逻辑合并
> 1. 优化存储开销: 考虑哪些存储单元是冗余的
> 1. 逻辑开销和存储开销之间的转换: 有时候与其存储一个信号,
>    不如重新将它计算一遍, 不过这可能还会影响关键路径, 需要具体情况具体分析
>
> 对大家来说, 上述面积限制的要求大概率不是随手写写就能达到, 但也并不是不可能达到.
> yzh的参考设计在添加icache后面积为22730$um^2$, 同时频率可以达到1081MHz,
> 运行microbench显示的`Total time`为4.49s.
> 我们设置上述面积限制, 一方面是锻炼大家如何优化自己的设计:
> 如果你是初学者, 你总得需要一个契机来接触这方面的工作,
> 在不断的试错当中建立每一行RTL代码和面积开销之间的关联认识.
>
> 另一方面也是为了让大家再次体会到体系结构设计的目标:
> 性能和面积是相互制约的, 如果你觉得你的设计很难优化,
> 最简单的方法就是减少cache的容量, 但你需要付出性能的代价;
> 如果你想兼顾面积和性能, 你就需要尽可能节省不必要的面积开销,
> 然后规划这些面积用来做什么更有利于提升性能. -->

> #### hint::Some ideas for optimizing area
> If your estimated area greatly exceeds the above requirements, you will most likely need to optimize your design.
> There are no tricks to optimizing area, but overall, you can consider the following directions:
> 1. Optimize logic overhead: Consider which logic functions are redundant and can be merged with existing logic
> 1. Optimize memory overhead: Consider which memory units are redundant
> 1. Conversion between logic overhead and memory overhead: Sometimes, instead of storing a signal,
> it is better to recalculate it, though this may affect the critical path and requires case-by-case analysis
>
> For most people, meeting the above area constraints is unlikely to be achieved with a quick write, but it is not impossible.
> yzh's reference design achieves an area of 22,730 $um^2$ after adding an ICache, while maintaining a frequency of 1,081 MHz,
> and the `Total time` displayed in the microbenchmark is 4.49 seconds.
> We set the above area constraints for two reasons:
> If you are a beginner, you need an opportunity to get started with this kind of work,
> and through trial and error, you will develop an understanding of the relationship between each line of RTL code and its area overhead.
>
> On the other hand, it is also to remind everyone of the goal of architectural design:
> Performance and area are mutually constrained. If you find your design difficult to optimize,
> the simplest method is to reduce the cache capacity, but you will have to sacrifice performance;
> If you want to balance area and performance, you need to minimize unnecessary area overhead,
> and then plan how to use this area to better improve performance.

<!-- -->
<!-- > #### danger::不要细抠综合的选项
> 有的同学试图通过探索综合器的各种选项来提升综合的质量.
> 尽管我们鼓励大家去了解综合的细节, 但需要说明的是,
> 通过改动代码提升综合质量, 和通过调参提升综合质量, 是两回事.
> 事实上, 后者一般只在追求极致优化目标的时候进行, 但前提也要先努力将前者做到最好.
> 更本质地, 前者才属于体系结构设计能力训练的内容, 而后者不是. -->

> #### danger::Do not focus too much on synthesis options
> Some students attempt to improve synthesis quality by exploring various options in the synthesizer.
> While we encourage everyone to understand the details of synthesis, it is important to note that
> improving synthesis quality by modifying the code is different from improving synthesis quality by tweaking parameters.
> In fact, the latter is generally only done when pursuing extreme optimization goals, but the former must be done well first.
> More fundamentally, the former belongs to the training of architectural design skills, while the latter does not.

<!-- -->
<!-- > #### todo::评估dcache的性价比
> 你已经在上文中估算了dcache在理想情况下的性能收益,
> 在这里我们继续估算这个dcache的性价比.
> 假设这个dcache的面积大小和icache一样, 其成本是多少元?
>
> 虽然你还没有设计dcache, 但因为dcache需要支持写操作,
> 其设计必定比icache复杂, 故占用的面积也应该大于同等容量的icache.
> 因此, 在这些条件估算出来的dcache性价比是高度乐观的,
> 如果考虑dcache的真实性能收益和实际面积, 设计dcache的性价比只会更低.
>
> 另一个考虑的方向是, 如果把dcache的面积用于扩大icache的容量, 能带来多少性能提升? -->

> #### todo::Evaluate the cost-effectiveness of dcache
> You have estimated the performance gain of dcache under ideal conditions in the previous section.
> Here, we will continue to estimate the cost-effectiveness of this dcache.
> Assuming that the area of this dcache is the same as that of the icache, what is its cost in CNY?
>
> Although you have not yet designed the dcache, since it needs to support write operations,
> its design must be more complex than that of the icache, and therefore it should occupy a larger area than an icache of the same capacity.
> Therefore, the cost-effectiveness of the dcache estimated under these conditions is highly optimistic.
> If we consider the actual performance gains and physical area of the dcache, the cost-effectiveness of designing a dcache will be even lower.
>
> Another direction to consider is: if the area of the dcache is used to expand the capacity of the icache, how much performance improvement can be achieved?

<!-- -->
<!-- > #### caution::真正的体系结构设计
> 尽管上述的icache设计空间探索任务与真实处理器中的设计空间探索相比简化了很多,
> 但对大部分同学来说也算是第一次接触到真正的处理器体系结构设计了.
> 更多地, 这也很可能是大部分同学第一次接触到一个模块设计的全流程,
> 从需求分析, 结构设计, 逻辑设计, 到功能验证, 性能验证, 性能优化,
> 最后到电路层次的面积评估, 时序分析.
> 其中, 逻辑设计就是大家常说的RTL编码.
>
> 这个任务再次让大家看到, 体系结构设计不等于RTL编码.
> 体系结构设计的工作是, 在满足约束条件的设计空间中寻找一组表现较好的设计参数.
> 但通常设计空间非常大, 要完整评估其中一组设计参数的性能表现, 也需要花费不少时间.
> 因此对体系结构设计工作来说, 如何能快速评估不同设计参数的性能表现, 是一个至关重要的问题.
>
> 因此, 模拟器是体系结构设计的重要工具.
> 通过模拟器, 我们不必仿真电路级别的行为(不跑verilator, 而是跑cachesim),
> 同时只需要模拟一些必要的模块(不模拟cache的数据, 只需要模拟cache的元数据),
> 此外也不必由处理器驱动(不跑完整的程序, 而是回放相应的itrace).
> 正是这些不同, 使得模拟器的运行效率比RTL仿真有数量级的提升,
> 从而可以快速评估不同设计参数的预期性能表现, 帮助我们快速排除明显不合适的设计参数.
>
> 根据香山团队的经验, 在verilator上跑一轮程序需要花费1周的时间,
> 但在全系统模拟器[gem5][gem5]中跑相同的程序, 只需要2小时.
> 这意味着, 在RTL仿真环境中评估一组设计参数的时间,
> 可以用模拟器探索84组不同设计参数的效果.
>
> 对于体系结构研究来说, 模拟器也是很常见的平台.
> 体系结构顶会ISCA已经多次举行基于[ChampSim][champsim]模拟器的大赛,
> 包括[cache替换算法大赛][cache replacement]和[cache预取算法大赛][data prefetch]等.
> 研究人员都是在模拟器中评估各种算法的表现, 从而快速调整算法的整体实现及其细致参数.
> 尽管一个合格的算法还需要经过RTL层次的实现和验证,
> 但如果一开始选择在RTL层次上探索各种算法, 效率是十分低下的.
>
> 所以, 当你真正明白`体系结构设计 != RTL编码`的时候,
> 你才算是真正入门了体系结构设计这个领域.

[gem5]: https://www.gem5.org/
[champsim]: https://github.com/ChampSim/ChampSim
[cache replacement]: https://www.sigarch.org/call-contributions/the-2nd-cache-replacement-championship/
[data prefetch]: https://www.sigarch.org/call-contributions/third-data-prefetching-championship/ -->

> #### caution::Real computer architecture design
> Although the above icache design space exploration task is significantly simplified compared to real processor design,
> it is still the first time most students have encountered real processor architecture design.
> More importantly, this is likely the first time most students have experienced the entire design process of a module,
> from requirement analysis, structural design, logical design, to functional verification, performance verification, performance optimization,
> finally to area evaluation and timing analysis at the circuit level.
> Among these, logic design is what is commonly referred to as RTL coding.
>
> This task once again demonstrates that computer architecture design is not equivalent to RTL coding.
> The work of computer architecture design is to find a set of design parameters with good performance within the design space that satisfies the constraints.
> However, the design space is typically very large, and thoroughly evaluating the performance of a set of design parameters can be time-consuming.
> Therefore, for computer architecture design, the ability to quickly assess the performance of different design parameters is a critical issue.
>
> Therefore, simulators are an essential tool for computer architecture design.
> With simulators, we do not need to simulate circuit-level behavior (no Verilator runs, only CacheSim runs),
> we only need to simulate necessary modules (no cache data simulation, only cache metadata simulation),
> and we do not need processor-driven simulation (no full program runs, only playback of corresponding ITrace).
> It is these differences that make simulators significantly more efficient than RTL simulation,
> enabling rapid evaluation of the expected performance of different design parameters and helping us quickly eliminate obviously unsuitable design parameters.
>
> According to the Xiangshan team's experience, running a program on Verilator takes one week,
> but running the same program on a full-system simulator[gem5][gem5], the same program can be run in just 2 hours. 
> This means that the time required to evaluate a set of design parameters in an RTL simulation environment
> can be used to explore the effects of 84 different design parameter combinations using the simulator.
>
> For architectural research, simulators are also a common platform.
> The ISCA conference has held multiple competitions based on the [ChampSim][champsim] simulator,
> including the [cache replacement algorithm competition][cache replacement] and the [data prefetch algorithm competition][data prefetch].
> Researchers evaluate the performance of various algorithms in the simulator to quickly adjust the overall implementation and fine-tune the parameters of the algorithms.
> Although a qualified algorithm still requires implementation and verification at the RTL level,
> exploring various algorithms at the RTL level from the outset is highly inefficient.
>
> Therefore, when you truly understand that `computer architecture design ≠ RTL coding`,
> you have truly begun your journey into the field of computer architecture design.

[gem5]: https://www.gem5.org/
[champsim]: https://github.com/ChampSim/ChampSim
[cache replacement]: https://www.sigarch.org/call-contributions/the-2nd-cache-replacement-championship/
[data prefetch]: https://www.sigarch.org/call-contributions/third-data-prefetching-championship/

<!-- ## 缓存一致性

当store指令修改数据块的内容后, 按照程序的语义,
后续应从相应地址中读出新数据, 否则程序的执行将会出错.
但内存中的数据可能因为cache机制而存在多个副本,
如何保证从每个副本中都能读出新数据, 称为缓存一致性(cache coherence)问题.

在计算机系统中, 小到处理器中的cache, 大到分布式系统和互联网,
只要存在数据副本, 它们之间就会存在一致性问题.
在NPC添加icache后, 我们可以通过以下的`smc.c`来复现一个一致性问题:

```c
// smc.c
int main() {
  asm volatile("li a0, 0;"
               "li a1, UART_TX;"     // change UART_TX to the correct address
               "li t1, 0x41;"        // 0x41 = 'A'
               "la a2, again;"
               "li t2, 0x00008067;"  // 0x00008067 = ret
               "again:"
               "sb t1, (a1);"
               "sw t2, (a2);"
               "j again;"
              );
  return 0;
}
``` -->

## Cache Coherence

When a store instruction modifies the content of a data block, according to the program's semantics,
subsequent instructions should read the new data from the corresponding address; otherwise, the program execution will fail.
However, due to the cache mechanism, multiple copies of the data may exist in memory.
Ensuring that the new data is read from each copy is known as the cache coherence problem.

In computer systems, from the cache in a processor to distributed systems and the internet,
whenever there are data copies, there will be consistency issues between them.
After adding icache to NPC, we can reproduce a consistency issue using the following `smc.c`:

```c
// smc.c
int main() {
asm volatile(“li a0, 0;”
“li a1, UART_TX;” // change UART_TX to the correct address
“li t1, 0x41;” // 0x41 = ‘A’
“la a2, again;”
“li t2, 0x00008067;” // 0x00008067 = ret
“again:”
“sb t1, (a1);”
“sw t2, (a2);”
“j again;”
);
return 0;
}
```

<!-- 上述程序首先对一些寄存器赋初值, 然后在标号`again`处往串口写入一个字符`A`,
接着将`again`处的指令改写为`ret`, 最后跳转回`again`处重新执行.
按照程序的语义, 程序应该在输出一个字符`A`后, 通过被改写的`ret`指令从`main()`返回.
这种在运行过程中修改自己的代码称为"自修改代码"(Self Modified Code). -->

The above program first initializes some registers, then writes the character ‘A’ to the serial port at the label ‘again’,
then rewrites the instruction at ‘again’ to ‘ret’, and finally jumps back to ‘again’ to re-execute.
According to the program's semantics, the program should output the character `A` and then return to `main()` via the modified `ret` instruction.
This type of code that modifies itself during execution is called “self-modifying code” (Self Modified Code).

<!-- > #### comment::计算机发展史中的自修改代码
> 在过去内存地址空间非常紧张的时代, 经常会使用自修改代码来提升内存的利用率,
> 从而让程序在有限的内存中实现更多的功能.
> 例如, 1980年代的[FC红白机只有64KB的地址空间][NES MMIO], 其中卡带上的ROM占了32KB.
> 有一些卡带上还有8KB的RAM, 但如果卡带上没有RAM, 那程序就只能使用CPU内部集成的2KB RAM了.
> 为了利用如此有限的资源开发出精彩的游戏,
> 开发者使用了非常多的黑科技, 自修改代码就是其中一种方法.
>
> 随着存储器技术的发展, 内存容量已经不像过去那样捉襟见肘了,
> 加上自修改代码难以阅读和维护, 现代程序中已经很难看到自修改代码的踪影.

[NES MMIO]: https://www.nesdev.org/wiki/CPU_memory_map -->

> #### comment::Self-modifying code in the history of computer development
> In the past, when memory address space was extremely limited, self-modifying code was often used to improve memory utilization,
> enabling programs to perform more functions within limited memory.
> For example, the [FC NES console from the 1980s had only 64KB of address space][NES MMIO], with 32KB occupied by the ROM on the cartridge.
> Some cartridges also had 8KB of RAM, but if the cartridge did not have RAM, the program could only use the 2KB of RAM integrated into the CPU.
> To develop exciting games with such limited resources,
> developers employed numerous cutting-edge techniques, and self-modifying code was one of them.
>
> With the development of memory technology, memory capacity is no longer as limited as it was in the past,
> and since self-modifying code is difficult to read and maintain, it is now rarely seen in modern programs.

[NES MMIO]: https://www.nesdev.org/wiki/CPU_memory_map

<!-- -->
<!-- > #### todo::复现缓存一致性问题
> 将上述程序编译到AM并在NPC上运行, 你发现了什么问题?
> 尝试给出分析, 并结合波形验证你的想法.
>
> 如果你没有发现问题, 可以尝试增加icache的容量.

为了解决上述问题, 一种直接的方案是让系统中的所有副本时刻保持一致.
例如, 在每次执行store指令时, 都马上检查系统中是否存在其他副本,
若存在, 则对其进行更新或使其无效, 从而保证后续操作无论从何处访问,
都能直接读出新数据(采用更新方式), 或因缺失而从下一层存储层次中读出新数据(采用无效方式);
x86指令集采用这种方案. 但显然, 这会提升CPU设计的复杂性.
特别地, 在一些高性能处理器中, store指令执行时, 其他部件也会同时访问系统中的各种cache,
如何在store指令完成所有副本的更新或无效之前避免其他部件访问到过时的数据, 是非常有挑战的.

另一种方案则更加宽松, 允许系统中的副本在某些时刻不一致,
但在程序访问这个数据块之前, 需要执行一条特殊的指令, 指示硬件对过时的副本进行处理.
通过这种方式, 程序的执行过程仍然能够访问到正确的数据, 结果仍然符合程序的语义.
RISC-V指令集采用这种方案. RISC-V中有一条`fence.i`指令,
其语义是让在其之后的取指操作都能看到在其之前的store指令修改的数据.
在这里, `fence.i`指令就像一道屏障, 让在其之后的取指操作无法跨越屏障来读取那些被store指令修改之前的旧数据.
此外, 在RISC-V手册中有如下描述:
```txt
RISC-V does not guarantee that stores to instruction memory will be made
visible to instruction fetches on a RISC-V hart until that hart executes
a FENCE.I instruction.
``` -->

> #### todo::Reproduce cache consistency issues
> Compile the above program for the AM and run it on the NPC. What issues did you encounter?
> Try to analyze the issues and verify your findings using waveforms.
>
> If you did not encounter any issues, try increasing the ICache capacity.

To solve the above problem, a direct solution is to ensure that all copies in the system are always consistent.
For example, each time a store instruction is executed, immediately check whether there are other copies in the system.
If there are, update them or invalidate them to ensure that subsequent operations, regardless of where they are accessed from,
can directly read the new data (using the update method) or read the new data from the next storage level due to its absence (using the invalid method).
The x86 instruction set adopts this solution. However, this clearly increases the complexity of CPU design.
In particular, in some high-performance processors, when a store instruction is executed, other components also access various caches in the system at the same time.
It is very challenging to prevent other components from accessing outdated data before the store instruction completes the update or invalidation of all copies.

Another solution is more lenient. allowing copies in the system to be inconsistent at certain times, 
but before the program accesses this data block, a special instruction must be executed to instruct the hardware to handle outdated copies.
In this way, the program's execution can still access the correct data, and the results remain consistent with the program's semantics.
The RISC-V instruction set adopts this approach. RISC-V includes a `fence.i` instruction,
whose semantics ensure that all subsequent fetch operations can see the data modified by store instructions that occurred before it.
 Here, the `fence.i` instruction acts as a barrier, preventing subsequent instruction fetches from crossing the barrier to read the old data modified by the store instruction.
Additionally, the RISC-V manual states:

```txt
RISC-V does not guarantee that stores to instruction memory will be made
visible to instruction fetches on a RISC-V hart until that hart executes
a FENCE.I instruction.
```

<!-- 也就是说, RISC-V允许icache中的副本在某些时刻与内存不一致, 这也符合上文的讨论.
关于`fence.i`的更多信息, 建议RTFM.

RISC-V只在指令集层次定义了`fence.i`的语义,
但具体如何在微结构层次中实现`fence.i`的功能, 则有多种不同的方案:

| 方案 | 执行store指令时      | 执行`fence.i`时  | 访问icache时   |
| :-:  | :-:                  | :-:              | :-:            |
| (1)  | 更新icache中的相应块 | nop              | 命中           |
| (2)  | 无效icache中的相应块 | nop              | 缺失, 访问内存 |
| (3)  | -                    | 冲刷整个icache   | 缺失, 访问内存 |

事实上, 实现方案(1)和(2)就是上文提到的"让系统中的所有副本时刻保持一致"的方案,
它是"允许系统中的副本在某些时刻不一致"的特殊情况:
由于在执行store指令时, 所有副本已保持一致, 故`fence.i`可实现成`nop`.
对于方案(3), 执行store指令时暂不对icache中的副本进行处理, 故这些副本会处于不一致的状态,
因此在执行`fence.i`时, 需要通过冲刷icache来实现其屏障的效果,
使后续对icache的访问必定缺失, 从而访问内存中的新数据.
不过无论采用何种实现方案, 程序中都需要添加`fence.i`指令, 从而满足RISC-V手册对程序的要求,
否则程序将无法在采用实现方案(3)的处理器上正确运行. -->

In other words, RISC-V allows the copy in the icache to be inconsistent with memory at certain times, which is consistent with the discussion above.
For more information about `fence.i`, please refer to the manual.

RISC-V only defines the semantics of `fence.i` at the instruction set level,
but there are multiple different schemes for implementing the functionality of `fence.i` at the microarchitecture level:

| Scheme | When executing a store instruction | When executing `fence.i` | When accessing the icache |
| :-: | :-: | :-: | :-: |
| (1) | Update the corresponding block in the icache | nop | Hit |
| (2) | Invalidate the corresponding block in the icache | nop | Miss, access memory |
| (3) | - | Flush the entire icache | Missing, access memory |

In fact, implementation schemes (1) and (2) are the “keep all copies in the system consistent at all times” scheme mentioned above,
which is a special case of “allow copies in the system to be inconsistent at certain times”:
Since all copies are already consistent when executing the store instruction, `fence.i` can be implemented as `nop`.
For scheme (3), the copies in the icache are not processed when the store instruction is executed, so these copies remain in an inconsistent state.
Therefore, when executing `fence.i`, the icache must be flushed to achieve its barrier effect,
ensuring that subsequent accesses to the icache are missing, thereby accessing the new data in memory.
However, regardless of the implementation scheme chosen, the program must include the `fence.i` instruction to meet the requirements of the RISC-V manual.
Otherwise, the program will not run correctly on processors implementing scheme (3).

<!-- 事实上, 这些实现方案之间的区别只不过是将副本的一致性问题放在硬件层处理还是放在软件层处理:
如果指令集规范要求处理器在硬件层次处理副本的一致性问题, 则这个问题对软件透明,
程序员无需考虑应该在程序中的何处添加类似`fence.i`的指令, 但代价是硬件设计更复杂;
如果指令集规范要求处理器在软件层次处理副本的一致性问题, 则硬件设计更简单, 但代价是增加了程序员的负担.
因此这个问题的本质是硬件设计的复杂性与程序开发负担之间的权衡. -->

In fact, the difference between these implementation schemes lies solely in whether the consistency of copies is handled at the hardware level or the software level:
If the instruction set specification requires the processor to handle copy consistency at the hardware level, this issue is transparent to the software,
and programmers do not need to consider where to add instructions like `fence.i` in the program, but the trade-off is more complex hardware design;
If the instruction set specification requires the processor to handle copy consistency issues at the software level, the hardware design is simpler, but the cost is increased programmer burden.
Therefore, the essence of this issue is a trade-off between hardware design complexity and programmer burden.

<!-- > #### todo::实现fence.i指令
> 根据你对`fence.i`指令的认识, 选择一种你认为合理的方案在NPC中实现`fence.i`指令.
> 实现后, 在上述`smc.c`中的合适位置添加`fence.i`指令, 并重新在NPC上运行.
> 如果你的实现正确, 你将看到程序输出一个字符`A`后成功结束.
>
> Hint: 你可能会遇到和`fence.i`相关的编译错误, 尝试根据报错信息解决问题.

事实上, 在真实计算机中缓存一致性问题还有更多的表现.
后续随着处理器变得复杂, 我们也会讨论更多的缓存一致性问题. -->

> #### todo::Implement the `fence.i` instruction
> Based on your understanding of the `fence.i` instruction, choose a reasonable approach to implement it in NPC.
> After implementation, add the `fence.i` instruction to the appropriate location in the aforementioned `smc.c` file and rerun NPC.
> If your implementation is correct, you will see the program output the character `A` and exit successfully.
>
> Hint: You may encounter compilation errors related to `fence.i`. Try to resolve the issues based on the error messages.

In fact, cache consistency issues manifest in more ways on real computers.
As processors become more complex, we will discuss additional cache consistency issues in future discussions.
